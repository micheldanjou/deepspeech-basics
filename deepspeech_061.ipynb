{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepspeech-061.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJkL+h307OKIIEJ27O70Vx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micheldanjou/deepspeech-basics/blob/master/deepspeech_061.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFlaFG68FzAl",
        "colab_type": "text"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-9Df-plEmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "998e213e-29a3-4488-9e6c-b556abca5846"
      },
      "source": [
        "!apt-get update && apt-get upgrade"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  r-cran-plogr\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages have been kept back:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "  r-cran-dbplyr\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A785b3dmJ8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2ef0b114-2608-48b5-9d16-e48bd442bf30"
      },
      "source": [
        "!apt-get install git-lfs"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  r-cran-plogr\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTMfxJQRYQHs",
        "colab_type": "text"
      },
      "source": [
        "# Unzip the model\n",
        "We assume that `deepspeech-0.6.1-models.tar`, `deepspeech-0.6.1-checkpoint.tar` and `audio-0.6.0.tar.gz` have been uploaded to your `deepspeech` folder on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNwWRiS-8bsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "42b7c992-d4a6-4ee2-f5ba-8832d4bbfb93"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Tw8QhA-Tvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDrHMSBPpbZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./deepspeech-0.6.1-checkpoint\n",
        "!rm -rf ./deepspeech-0.6.1-models\n",
        "!rm -rf ./deepspeech/audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjcScZrq_WQv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fd12e898-d466-414e-9d66-99d49735e863"
      },
      "source": [
        "!tar -xvf deepspeech-0.6.1-models.tar"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "._deepspeech-0.6.1-models\n",
            "deepspeech-0.6.1-models/\n",
            "deepspeech-0.6.1-models/._lm.binary\n",
            "deepspeech-0.6.1-models/lm.binary\n",
            "deepspeech-0.6.1-models/._output_graph.pbmm\n",
            "deepspeech-0.6.1-models/output_graph.pbmm\n",
            "deepspeech-0.6.1-models/._output_graph.pb\n",
            "deepspeech-0.6.1-models/output_graph.pb\n",
            "deepspeech-0.6.1-models/._trie\n",
            "deepspeech-0.6.1-models/trie\n",
            "deepspeech-0.6.1-models/output_graph.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xFEX4lC_sO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f331561d-b73e-47d4-c66a-c62c473adb47"
      },
      "source": [
        "!tar -xvf deepspeech-0.6.1-checkpoint.tar"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deepspeech-0.6.1-checkpoint/\n",
            "deepspeech-0.6.1-checkpoint/checkpoint\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.data-00000-of-00001\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.index\n",
            "deepspeech-0.6.1-checkpoint/best_dev_checkpoint\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.meta\n",
            "deepspeech-0.6.1-checkpoint/alphabet.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV7eSSO2XRII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cde52227-fae4-4066-e633-7a0dda378d6c"
      },
      "source": [
        "!tar -xvf audio-0.6.0.tar.gz"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "audio/\n",
            "audio/2830-3980-0043.wav\n",
            "audio/Attribution.txt\n",
            "audio/4507-16021-0012.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/License.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLMZgcmX-_Nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5647a91b-0ca1-49b2-b9fd-4b13f4b2b0d9"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/deepspeech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_DvwwCI56c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip training data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MkzwTVzIyyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip lm_true_alpha.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9zzuhpoYMfZ",
        "colab_type": "text"
      },
      "source": [
        "# Clone the DeepSpeech git repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puRG2nxgN-bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete the DeepSpeech git repo\n",
        "os.chdir('/content/drive/My Drive/deepspeech/')\n",
        "!rm -rf ./DeepSpeech/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFgAWobHKls6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "14d96016-3e42-40f8-dd5a-e362a3cb35a2"
      },
      "source": [
        "!git lfs clone --branch v0.6.1 https://github.com/mozilla/DeepSpeech.git"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18527 (delta 15), reused 24 (delta 14), pack-reused 18496\u001b[K\n",
            "Receiving objects: 100% (18527/18527), 47.68 MiB | 8.30 MiB/s, done.\n",
            "Resolving deltas: 100% (12605/12605), done.\n",
            "Note: checking out '3df20fee52fda47d08e3726fd0da86dbb414e9d8'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (1819/1819), done.\n",
            "Git LFS: (2 of 2 files) 913.52 MB / 913.52 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJndl5eU19g",
        "colab_type": "text"
      },
      "source": [
        "Somehow, `git status` is reporting modified files as soon as we checkout the git repo. I don't understand why, but since this is causing `!git checkout tags/v0.6.1` to fail, I am simply adding & committing these files to the local repo as a workaround. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwMFqx-cYtap",
        "colab_type": "text"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blRqtlTUJAVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "25735bc7-302e-43c7-9e71-cc14bde86caa"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech')\n",
        "!pip3 install $(python3 util/taskcluster.py --decoder)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ds-ctcdecoder==0.6.1 from https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.6.1.cpu-ctc/artifacts/public/ds_ctcdecoder-0.6.1-cp36-cp36m-manylinux1_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from ds-ctcdecoder==0.6.1) (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1uXzZ2xLcdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1993c693-9782-47be-c926-072ca470a31e"
      },
      "source": [
        "!pip3 install deepspeech==0.6.1"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepspeech==0.6.1 in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech==0.6.1) (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svWduKROMR9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed140e85-549d-42b9-d611-81223759a2cb"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.15.4)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.47.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.13.0)\n",
            "Requirement already satisfied: pyxdg in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.26)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (46.4.0)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (2.0.10)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (1.3.7)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (2.23.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.6.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (0.10.3.post1)\n",
            "Requirement already satisfied: paramiko>=2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (2.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.29.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->-r requirements.txt (line 19)) (4.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (1.24.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (2.1.8)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.15.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.48.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->-r requirements.txt (line 22)) (1.14.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (2.9.2)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (3.1.7)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa->-r requirements.txt (line 21)) (0.31.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 22)) (2.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9XgWizdNcp",
        "colab_type": "text"
      },
      "source": [
        "# Training a network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcB3pDlQ_6t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyrLQSXgcXRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c62f93c8-5833-4ed1-e1a2-57ab856448d9"
      },
      "source": [
        "!python3  ./DeepSpeech.py --helpfull"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/sh: 1: sox: not found\n",
            "SoX could not be found!\n",
            "\n",
            "    If you do not have SoX, proceed here:\n",
            "     - - - http://sox.sourceforge.net/ - - -\n",
            "\n",
            "    If you do (or think that you should) have SoX, double-check your\n",
            "    path variables.\n",
            "    \n",
            "\n",
            "       USAGE: ./DeepSpeech.py [flags]\n",
            "flags:\n",
            "\n",
            "absl.app:\n",
            "  -?,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpxml: like --helpfull, but generates XML output\n",
            "    (default: 'false')\n",
            "  --[no]only_check_args: Set to true to validate args and exit.\n",
            "    (default: 'false')\n",
            "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
            "    mortem.\n",
            "    (default: 'false')\n",
            "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
            "    Implies --run_with_profiling.\n",
            "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
            "    (default: 'false')\n",
            "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
            "    be slower, and the output format might change over time.\n",
            "    (default: 'false')\n",
            "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
            "    for profiling. This has no effect unless --run_with_profiling is set.\n",
            "    (default: 'true')\n",
            "\n",
            "absl.logging:\n",
            "  --[no]alsologtostderr: also log to stderr?\n",
            "    (default: 'false')\n",
            "  --log_dir: directory to write logfiles into\n",
            "    (default: '')\n",
            "  --[no]logtostderr: Should only log to stderr?\n",
            "    (default: 'false')\n",
            "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
            "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
            "    is used.\n",
            "    (default: 'true')\n",
            "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
            "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
            "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
            "    cancels the effect of this flag. Please also note that this flag is subject\n",
            "    to --verbosity and requires logfile not be stderr.\n",
            "    (default: 'fatal')\n",
            "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
            "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
            "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
            "    (info) after flags are parsed.\n",
            "    (default: '-1')\n",
            "    (an integer)\n",
            "\n",
            "absl.testing.absltest:\n",
            "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
            "    the default value of this flag between runs, so it is not appropriate for\n",
            "    seeding probabilistic tests.\n",
            "    (default: '301')\n",
            "    (an integer)\n",
            "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
            "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
            "    If 0 or not set, do not randomize test case execution order. This flag also\n",
            "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
            "    (default: '')\n",
            "  --test_srcdir: Root of directory tree where source files live\n",
            "    (default: '')\n",
            "  --test_tmpdir: Directory for temporary testing files\n",
            "    (default: '/tmp/absl_testing')\n",
            "  --xml_output_file: File to store XML test results\n",
            "    (default: '')\n",
            "\n",
            "tensorflow.python.ops.parallel_for.pfor:\n",
            "  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a\n",
            "    while loop for ops for which a converter is not defined.\n",
            "    (default: 'false')\n",
            "\n",
            "util.flags:\n",
            "  --alphabet_config_path: path to the configuration file specifying the alphabet\n",
            "    used by the network. See the comment in data/alphabet.txt for a description\n",
            "    of the format.\n",
            "    (default: 'data/alphabet.txt')\n",
            "  --audio_sample_rate: sample rate value expected by model\n",
            "    (default: '16000')\n",
            "    (an integer)\n",
            "  --[no]augmentation_freq_and_time_masking: whether to use frequency and time\n",
            "    masking augmentation\n",
            "    (default: 'false')\n",
            "  --augmentation_freq_and_time_masking_freq_mask_range: max range of masks in\n",
            "    the frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_freq_masks: number of masks in the\n",
            "    frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_time_masks: number of masks in the\n",
            "    time domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_time_mask_range: max range of masks in\n",
            "    the time domain when performing freqtime-mask augmentation\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --[no]augmentation_pitch_and_tempo_scaling: whether to use spectrogram speed\n",
            "    and tempo scaling\n",
            "    (default: 'false')\n",
            "  --augmentation_pitch_and_tempo_scaling_max_pitch: max value of pitch scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_max_tempo: max vlaue of tempo scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_min_pitch: min value of pitch scaling\n",
            "    (default: '0.95')\n",
            "    (a number)\n",
            "  --[no]augmentation_sparse_warp: whether to use spectrogram sparse warp. USE OF\n",
            "    THIS FLAG IS UNSUPPORTED, enable sparse warp will increase training time\n",
            "    drastically, and the paper also mentioned that this is not a major factor to\n",
            "    improve accuracy.\n",
            "    (default: 'false')\n",
            "  --augmentation_sparse_warp_interpolation_order:\n",
            "    sparse_warp_interpolation_order\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_boundary_points:\n",
            "    sparse_warp_num_boundary_points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_control_points: specify number of control\n",
            "    points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_regularization_weight:\n",
            "    sparse_warp_regularization_weight\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --augmentation_sparse_warp_time_warping_para: time_warping_para\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --augmentation_spec_dropout_keeprate: keep rate of dropout augmentation on\n",
            "    spectrogram (if 1, no dropout will be performed on spectrogram)\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --augmentation_speed_up_std: std for speeding-up tempo. If std is 0, this\n",
            "    augmentation is not performed\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --[no]automatic_mixed_precision: whether to allow automatic mixed precision\n",
            "    training. USE OF THIS FLAG IS UNSUPPORTED. Checkpoints created with\n",
            "    automatic mixed precision training will not be usable without mixed\n",
            "    precision.\n",
            "    (default: 'false')\n",
            "  --beam_width: beam width used in the CTC decoder when building candidate\n",
            "    transcriptions\n",
            "    (default: '1024')\n",
            "    (an integer)\n",
            "  --beta1: beta 1 parameter of Adam optimizer\n",
            "    (default: '0.9')\n",
            "    (a number)\n",
            "  --beta2: beta 2 parameter of Adam optimizer\n",
            "    (default: '0.999')\n",
            "    (a number)\n",
            "  --checkpoint_dir: directory in which checkpoints are stored - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --checkpoint_secs: checkpoint saving interval in seconds\n",
            "    (default: '600')\n",
            "    (an integer)\n",
            "  --cudnn_checkpoint: path to a checkpoint created using --use_cudnn_rnn.\n",
            "    Specifying this flag allows one to convert a CuDNN RNN checkpoint to a\n",
            "    checkpoint capable of running on a CPU graph.\n",
            "    (default: '')\n",
            "  --cutoff_prob: only consider characters until this probability mass is\n",
            "    reached. 1.0 = disabled.\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --cutoff_top_n: only process this number of characters sorted by probability\n",
            "    mass for each time step. If bigger than alphabet size, disabled.\n",
            "    (default: '300')\n",
            "    (an integer)\n",
            "  --data_aug_features_additive: std of the Gaussian additive noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --data_aug_features_multiplicative: std of normal distribution around 1 for\n",
            "    multiplicative noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dev_batch_size: number of elements in a validation batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --dev_files: comma separated list of files specifying the dataset used for\n",
            "    validation. Multiple files will get merged. If empty, validation will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --dropout_rate: dropout rate for feedforward layers\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --dropout_rate2: dropout rate for layer 2 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate3: dropout rate for layer 3 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate4: dropout rate for layer 4 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate5: dropout rate for layer 5 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate6: dropout rate for layer 6 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --[no]early_stop: enable early stopping mechanism over validation dataset. If\n",
            "    validation is not being run, early stopping is disabled.\n",
            "    (default: 'true')\n",
            "  --epochs: how many epochs (complete runs through the train files) to train for\n",
            "    (default: '75')\n",
            "    (an integer)\n",
            "  --epsilon: epsilon parameter of Adam optimizer\n",
            "    (default: '1e-08')\n",
            "    (a number)\n",
            "  --es_mean_th: mean threshold for loss to determine the condition if early\n",
            "    stopping is required\n",
            "    (default: '0.5')\n",
            "    (a number)\n",
            "  --es_std_th: standard deviation threshold for loss to determine the condition\n",
            "    if early stopping is required\n",
            "    (default: '0.5')\n",
            "    (a number)\n",
            "  --es_steps: number of validations to consider for early stopping. Loss is not\n",
            "    stored in the checkpoint so when checkpoint is revived it starts the loss\n",
            "    calculation from start at that point\n",
            "    (default: '4')\n",
            "    (an integer)\n",
            "  --export_batch_size: number of elements per batch on the exported graph\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --export_dir: directory in which exported models are stored - if omitted, the\n",
            "    model won't get exported\n",
            "    (default: '')\n",
            "  --export_language: language the model was trained on e.g. \"en\" or \"English\".\n",
            "    Gets embedded into exported model.\n",
            "    (default: '')\n",
            "  --[no]export_tflite: export a graph ready for TF Lite engine\n",
            "    (default: 'false')\n",
            "  --[no]export_zip: export a TFLite model and package with LM and info.json\n",
            "    (default: 'false')\n",
            "  --feature_cache: cache MFCC features to disk to speed up future training runs\n",
            "    ont he same data. This flag specifies the path where cached features\n",
            "    extracted from --train_files will be saved. If empty, or if online\n",
            "    augmentation flags are enabled, caching will be disabled.\n",
            "    (default: '')\n",
            "  --feature_win_len: feature extraction audio window length in milliseconds\n",
            "    (default: '32')\n",
            "    (an integer)\n",
            "  --feature_win_step: feature extraction window step length in milliseconds\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --inter_op_parallelism_threads: number of inter-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --intra_op_parallelism_threads: number of intra-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --learning_rate: learning rate of Adam optimizer\n",
            "    (default: '0.001')\n",
            "    (a number)\n",
            "  --limit_dev: maximum number of elements to use from validation set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_test: maximum number of elements to use from test set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_train: maximum number of elements to use from train set - 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --lm: Alias for --lm_binary_path.\n",
            "    (default: 'data/lm/lm.binary')\n",
            "  --lm_alpha: the alpha hyperparameter of the CTC decoder. Language Model\n",
            "    weight.\n",
            "    (default: '0.75')\n",
            "    (a number)\n",
            "  --lm_beta: the beta hyperparameter of the CTC decoder. Word insertion weight.\n",
            "    (default: '1.85')\n",
            "    (a number)\n",
            "  --lm_binary_path: path to the language model binary file created with KenLM\n",
            "    (default: 'data/lm/lm.binary')\n",
            "  --lm_trie_path: path to the language model trie file created with\n",
            "    native_client/generate_trie\n",
            "    (default: 'data/lm/trie')\n",
            "  --load: \"last\" for loading most recent epoch checkpoint, \"best\" for loading\n",
            "    best validated checkpoint, \"init\" for initializing a fresh model, \"auto\" for\n",
            "    trying the other options in order last > best > init\n",
            "    (default: 'auto')\n",
            "  --log_level: log level for console logs - 0: INFO, 1: WARN, 2: ERROR, 3: FATAL\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]log_placement: whether to log device placement of the operators to the\n",
            "    console\n",
            "    (default: 'false')\n",
            "  --max_to_keep: number of checkpoint files to keep - default value is 5\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --n_hidden: layer width to use when initialising layers\n",
            "    (default: '2048')\n",
            "    (an integer)\n",
            "  --n_steps: how many timesteps to process at once by the export graph, higher\n",
            "    values mean more latency\n",
            "    (default: '16')\n",
            "    (an integer)\n",
            "  --one_shot_infer: one-shot inference mode: specify a wav file and the script\n",
            "    will load the checkpoint and perform inference on it.\n",
            "    (default: '')\n",
            "  --random_seed: default random seed that is used to initialize variables\n",
            "    (default: '4568')\n",
            "    (an integer)\n",
            "  --relu_clip: ReLU clipping value for non-recurrent layers\n",
            "    (default: '20.0')\n",
            "    (a number)\n",
            "  --[no]remove_export: whether to remove old exported models\n",
            "    (default: 'false')\n",
            "  --report_count: number of phrases with lowest WER(best matching) to print out\n",
            "    during a WER report\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "  --[no]show_progressbar: Show progress for training, validation and testing\n",
            "    processes. Log level should be > 0.\n",
            "    (default: 'true')\n",
            "  --summary_dir: target directory for TensorBoard summaries - defaults to\n",
            "    directory \"deepspeech/summaries\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --test_batch_size: number of elements in a test batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --test_files: comma separated list of files specifying the dataset used for\n",
            "    testing. Multiple files will get merged. If empty, the model will not be\n",
            "    tested.\n",
            "    (default: '')\n",
            "  --test_output_file: path to a file to save all src/decoded/distance/loss\n",
            "    tuples generated during a test epoch\n",
            "    (default: '')\n",
            "  --train_batch_size: number of elements in a training batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --train_files: comma separated list of files specifying the dataset used for\n",
            "    training. Multiple files will get merged. If empty, training will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --trie: Alias for --lm_trie_path.\n",
            "    (default: 'data/lm/trie')\n",
            "  --[no]use_allow_growth: use Allow Growth flag which will allocate only\n",
            "    required amount of GPU memory and prevent full allocation of available GPU\n",
            "    memory\n",
            "    (default: 'false')\n",
            "  --[no]use_cudnn_rnn: use CuDNN RNN backend for training on GPU. Note that\n",
            "    checkpoints created with this flag can only be used with CuDNN RNN, i.e.\n",
            "    fine tuning on a CPU device will not work\n",
            "    (default: 'false')\n",
            "  --[no]utf8: enable UTF-8 mode. When this is used the model outputs UTF-8\n",
            "    sequences directly rather than using an alphabet mapping.\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZsOqfy7dSEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15323b72-f4b5-4e11-a37f-157fcc6920f6"
      },
      "source": [
        "!bash ./bin/run-ldc93s1.sh"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ '[' '!' -f DeepSpeech.py ']'\n",
            "+ '[' '!' -f data/ldc93s1/ldc93s1.csv ']'\n",
            "+ echo 'Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.'\n",
            "Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "+ python -u bin/import_ldc93s1.py ./data/ldc93s1\n",
            "No path \"./data/ldc93s1\" - creating ...\n",
            "No archive \"./data/ldc93s1/LDC93S1.wav\" - downloading...\n",
            "Progress |                                                      | N/A% completedNo archive \"./data/ldc93s1/LDC93S1.txt\" - downloading...\n",
            "Progress |######################################################| 100% completed\n",
            "Progress |######################################################| 100% completed\n",
            "+ '[' -d '' ']'\n",
            "++ python -c 'from xdg import BaseDirectory as xdg; print(xdg.save_data_path(\"deepspeech/ldc93s1\"))'\n",
            "+ checkpoint_dir=/root/.local/share/deepspeech/ldc93s1\n",
            "+ export CUDA_VISIBLE_DEVICES=0\n",
            "+ CUDA_VISIBLE_DEVICES=0\n",
            "+ python -u DeepSpeech.py --noshow_progressbar --train_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --test_batch_size 1 --n_hidden 100 --epochs 200 --checkpoint_dir /root/.local/share/deepspeech/ldc93s1\n",
            "/bin/sh: 1: sox: not found\n",
            "SoX could not be found!\n",
            "\n",
            "    If you do not have SoX, proceed here:\n",
            "     - - - http://sox.sourceforge.net/ - - -\n",
            "\n",
            "    If you do (or think that you should) have SoX, double-check your\n",
            "    path variables.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0530 22:06:55.806165 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "W0530 22:06:56.020519 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "W0530 22:06:56.020887 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "W0530 22:06:56.021119 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0530 22:06:56.149055 140080214624128 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66b1cff898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66b1cff898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0530 22:06:56.194641 140080214624128 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66b1cff898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66b1cff898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0530 22:06:56.291368 140080214624128 deprecation.py:323] From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0530 22:06:57.101484 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1200\n",
            "I0530 22:06:57.103777 140080214624128 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1200\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-1200, step 1200\n",
            "I STARTING Optimization\n",
            "I Training epoch 0...\n",
            "I Finished training epoch 0 - loss: 0.011978\n",
            "I Training epoch 1...\n",
            "I Finished training epoch 1 - loss: 0.009510\n",
            "I Training epoch 2...\n",
            "I Finished training epoch 2 - loss: 0.013341\n",
            "I Training epoch 3...\n",
            "I Finished training epoch 3 - loss: 0.013158\n",
            "I Training epoch 4...\n",
            "I Finished training epoch 4 - loss: 0.012069\n",
            "I Training epoch 5...\n",
            "I Finished training epoch 5 - loss: 0.009058\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0530 22:06:59.056233 140080214624128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I Training epoch 6...\n",
            "I Finished training epoch 6 - loss: 0.010157\n",
            "I Training epoch 7...\n",
            "I Finished training epoch 7 - loss: 0.013876\n",
            "I Training epoch 8...\n",
            "I Finished training epoch 8 - loss: 0.016818\n",
            "I Training epoch 9...\n",
            "I Finished training epoch 9 - loss: 0.012739\n",
            "I Training epoch 10...\n",
            "I Finished training epoch 10 - loss: 0.018939\n",
            "I Training epoch 11...\n",
            "I Finished training epoch 11 - loss: 0.008198\n",
            "I Training epoch 12...\n",
            "I Finished training epoch 12 - loss: 0.006559\n",
            "I Training epoch 13...\n",
            "I Finished training epoch 13 - loss: 0.012028\n",
            "I Training epoch 14...\n",
            "I Finished training epoch 14 - loss: 0.009236\n",
            "I Training epoch 15...\n",
            "I Finished training epoch 15 - loss: 0.013312\n",
            "I Training epoch 16...\n",
            "I Finished training epoch 16 - loss: 0.009252\n",
            "I Training epoch 17...\n",
            "I Finished training epoch 17 - loss: 0.012272\n",
            "I Training epoch 18...\n",
            "I Finished training epoch 18 - loss: 0.016648\n",
            "I Training epoch 19...\n",
            "I Finished training epoch 19 - loss: 0.009943\n",
            "I Training epoch 20...\n",
            "I Finished training epoch 20 - loss: 0.011241\n",
            "I Training epoch 21...\n",
            "I Finished training epoch 21 - loss: 0.011162\n",
            "I Training epoch 22...\n",
            "I Finished training epoch 22 - loss: 0.007534\n",
            "I Training epoch 23...\n",
            "I Finished training epoch 23 - loss: 0.014344\n",
            "I Training epoch 24...\n",
            "I Finished training epoch 24 - loss: 0.015668\n",
            "I Training epoch 25...\n",
            "I Finished training epoch 25 - loss: 0.019003\n",
            "I Training epoch 26...\n",
            "I Finished training epoch 26 - loss: 0.009993\n",
            "I Training epoch 27...\n",
            "I Finished training epoch 27 - loss: 0.007980\n",
            "I Training epoch 28...\n",
            "I Finished training epoch 28 - loss: 0.016416\n",
            "I Training epoch 29...\n",
            "I Finished training epoch 29 - loss: 0.009564\n",
            "I Training epoch 30...\n",
            "I Finished training epoch 30 - loss: 0.018593\n",
            "I Training epoch 31...\n",
            "I Finished training epoch 31 - loss: 0.010576\n",
            "I Training epoch 32...\n",
            "I Finished training epoch 32 - loss: 0.012933\n",
            "I Training epoch 33...\n",
            "I Finished training epoch 33 - loss: 0.009372\n",
            "I Training epoch 34...\n",
            "I Finished training epoch 34 - loss: 0.011822\n",
            "I Training epoch 35...\n",
            "I Finished training epoch 35 - loss: 0.011179\n",
            "I Training epoch 36...\n",
            "I Finished training epoch 36 - loss: 0.006479\n",
            "I Training epoch 37...\n",
            "I Finished training epoch 37 - loss: 0.015784\n",
            "I Training epoch 38...\n",
            "I Finished training epoch 38 - loss: 0.012231\n",
            "I Training epoch 39...\n",
            "I Finished training epoch 39 - loss: 0.012046\n",
            "I Training epoch 40...\n",
            "I Finished training epoch 40 - loss: 0.008974\n",
            "I Training epoch 41...\n",
            "I Finished training epoch 41 - loss: 0.006754\n",
            "I Training epoch 42...\n",
            "I Finished training epoch 42 - loss: 0.009869\n",
            "I Training epoch 43...\n",
            "I Finished training epoch 43 - loss: 0.008854\n",
            "I Training epoch 44...\n",
            "I Finished training epoch 44 - loss: 0.010162\n",
            "I Training epoch 45...\n",
            "I Finished training epoch 45 - loss: 0.009995\n",
            "I Training epoch 46...\n",
            "I Finished training epoch 46 - loss: 0.008721\n",
            "I Training epoch 47...\n",
            "I Finished training epoch 47 - loss: 0.016024\n",
            "I Training epoch 48...\n",
            "I Finished training epoch 48 - loss: 0.009200\n",
            "I Training epoch 49...\n",
            "I Finished training epoch 49 - loss: 0.009362\n",
            "I Training epoch 50...\n",
            "I Finished training epoch 50 - loss: 0.010718\n",
            "I Training epoch 51...\n",
            "I Finished training epoch 51 - loss: 0.012017\n",
            "I Training epoch 52...\n",
            "I Finished training epoch 52 - loss: 0.009263\n",
            "I Training epoch 53...\n",
            "I Finished training epoch 53 - loss: 0.013780\n",
            "I Training epoch 54...\n",
            "I Finished training epoch 54 - loss: 0.010089\n",
            "I Training epoch 55...\n",
            "I Finished training epoch 55 - loss: 0.015731\n",
            "I Training epoch 56...\n",
            "I Finished training epoch 56 - loss: 0.011523\n",
            "I Training epoch 57...\n",
            "I Finished training epoch 57 - loss: 0.009505\n",
            "I Training epoch 58...\n",
            "I Finished training epoch 58 - loss: 0.010764\n",
            "I Training epoch 59...\n",
            "I Finished training epoch 59 - loss: 0.014252\n",
            "I Training epoch 60...\n",
            "I Finished training epoch 60 - loss: 0.014417\n",
            "I Training epoch 61...\n",
            "I Finished training epoch 61 - loss: 0.009716\n",
            "I Training epoch 62...\n",
            "I Finished training epoch 62 - loss: 0.010668\n",
            "I Training epoch 63...\n",
            "I Finished training epoch 63 - loss: 0.012411\n",
            "I Training epoch 64...\n",
            "I Finished training epoch 64 - loss: 0.015519\n",
            "I Training epoch 65...\n",
            "I Finished training epoch 65 - loss: 0.009186\n",
            "I Training epoch 66...\n",
            "I Finished training epoch 66 - loss: 0.010938\n",
            "I Training epoch 67...\n",
            "I Finished training epoch 67 - loss: 0.008295\n",
            "I Training epoch 68...\n",
            "I Finished training epoch 68 - loss: 0.009759\n",
            "I Training epoch 69...\n",
            "I Finished training epoch 69 - loss: 0.008276\n",
            "I Training epoch 70...\n",
            "I Finished training epoch 70 - loss: 0.009111\n",
            "I Training epoch 71...\n",
            "I Finished training epoch 71 - loss: 0.009436\n",
            "I Training epoch 72...\n",
            "I Finished training epoch 72 - loss: 0.012863\n",
            "I Training epoch 73...\n",
            "I Finished training epoch 73 - loss: 0.007326\n",
            "I Training epoch 74...\n",
            "I Finished training epoch 74 - loss: 0.009869\n",
            "I Training epoch 75...\n",
            "I Finished training epoch 75 - loss: 0.010852\n",
            "I Training epoch 76...\n",
            "I Finished training epoch 76 - loss: 0.008524\n",
            "I Training epoch 77...\n",
            "I Finished training epoch 77 - loss: 0.007400\n",
            "I Training epoch 78...\n",
            "I Finished training epoch 78 - loss: 0.012497\n",
            "I Training epoch 79...\n",
            "I Finished training epoch 79 - loss: 0.008499\n",
            "I Training epoch 80...\n",
            "I Finished training epoch 80 - loss: 0.011069\n",
            "I Training epoch 81...\n",
            "I Finished training epoch 81 - loss: 0.008537\n",
            "I Training epoch 82...\n",
            "I Finished training epoch 82 - loss: 0.007734\n",
            "I Training epoch 83...\n",
            "I Finished training epoch 83 - loss: 0.011355\n",
            "I Training epoch 84...\n",
            "I Finished training epoch 84 - loss: 0.012447\n",
            "I Training epoch 85...\n",
            "I Finished training epoch 85 - loss: 0.007552\n",
            "I Training epoch 86...\n",
            "I Finished training epoch 86 - loss: 0.009974\n",
            "I Training epoch 87...\n",
            "I Finished training epoch 87 - loss: 0.009085\n",
            "I Training epoch 88...\n",
            "I Finished training epoch 88 - loss: 0.010686\n",
            "I Training epoch 89...\n",
            "I Finished training epoch 89 - loss: 0.011073\n",
            "I Training epoch 90...\n",
            "I Finished training epoch 90 - loss: 0.007948\n",
            "I Training epoch 91...\n",
            "I Finished training epoch 91 - loss: 0.009268\n",
            "I Training epoch 92...\n",
            "I Finished training epoch 92 - loss: 0.006796\n",
            "I Training epoch 93...\n",
            "I Finished training epoch 93 - loss: 0.015088\n",
            "I Training epoch 94...\n",
            "I Finished training epoch 94 - loss: 0.008410\n",
            "I Training epoch 95...\n",
            "I Finished training epoch 95 - loss: 0.007489\n",
            "I Training epoch 96...\n",
            "I Finished training epoch 96 - loss: 0.006997\n",
            "I Training epoch 97...\n",
            "I Finished training epoch 97 - loss: 0.006252\n",
            "I Training epoch 98...\n",
            "I Finished training epoch 98 - loss: 0.009503\n",
            "I Training epoch 99...\n",
            "I Finished training epoch 99 - loss: 0.012863\n",
            "I Training epoch 100...\n",
            "I Finished training epoch 100 - loss: 0.013482\n",
            "I Training epoch 101...\n",
            "I Finished training epoch 101 - loss: 0.009182\n",
            "I Training epoch 102...\n",
            "I Finished training epoch 102 - loss: 0.016561\n",
            "I Training epoch 103...\n",
            "I Finished training epoch 103 - loss: 0.011213\n",
            "I Training epoch 104...\n",
            "I Finished training epoch 104 - loss: 0.011169\n",
            "I Training epoch 105...\n",
            "I Finished training epoch 105 - loss: 0.007715\n",
            "I Training epoch 106...\n",
            "I Finished training epoch 106 - loss: 0.005920\n",
            "I Training epoch 107...\n",
            "I Finished training epoch 107 - loss: 0.006742\n",
            "I Training epoch 108...\n",
            "I Finished training epoch 108 - loss: 0.008119\n",
            "I Training epoch 109...\n",
            "I Finished training epoch 109 - loss: 0.009009\n",
            "I Training epoch 110...\n",
            "I Finished training epoch 110 - loss: 0.008057\n",
            "I Training epoch 111...\n",
            "I Finished training epoch 111 - loss: 0.011067\n",
            "I Training epoch 112...\n",
            "I Finished training epoch 112 - loss: 0.012628\n",
            "I Training epoch 113...\n",
            "I Finished training epoch 113 - loss: 0.009352\n",
            "I Training epoch 114...\n",
            "I Finished training epoch 114 - loss: 0.009034\n",
            "I Training epoch 115...\n",
            "I Finished training epoch 115 - loss: 0.008408\n",
            "I Training epoch 116...\n",
            "I Finished training epoch 116 - loss: 0.012886\n",
            "I Training epoch 117...\n",
            "I Finished training epoch 117 - loss: 0.007117\n",
            "I Training epoch 118...\n",
            "I Finished training epoch 118 - loss: 0.008818\n",
            "I Training epoch 119...\n",
            "I Finished training epoch 119 - loss: 0.011139\n",
            "I Training epoch 120...\n",
            "I Finished training epoch 120 - loss: 0.007569\n",
            "I Training epoch 121...\n",
            "I Finished training epoch 121 - loss: 0.007814\n",
            "I Training epoch 122...\n",
            "I Finished training epoch 122 - loss: 0.007000\n",
            "I Training epoch 123...\n",
            "I Finished training epoch 123 - loss: 0.010204\n",
            "I Training epoch 124...\n",
            "I Finished training epoch 124 - loss: 0.006767\n",
            "I Training epoch 125...\n",
            "I Finished training epoch 125 - loss: 0.009532\n",
            "I Training epoch 126...\n",
            "I Finished training epoch 126 - loss: 0.010206\n",
            "I Training epoch 127...\n",
            "I Finished training epoch 127 - loss: 0.008245\n",
            "I Training epoch 128...\n",
            "I Finished training epoch 128 - loss: 0.010373\n",
            "I Training epoch 129...\n",
            "I Finished training epoch 129 - loss: 0.009634\n",
            "I Training epoch 130...\n",
            "I Finished training epoch 130 - loss: 0.006550\n",
            "I Training epoch 131...\n",
            "I Finished training epoch 131 - loss: 0.020192\n",
            "I Training epoch 132...\n",
            "I Finished training epoch 132 - loss: 0.023749\n",
            "I Training epoch 133...\n",
            "I Finished training epoch 133 - loss: 0.007084\n",
            "I Training epoch 134...\n",
            "I Finished training epoch 134 - loss: 0.017585\n",
            "I Training epoch 135...\n",
            "I Finished training epoch 135 - loss: 0.013307\n",
            "I Training epoch 136...\n",
            "I Finished training epoch 136 - loss: 0.010431\n",
            "I Training epoch 137...\n",
            "I Finished training epoch 137 - loss: 0.009160\n",
            "I Training epoch 138...\n",
            "I Finished training epoch 138 - loss: 0.009685\n",
            "I Training epoch 139...\n",
            "I Finished training epoch 139 - loss: 0.008618\n",
            "I Training epoch 140...\n",
            "I Finished training epoch 140 - loss: 0.013082\n",
            "I Training epoch 141...\n",
            "I Finished training epoch 141 - loss: 0.009776\n",
            "I Training epoch 142...\n",
            "I Finished training epoch 142 - loss: 0.011349\n",
            "I Training epoch 143...\n",
            "I Finished training epoch 143 - loss: 0.006231\n",
            "I Training epoch 144...\n",
            "I Finished training epoch 144 - loss: 0.007426\n",
            "I Training epoch 145...\n",
            "I Finished training epoch 145 - loss: 0.010443\n",
            "I Training epoch 146...\n",
            "I Finished training epoch 146 - loss: 0.007094\n",
            "I Training epoch 147...\n",
            "I Finished training epoch 147 - loss: 0.008662\n",
            "I Training epoch 148...\n",
            "I Finished training epoch 148 - loss: 0.011188\n",
            "I Training epoch 149...\n",
            "I Finished training epoch 149 - loss: 0.009520\n",
            "I Training epoch 150...\n",
            "I Finished training epoch 150 - loss: 0.005255\n",
            "I Training epoch 151...\n",
            "I Finished training epoch 151 - loss: 0.007292\n",
            "I Training epoch 152...\n",
            "I Finished training epoch 152 - loss: 0.007032\n",
            "I Training epoch 153...\n",
            "I Finished training epoch 153 - loss: 0.016581\n",
            "I Training epoch 154...\n",
            "I Finished training epoch 154 - loss: 0.008438\n",
            "I Training epoch 155...\n",
            "I Finished training epoch 155 - loss: 0.009574\n",
            "I Training epoch 156...\n",
            "I Finished training epoch 156 - loss: 0.011398\n",
            "I Training epoch 157...\n",
            "I Finished training epoch 157 - loss: 0.008399\n",
            "I Training epoch 158...\n",
            "I Finished training epoch 158 - loss: 0.007940\n",
            "I Training epoch 159...\n",
            "I Finished training epoch 159 - loss: 0.008392\n",
            "I Training epoch 160...\n",
            "I Finished training epoch 160 - loss: 0.006521\n",
            "I Training epoch 161...\n",
            "I Finished training epoch 161 - loss: 0.005144\n",
            "I Training epoch 162...\n",
            "I Finished training epoch 162 - loss: 0.005716\n",
            "I Training epoch 163...\n",
            "I Finished training epoch 163 - loss: 0.009220\n",
            "I Training epoch 164...\n",
            "I Finished training epoch 164 - loss: 0.007394\n",
            "I Training epoch 165...\n",
            "I Finished training epoch 165 - loss: 0.010644\n",
            "I Training epoch 166...\n",
            "I Finished training epoch 166 - loss: 0.010917\n",
            "I Training epoch 167...\n",
            "I Finished training epoch 167 - loss: 0.007158\n",
            "I Training epoch 168...\n",
            "I Finished training epoch 168 - loss: 0.009744\n",
            "I Training epoch 169...\n",
            "I Finished training epoch 169 - loss: 0.007400\n",
            "I Training epoch 170...\n",
            "I Finished training epoch 170 - loss: 0.017977\n",
            "I Training epoch 171...\n",
            "I Finished training epoch 171 - loss: 0.009240\n",
            "I Training epoch 172...\n",
            "I Finished training epoch 172 - loss: 0.011408\n",
            "I Training epoch 173...\n",
            "I Finished training epoch 173 - loss: 0.009970\n",
            "I Training epoch 174...\n",
            "I Finished training epoch 174 - loss: 0.010859\n",
            "I Training epoch 175...\n",
            "I Finished training epoch 175 - loss: 0.006754\n",
            "I Training epoch 176...\n",
            "I Finished training epoch 176 - loss: 0.008361\n",
            "I Training epoch 177...\n",
            "I Finished training epoch 177 - loss: 0.013229\n",
            "I Training epoch 178...\n",
            "I Finished training epoch 178 - loss: 0.005969\n",
            "I Training epoch 179...\n",
            "I Finished training epoch 179 - loss: 0.007500\n",
            "I Training epoch 180...\n",
            "I Finished training epoch 180 - loss: 0.006342\n",
            "I Training epoch 181...\n",
            "I Finished training epoch 181 - loss: 0.006450\n",
            "I Training epoch 182...\n",
            "I Finished training epoch 182 - loss: 0.005212\n",
            "I Training epoch 183...\n",
            "I Finished training epoch 183 - loss: 0.007276\n",
            "I Training epoch 184...\n",
            "I Finished training epoch 184 - loss: 0.007353\n",
            "I Training epoch 185...\n",
            "I Finished training epoch 185 - loss: 0.014647\n",
            "I Training epoch 186...\n",
            "I Finished training epoch 186 - loss: 0.007590\n",
            "I Training epoch 187...\n",
            "I Finished training epoch 187 - loss: 0.006516\n",
            "I Training epoch 188...\n",
            "I Finished training epoch 188 - loss: 0.011102\n",
            "I Training epoch 189...\n",
            "I Finished training epoch 189 - loss: 0.007505\n",
            "I Training epoch 190...\n",
            "I Finished training epoch 190 - loss: 0.011188\n",
            "I Training epoch 191...\n",
            "I Finished training epoch 191 - loss: 0.010687\n",
            "I Training epoch 192...\n",
            "I Finished training epoch 192 - loss: 0.008396\n",
            "I Training epoch 193...\n",
            "I Finished training epoch 193 - loss: 0.009167\n",
            "I Training epoch 194...\n",
            "I Finished training epoch 194 - loss: 0.007779\n",
            "I Training epoch 195...\n",
            "I Finished training epoch 195 - loss: 0.007498\n",
            "I Training epoch 196...\n",
            "I Finished training epoch 196 - loss: 0.008849\n",
            "I Training epoch 197...\n",
            "I Finished training epoch 197 - loss: 0.010302\n",
            "I Training epoch 198...\n",
            "I Finished training epoch 198 - loss: 0.008088\n",
            "I Training epoch 199...\n",
            "I Finished training epoch 199 - loss: 0.009218\n",
            "I FINISHED optimization in 0:00:46.060114\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66ed7456a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66ed7456a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0530 22:07:43.596881 140080214624128 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66ed7456a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f66ed7456a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1400\n",
            "I0530 22:07:43.707473 140080214624128 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1400\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-1400, step 1400\n",
            "Testing model on data/ldc93s1/ldc93s1.csv\n",
            "I Test epoch...\n",
            "Test on data/ldc93s1/ldc93s1.csv - WER: 0.000000, CER: 0.000000, loss: 0.001733\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.001733\n",
            " - wav: file:///content/drive/My Drive/deepspeech/DeepSpeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}