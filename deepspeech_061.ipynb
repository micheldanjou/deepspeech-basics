{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepspeech-061.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0hISRSClnVanLJMasVJz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micheldanjou/deepspeech-basics/blob/master/deepspeech_061.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFlaFG68FzAl",
        "colab_type": "text"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-9Df-plEmy",
        "colab_type": "code",
        "outputId": "d9bf2300-c943-48ea-8b0b-d1539537ad25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!apt-get update && apt-get upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Wa\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rGet:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 3s (95.9 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  r-cran-plogr\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages have been kept back:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "  r-cran-dbplyr\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A785b3dmJ8J",
        "colab_type": "code",
        "outputId": "fe0271e0-9117-44fd-dfd9-877923c46561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!apt-get install git-lfs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  r-cran-plogr\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTMfxJQRYQHs",
        "colab_type": "text"
      },
      "source": [
        "# Download the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNwWRiS-8bsG",
        "colab_type": "code",
        "outputId": "a092d6a7-bcdb-44d7-964f-ca445b796851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puRG2nxgN-bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete the DeepSpeech git repo if cleanup needed\n",
        "os.chdir('/content/drive/My Drive/deepspeech/')\n",
        "!rm -rf ./DeepSpeech/\n",
        "!rm -rf ./deepspeech-0.6.1-checkpoint\n",
        "!rm -rf ./deepspeech-0.6.1-models\n",
        "!rm -rf ./audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjcScZrq_WQv",
        "colab_type": "code",
        "outputId": "eb03df9e-def4-40f2-9f11-624273e4c7ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-models.tar.gz\n",
        "!tar -xvf deepspeech-0.6.1-models.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 22:29:23--  https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-models.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/f29e6300-33cd-11ea-8523-3fc40b31be9a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T222924Z&X-Amz-Expires=300&X-Amz-Signature=7fb74378e62bba67d74f66473d405385fceeca7c859ce8b9179cbcf286af6ff3&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.6.1-models.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-05-30 22:29:24--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/f29e6300-33cd-11ea-8523-3fc40b31be9a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T222924Z&X-Amz-Expires=300&X-Amz-Signature=7fb74378e62bba67d74f66473d405385fceeca7c859ce8b9179cbcf286af6ff3&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.6.1-models.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.170.227\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.170.227|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1229020343 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘deepspeech-0.6.1-models.tar.gz’\n",
            "\n",
            "deepspeech-0.6.1-mo 100%[===================>]   1.14G  13.9MB/s    in 79s     \n",
            "\n",
            "2020-05-30 22:30:43 (14.9 MB/s) - ‘deepspeech-0.6.1-models.tar.gz’ saved [1229020343/1229020343]\n",
            "\n",
            "._deepspeech-0.6.1-models\n",
            "deepspeech-0.6.1-models/\n",
            "deepspeech-0.6.1-models/._lm.binary\n",
            "deepspeech-0.6.1-models/lm.binary\n",
            "deepspeech-0.6.1-models/._output_graph.pbmm\n",
            "deepspeech-0.6.1-models/output_graph.pbmm\n",
            "deepspeech-0.6.1-models/._output_graph.pb\n",
            "deepspeech-0.6.1-models/output_graph.pb\n",
            "deepspeech-0.6.1-models/._trie\n",
            "deepspeech-0.6.1-models/trie\n",
            "deepspeech-0.6.1-models/output_graph.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xFEX4lC_sO-",
        "colab_type": "code",
        "outputId": "18f6e194-449c-40d5-9af2-6a6e6580ca5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-checkpoint.tar.gz\n",
        "!tar -xvf deepspeech-0.6.1-checkpoint.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 22:31:18--  https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-checkpoint.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/4fe5cb00-1db4-11ea-998b-9afabbb9218b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T223118Z&X-Amz-Expires=300&X-Amz-Signature=80c6f3d247035ef6015bfda6837bad4b896530fb4202dfde82346ad34d96c025&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.6.1-checkpoint.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-05-30 22:31:19--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/4fe5cb00-1db4-11ea-998b-9afabbb9218b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T223118Z&X-Amz-Expires=300&X-Amz-Signature=80c6f3d247035ef6015bfda6837bad4b896530fb4202dfde82346ad34d96c025&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.6.1-checkpoint.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.18.164\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.18.164|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 642672227 (613M) [application/octet-stream]\n",
            "Saving to: ‘deepspeech-0.6.1-checkpoint.tar.gz’\n",
            "\n",
            "deepspeech-0.6.1-ch 100%[===================>] 612.90M  15.8MB/s    in 44s     \n",
            "\n",
            "2020-05-30 22:32:03 (14.0 MB/s) - ‘deepspeech-0.6.1-checkpoint.tar.gz’ saved [642672227/642672227]\n",
            "\n",
            "deepspeech-0.6.1-checkpoint/\n",
            "deepspeech-0.6.1-checkpoint/checkpoint\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.data-00000-of-00001\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.index\n",
            "deepspeech-0.6.1-checkpoint/best_dev_checkpoint\n",
            "deepspeech-0.6.1-checkpoint/best_dev-233784.meta\n",
            "deepspeech-0.6.1-checkpoint/alphabet.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV7eSSO2XRII",
        "colab_type": "code",
        "outputId": "27c62da4-5f10-46f2-fa16-76cf28151cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/audio-0.6.1.tar.gz\n",
        "!tar -xvf audio-0.6.1.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 22:32:20--  https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/audio-0.6.1.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/4fe5cb00-1db4-11ea-9885-cab10c27c3f5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T223221Z&X-Amz-Expires=300&X-Amz-Signature=850387d77d49369e3d9a593a73b162f92a8ee915c0a220bf518316f53b2e736e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Daudio-0.6.1.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-05-30 22:32:21--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/4fe5cb00-1db4-11ea-9885-cab10c27c3f5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T223221Z&X-Amz-Expires=300&X-Amz-Signature=850387d77d49369e3d9a593a73b162f92a8ee915c0a220bf518316f53b2e736e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Daudio-0.6.1.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.25.100\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.25.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 197168 (193K) [application/octet-stream]\n",
            "Saving to: ‘audio-0.6.1.tar.gz’\n",
            "\n",
            "audio-0.6.1.tar.gz  100%[===================>] 192.55K   347KB/s    in 0.6s    \n",
            "\n",
            "2020-05-30 22:32:22 (347 KB/s) - ‘audio-0.6.1.tar.gz’ saved [197168/197168]\n",
            "\n",
            "audio/\n",
            "audio/2830-3980-0043.wav\n",
            "audio/Attribution.txt\n",
            "audio/4507-16021-0012.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/License.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_DvwwCI56c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip training data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9zzuhpoYMfZ",
        "colab_type": "text"
      },
      "source": [
        "# Clone the DeepSpeech git repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFgAWobHKls6",
        "colab_type": "code",
        "outputId": "97c104e3-2a62-4650-8bcc-eb52a7e41619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!git lfs clone --branch v0.6.1 https://github.com/mozilla/DeepSpeech.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18527 (delta 15), reused 24 (delta 14), pack-reused 18496\u001b[K\n",
            "Receiving objects: 100% (18527/18527), 47.68 MiB | 8.10 MiB/s, done.\n",
            "Resolving deltas: 100% (12605/12605), done.\n",
            "Note: checking out '3df20fee52fda47d08e3726fd0da86dbb414e9d8'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (1819/1819), done.\n",
            "Git LFS: (2 of 2 files) 913.52 MB / 913.52 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwMFqx-cYtap",
        "colab_type": "text"
      },
      "source": [
        "# Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blRqtlTUJAVx",
        "colab_type": "code",
        "outputId": "aa668c35-c166-4c31-f683-2a10b706bb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech')\n",
        "!pip3 install $(python3 util/taskcluster.py --decoder)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ds-ctcdecoder==0.6.1 from https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.6.1.cpu-ctc/artifacts/public/ds_ctcdecoder-0.6.1-cp36-cp36m-manylinux1_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from ds-ctcdecoder==0.6.1) (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1uXzZ2xLcdF",
        "colab_type": "code",
        "outputId": "0ad8db08-0695-4d28-e0bb-485de704b9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip3 install deepspeech==0.6.1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepspeech==0.6.1 in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech==0.6.1) (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svWduKROMR9n",
        "colab_type": "code",
        "outputId": "1e765b42-e813-40e9-eb0f-4f64bc966298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.15.4)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.47.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.13.0)\n",
            "Requirement already satisfied: pyxdg in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.26)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (46.4.0)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (2.0.10)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (1.3.7)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (2.23.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.6.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (0.10.3.post1)\n",
            "Requirement already satisfied: paramiko>=2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (2.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (3.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 2)) (1.29.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->-r requirements.txt (line 19)) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 20)) (3.0.4)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (2.1.8)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.22.2.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.15.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa->-r requirements.txt (line 21)) (0.48.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->-r requirements.txt (line 22)) (1.14.0)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (3.1.7)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (2.9.2)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko>=2.1->-r requirements.txt (line 25)) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 27)) (1.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa->-r requirements.txt (line 21)) (0.31.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 22)) (2.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 2)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAv0mjvU0Gk0",
        "colab_type": "text"
      },
      "source": [
        "# Transcription example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9GxebSh1Kez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "40f52ec6-4155-466c-b8c2-82af88287798"
      },
      "source": [
        "!deepspeech --h"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: deepspeech [-h] --model MODEL [--lm [LM]] [--trie [TRIE]] --audio AUDIO\n",
            "                  [--beam_width BEAM_WIDTH] [--lm_alpha LM_ALPHA]\n",
            "                  [--lm_beta LM_BETA] [--version] [--extended] [--json]\n",
            "\n",
            "Running DeepSpeech inference.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         Path to the model (protocol buffer binary file)\n",
            "  --lm [LM]             Path to the language model binary file\n",
            "  --trie [TRIE]         Path to the language model trie file created with\n",
            "                        native_client/generate_trie\n",
            "  --audio AUDIO         Path to the audio file to run (WAV format)\n",
            "  --beam_width BEAM_WIDTH\n",
            "                        Beam width for the CTC decoder\n",
            "  --lm_alpha LM_ALPHA   Language model weight (lm_alpha)\n",
            "  --lm_beta LM_BETA     Word insertion bonus (lm_beta)\n",
            "  --version             Print version and exits\n",
            "  --extended            Output string from extended metadata\n",
            "  --json                Output json from metadata with timestamp of each word\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz5eGTswzbfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L38Om0OKyQoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5bc32004-69af-4c3a-fe11-4642012827d4"
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.1-models/output_graph.pbmm --lm deepspeech-0.6.1-models/lm.binary --trie deepspeech-0.6.1-models/trie --audio audio/2830-3980-0043.wav"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.1-models/output_graph.pbmm\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.1-0-g3df20fe\n",
            "2020-05-30 22:59:04.337278: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.0732s.\n",
            "Loading language model from files deepspeech-0.6.1-models/lm.binary deepspeech-0.6.1-models/trie\n",
            "Loaded language model in 0.0387s.\n",
            "Running inference.\n",
            "experience proof less\n",
            "Inference took 9.006s for 1.975s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asWqSsrt0bC2",
        "colab_type": "text"
      },
      "source": [
        "I believe `deespeech` is a wrapper around `DeepSpeech.py`. \n",
        "\n",
        "The `DeepSpeech.py` help page provides a complete list of all the options available.\n",
        "\n",
        "```!python3  ./DeepSpeech.py --helpfull```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9XgWizdNcp",
        "colab_type": "text"
      },
      "source": [
        "# Training a network\n",
        "## Training a network using Mozilla's example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcB3pDlQ_6t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZsOqfy7dSEG",
        "colab_type": "code",
        "outputId": "662ce162-38d7-4c5a-c41c-fcf39fd62be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!bash ./bin/run-ldc93s1.sh"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ '[' '!' -f DeepSpeech.py ']'\n",
            "+ '[' '!' -f data/ldc93s1/ldc93s1.csv ']'\n",
            "+ echo 'Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.'\n",
            "Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "+ python -u bin/import_ldc93s1.py ./data/ldc93s1\n",
            "No path \"./data/ldc93s1\" - creating ...\n",
            "No archive \"./data/ldc93s1/LDC93S1.wav\" - downloading...\n",
            "Progress |                                                      | N/A% completedNo archive \"./data/ldc93s1/LDC93S1.txt\" - downloading...\n",
            "Progress |######################################################| 100% completed\n",
            "Progress |######################################################| 100% completed\n",
            "+ '[' -d '' ']'\n",
            "++ python -c 'from xdg import BaseDirectory as xdg; print(xdg.save_data_path(\"deepspeech/ldc93s1\"))'\n",
            "+ checkpoint_dir=/root/.local/share/deepspeech/ldc93s1\n",
            "+ export CUDA_VISIBLE_DEVICES=0\n",
            "+ CUDA_VISIBLE_DEVICES=0\n",
            "+ python -u DeepSpeech.py --noshow_progressbar --train_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --test_batch_size 1 --n_hidden 100 --epochs 200 --checkpoint_dir /root/.local/share/deepspeech/ldc93s1\n",
            "/bin/sh: 1: sox: not found\n",
            "SoX could not be found!\n",
            "\n",
            "    If you do not have SoX, proceed here:\n",
            "     - - - http://sox.sourceforge.net/ - - -\n",
            "\n",
            "    If you do (or think that you should) have SoX, double-check your\n",
            "    path variables.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0530 22:36:03.016537 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "W0530 22:36:03.260209 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "W0530 22:36:03.260698 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "W0530 22:36:03.260960 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0530 22:36:03.458759 140266782492544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f92221f2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f92221f2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0530 22:36:03.517994 140266782492544 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f92221f2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f92221f2cf8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0530 22:36:03.674461 140266782492544 deprecation.py:323] From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0530 22:36:04.870562 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1400\n",
            "I0530 22:36:04.876060 140266782492544 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1400\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-1400, step 1400\n",
            "I STARTING Optimization\n",
            "I Training epoch 0...\n",
            "I Finished training epoch 0 - loss: 0.008439\n",
            "I Training epoch 1...\n",
            "I Finished training epoch 1 - loss: 0.006878\n",
            "I Training epoch 2...\n",
            "I Finished training epoch 2 - loss: 0.009603\n",
            "I Training epoch 3...\n",
            "I Finished training epoch 3 - loss: 0.008694\n",
            "I Training epoch 4...\n",
            "I Finished training epoch 4 - loss: 0.008561\n",
            "I Training epoch 5...\n",
            "I Finished training epoch 5 - loss: 0.006241\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0530 22:36:07.180014 140266782492544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I Training epoch 6...\n",
            "I Finished training epoch 6 - loss: 0.007270\n",
            "I Training epoch 7...\n",
            "I Finished training epoch 7 - loss: 0.010171\n",
            "I Training epoch 8...\n",
            "I Finished training epoch 8 - loss: 0.012119\n",
            "I Training epoch 9...\n",
            "I Finished training epoch 9 - loss: 0.009534\n",
            "I Training epoch 10...\n",
            "I Finished training epoch 10 - loss: 0.013354\n",
            "I Training epoch 11...\n",
            "I Finished training epoch 11 - loss: 0.005530\n",
            "I Training epoch 12...\n",
            "I Finished training epoch 12 - loss: 0.004620\n",
            "I Training epoch 13...\n",
            "I Finished training epoch 13 - loss: 0.008706\n",
            "I Training epoch 14...\n",
            "I Finished training epoch 14 - loss: 0.006603\n",
            "I Training epoch 15...\n",
            "I Finished training epoch 15 - loss: 0.009684\n",
            "I Training epoch 16...\n",
            "I Finished training epoch 16 - loss: 0.006637\n",
            "I Training epoch 17...\n",
            "I Finished training epoch 17 - loss: 0.008916\n",
            "I Training epoch 18...\n",
            "I Finished training epoch 18 - loss: 0.013078\n",
            "I Training epoch 19...\n",
            "I Finished training epoch 19 - loss: 0.007017\n",
            "I Training epoch 20...\n",
            "I Finished training epoch 20 - loss: 0.008181\n",
            "I Training epoch 21...\n",
            "I Finished training epoch 21 - loss: 0.008109\n",
            "I Training epoch 22...\n",
            "I Finished training epoch 22 - loss: 0.005362\n",
            "I Training epoch 23...\n",
            "I Finished training epoch 23 - loss: 0.010026\n",
            "I Training epoch 24...\n",
            "I Finished training epoch 24 - loss: 0.009687\n",
            "I Training epoch 25...\n",
            "I Finished training epoch 25 - loss: 0.013793\n",
            "I Training epoch 26...\n",
            "I Finished training epoch 26 - loss: 0.007066\n",
            "I Training epoch 27...\n",
            "I Finished training epoch 27 - loss: 0.005632\n",
            "I Training epoch 28...\n",
            "I Finished training epoch 28 - loss: 0.011923\n",
            "I Training epoch 29...\n",
            "I Finished training epoch 29 - loss: 0.006488\n",
            "I Training epoch 30...\n",
            "I Finished training epoch 30 - loss: 0.014416\n",
            "I Training epoch 31...\n",
            "I Finished training epoch 31 - loss: 0.007840\n",
            "I Training epoch 32...\n",
            "I Finished training epoch 32 - loss: 0.009677\n",
            "I Training epoch 33...\n",
            "I Finished training epoch 33 - loss: 0.006564\n",
            "I Training epoch 34...\n",
            "I Finished training epoch 34 - loss: 0.008237\n",
            "I Training epoch 35...\n",
            "I Finished training epoch 35 - loss: 0.007915\n",
            "I Training epoch 36...\n",
            "I Finished training epoch 36 - loss: 0.004539\n",
            "I Training epoch 37...\n",
            "I Finished training epoch 37 - loss: 0.011611\n",
            "I Training epoch 38...\n",
            "I Finished training epoch 38 - loss: 0.008646\n",
            "I Training epoch 39...\n",
            "I Finished training epoch 39 - loss: 0.008493\n",
            "I Training epoch 40...\n",
            "I Finished training epoch 40 - loss: 0.006069\n",
            "I Training epoch 41...\n",
            "I Finished training epoch 41 - loss: 0.004732\n",
            "I Training epoch 42...\n",
            "I Finished training epoch 42 - loss: 0.006822\n",
            "I Training epoch 43...\n",
            "I Finished training epoch 43 - loss: 0.006119\n",
            "I Training epoch 44...\n",
            "I Finished training epoch 44 - loss: 0.007632\n",
            "I Training epoch 45...\n",
            "I Finished training epoch 45 - loss: 0.007174\n",
            "I Training epoch 46...\n",
            "I Finished training epoch 46 - loss: 0.006315\n",
            "I Training epoch 47...\n",
            "I Finished training epoch 47 - loss: 0.010947\n",
            "I Training epoch 48...\n",
            "I Finished training epoch 48 - loss: 0.006317\n",
            "I Training epoch 49...\n",
            "I Finished training epoch 49 - loss: 0.006445\n",
            "I Training epoch 50...\n",
            "I Finished training epoch 50 - loss: 0.007632\n",
            "I Training epoch 51...\n",
            "I Finished training epoch 51 - loss: 0.009287\n",
            "I Training epoch 52...\n",
            "I Finished training epoch 52 - loss: 0.006571\n",
            "I Training epoch 53...\n",
            "I Finished training epoch 53 - loss: 0.009931\n",
            "I Training epoch 54...\n",
            "I Finished training epoch 54 - loss: 0.006987\n",
            "I Training epoch 55...\n",
            "I Finished training epoch 55 - loss: 0.012556\n",
            "I Training epoch 56...\n",
            "I Finished training epoch 56 - loss: 0.008853\n",
            "I Training epoch 57...\n",
            "I Finished training epoch 57 - loss: 0.006546\n",
            "I Training epoch 58...\n",
            "I Finished training epoch 58 - loss: 0.007624\n",
            "I Training epoch 59...\n",
            "I Finished training epoch 59 - loss: 0.010390\n",
            "I Training epoch 60...\n",
            "I Finished training epoch 60 - loss: 0.010738\n",
            "I Training epoch 61...\n",
            "I Finished training epoch 61 - loss: 0.006810\n",
            "I Training epoch 62...\n",
            "I Finished training epoch 62 - loss: 0.008044\n",
            "I Training epoch 63...\n",
            "I Finished training epoch 63 - loss: 0.009105\n",
            "I Training epoch 64...\n",
            "I Finished training epoch 64 - loss: 0.011206\n",
            "I Training epoch 65...\n",
            "I Finished training epoch 65 - loss: 0.006756\n",
            "I Training epoch 66...\n",
            "I Finished training epoch 66 - loss: 0.008564\n",
            "I Training epoch 67...\n",
            "I Finished training epoch 67 - loss: 0.006150\n",
            "I Training epoch 68...\n",
            "I Finished training epoch 68 - loss: 0.006990\n",
            "I Training epoch 69...\n",
            "I Finished training epoch 69 - loss: 0.005650\n",
            "I Training epoch 70...\n",
            "I Finished training epoch 70 - loss: 0.006563\n",
            "I Training epoch 71...\n",
            "I Finished training epoch 71 - loss: 0.006612\n",
            "I Training epoch 72...\n",
            "I Finished training epoch 72 - loss: 0.008970\n",
            "I Training epoch 73...\n",
            "I Finished training epoch 73 - loss: 0.005151\n",
            "I Training epoch 74...\n",
            "I Finished training epoch 74 - loss: 0.007002\n",
            "I Training epoch 75...\n",
            "I Finished training epoch 75 - loss: 0.007607\n",
            "I Training epoch 76...\n",
            "I Finished training epoch 76 - loss: 0.005795\n",
            "I Training epoch 77...\n",
            "I Finished training epoch 77 - loss: 0.004990\n",
            "I Training epoch 78...\n",
            "I Finished training epoch 78 - loss: 0.009067\n",
            "I Training epoch 79...\n",
            "I Finished training epoch 79 - loss: 0.006173\n",
            "I Training epoch 80...\n",
            "I Finished training epoch 80 - loss: 0.008106\n",
            "I Training epoch 81...\n",
            "I Finished training epoch 81 - loss: 0.006022\n",
            "I Training epoch 82...\n",
            "I Finished training epoch 82 - loss: 0.005590\n",
            "I Training epoch 83...\n",
            "I Finished training epoch 83 - loss: 0.008237\n",
            "I Training epoch 84...\n",
            "I Finished training epoch 84 - loss: 0.009061\n",
            "I Training epoch 85...\n",
            "I Finished training epoch 85 - loss: 0.005419\n",
            "I Training epoch 86...\n",
            "I Finished training epoch 86 - loss: 0.007001\n",
            "I Training epoch 87...\n",
            "I Finished training epoch 87 - loss: 0.006545\n",
            "I Training epoch 88...\n",
            "I Finished training epoch 88 - loss: 0.007745\n",
            "I Training epoch 89...\n",
            "I Finished training epoch 89 - loss: 0.007802\n",
            "I Training epoch 90...\n",
            "I Finished training epoch 90 - loss: 0.005992\n",
            "I Training epoch 91...\n",
            "I Finished training epoch 91 - loss: 0.006779\n",
            "I Training epoch 92...\n",
            "I Finished training epoch 92 - loss: 0.004719\n",
            "I Training epoch 93...\n",
            "I Finished training epoch 93 - loss: 0.010392\n",
            "I Training epoch 94...\n",
            "I Finished training epoch 94 - loss: 0.006125\n",
            "I Training epoch 95...\n",
            "I Finished training epoch 95 - loss: 0.005635\n",
            "I Training epoch 96...\n",
            "I Finished training epoch 96 - loss: 0.004951\n",
            "I Training epoch 97...\n",
            "I Finished training epoch 97 - loss: 0.004442\n",
            "I Training epoch 98...\n",
            "I Finished training epoch 98 - loss: 0.006763\n",
            "I Training epoch 99...\n",
            "I Finished training epoch 99 - loss: 0.009584\n",
            "I Training epoch 100...\n",
            "I Finished training epoch 100 - loss: 0.009896\n",
            "I Training epoch 101...\n",
            "I Finished training epoch 101 - loss: 0.006481\n",
            "I Training epoch 102...\n",
            "I Finished training epoch 102 - loss: 0.011935\n",
            "I Training epoch 103...\n",
            "I Finished training epoch 103 - loss: 0.007950\n",
            "I Training epoch 104...\n",
            "I Finished training epoch 104 - loss: 0.007919\n",
            "I Training epoch 105...\n",
            "I Finished training epoch 105 - loss: 0.005597\n",
            "I Training epoch 106...\n",
            "I Finished training epoch 106 - loss: 0.004358\n",
            "I Training epoch 107...\n",
            "I Finished training epoch 107 - loss: 0.005255\n",
            "I Training epoch 108...\n",
            "I Finished training epoch 108 - loss: 0.005996\n",
            "I Training epoch 109...\n",
            "I Finished training epoch 109 - loss: 0.006734\n",
            "I Training epoch 110...\n",
            "I Finished training epoch 110 - loss: 0.005664\n",
            "I Training epoch 111...\n",
            "I Finished training epoch 111 - loss: 0.008060\n",
            "I Training epoch 112...\n",
            "I Finished training epoch 112 - loss: 0.009369\n",
            "I Training epoch 113...\n",
            "I Finished training epoch 113 - loss: 0.007006\n",
            "I Training epoch 114...\n",
            "I Finished training epoch 114 - loss: 0.006901\n",
            "I Training epoch 115...\n",
            "I Finished training epoch 115 - loss: 0.006234\n",
            "I Training epoch 116...\n",
            "I Finished training epoch 116 - loss: 0.009467\n",
            "I Training epoch 117...\n",
            "I Finished training epoch 117 - loss: 0.005321\n",
            "I Training epoch 118...\n",
            "I Finished training epoch 118 - loss: 0.006434\n",
            "I Training epoch 119...\n",
            "I Finished training epoch 119 - loss: 0.008309\n",
            "I Training epoch 120...\n",
            "I Finished training epoch 120 - loss: 0.005254\n",
            "I Training epoch 121...\n",
            "I Finished training epoch 121 - loss: 0.005405\n",
            "I Training epoch 122...\n",
            "I Finished training epoch 122 - loss: 0.004982\n",
            "I Training epoch 123...\n",
            "I Finished training epoch 123 - loss: 0.007544\n",
            "I Training epoch 124...\n",
            "I Finished training epoch 124 - loss: 0.004753\n",
            "I Training epoch 125...\n",
            "I Finished training epoch 125 - loss: 0.006905\n",
            "I Training epoch 126...\n",
            "I Finished training epoch 126 - loss: 0.007270\n",
            "I Training epoch 127...\n",
            "I Finished training epoch 127 - loss: 0.005944\n",
            "I Training epoch 128...\n",
            "I Finished training epoch 128 - loss: 0.007553\n",
            "I Training epoch 129...\n",
            "I Finished training epoch 129 - loss: 0.006738\n",
            "I Training epoch 130...\n",
            "I Finished training epoch 130 - loss: 0.004812\n",
            "I Training epoch 131...\n",
            "I Finished training epoch 131 - loss: 0.015791\n",
            "I Training epoch 132...\n",
            "I Finished training epoch 132 - loss: 0.017738\n",
            "I Training epoch 133...\n",
            "I Finished training epoch 133 - loss: 0.005015\n",
            "I Training epoch 134...\n",
            "I Finished training epoch 134 - loss: 0.012943\n",
            "I Training epoch 135...\n",
            "I Finished training epoch 135 - loss: 0.009980\n",
            "I Training epoch 136...\n",
            "I Finished training epoch 136 - loss: 0.007368\n",
            "I Training epoch 137...\n",
            "I Finished training epoch 137 - loss: 0.006927\n",
            "I Training epoch 138...\n",
            "I Finished training epoch 138 - loss: 0.007068\n",
            "I Training epoch 139...\n",
            "I Finished training epoch 139 - loss: 0.006225\n",
            "I Training epoch 140...\n",
            "I Finished training epoch 140 - loss: 0.010122\n",
            "I Training epoch 141...\n",
            "I Finished training epoch 141 - loss: 0.007240\n",
            "I Training epoch 142...\n",
            "I Finished training epoch 142 - loss: 0.008288\n",
            "I Training epoch 143...\n",
            "I Finished training epoch 143 - loss: 0.004382\n",
            "I Training epoch 144...\n",
            "I Finished training epoch 144 - loss: 0.005406\n",
            "I Training epoch 145...\n",
            "I Finished training epoch 145 - loss: 0.007701\n",
            "I Training epoch 146...\n",
            "I Finished training epoch 146 - loss: 0.005478\n",
            "I Training epoch 147...\n",
            "I Finished training epoch 147 - loss: 0.006066\n",
            "I Training epoch 148...\n",
            "I Finished training epoch 148 - loss: 0.008355\n",
            "I Training epoch 149...\n",
            "I Finished training epoch 149 - loss: 0.007275\n",
            "I Training epoch 150...\n",
            "I Finished training epoch 150 - loss: 0.003935\n",
            "I Training epoch 151...\n",
            "I Finished training epoch 151 - loss: 0.005500\n",
            "I Training epoch 152...\n",
            "I Finished training epoch 152 - loss: 0.004898\n",
            "I Training epoch 153...\n",
            "I Finished training epoch 153 - loss: 0.012206\n",
            "I Training epoch 154...\n",
            "I Finished training epoch 154 - loss: 0.006129\n",
            "I Training epoch 155...\n",
            "I Finished training epoch 155 - loss: 0.006801\n",
            "I Training epoch 156...\n",
            "I Finished training epoch 156 - loss: 0.008639\n",
            "I Training epoch 157...\n",
            "I Finished training epoch 157 - loss: 0.006164\n",
            "I Training epoch 158...\n",
            "I Finished training epoch 158 - loss: 0.005834\n",
            "I Training epoch 159...\n",
            "I Finished training epoch 159 - loss: 0.006080\n",
            "I Training epoch 160...\n",
            "I Finished training epoch 160 - loss: 0.004698\n",
            "I Training epoch 161...\n",
            "I Finished training epoch 161 - loss: 0.003758\n",
            "I Training epoch 162...\n",
            "I Finished training epoch 162 - loss: 0.003979\n",
            "I Training epoch 163...\n",
            "I Finished training epoch 163 - loss: 0.006620\n",
            "I Training epoch 164...\n",
            "I Finished training epoch 164 - loss: 0.005588\n",
            "I Training epoch 165...\n",
            "I Finished training epoch 165 - loss: 0.007869\n",
            "I Training epoch 166...\n",
            "I Finished training epoch 166 - loss: 0.007668\n",
            "I Training epoch 167...\n",
            "I Finished training epoch 167 - loss: 0.005418\n",
            "I Training epoch 168...\n",
            "I Finished training epoch 168 - loss: 0.007433\n",
            "I Training epoch 169...\n",
            "I Finished training epoch 169 - loss: 0.005342\n",
            "I Training epoch 170...\n",
            "I Finished training epoch 170 - loss: 0.012811\n",
            "I Training epoch 171...\n",
            "I Finished training epoch 171 - loss: 0.006786\n",
            "I Training epoch 172...\n",
            "I Finished training epoch 172 - loss: 0.008458\n",
            "I Training epoch 173...\n",
            "I Finished training epoch 173 - loss: 0.007256\n",
            "I Training epoch 174...\n",
            "I Finished training epoch 174 - loss: 0.007508\n",
            "I Training epoch 175...\n",
            "I Finished training epoch 175 - loss: 0.004970\n",
            "I Training epoch 176...\n",
            "I Finished training epoch 176 - loss: 0.006263\n",
            "I Training epoch 177...\n",
            "I Finished training epoch 177 - loss: 0.009672\n",
            "I Training epoch 178...\n",
            "I Finished training epoch 178 - loss: 0.004232\n",
            "I Training epoch 179...\n",
            "I Finished training epoch 179 - loss: 0.005728\n",
            "I Training epoch 180...\n",
            "I Finished training epoch 180 - loss: 0.004674\n",
            "I Training epoch 181...\n",
            "I Finished training epoch 181 - loss: 0.004853\n",
            "I Training epoch 182...\n",
            "I Finished training epoch 182 - loss: 0.003710\n",
            "I Training epoch 183...\n",
            "I Finished training epoch 183 - loss: 0.005336\n",
            "I Training epoch 184...\n",
            "I Finished training epoch 184 - loss: 0.005400\n",
            "I Training epoch 185...\n",
            "I Finished training epoch 185 - loss: 0.010777\n",
            "I Training epoch 186...\n",
            "I Finished training epoch 186 - loss: 0.005808\n",
            "I Training epoch 187...\n",
            "I Finished training epoch 187 - loss: 0.004863\n",
            "I Training epoch 188...\n",
            "I Finished training epoch 188 - loss: 0.008107\n",
            "I Training epoch 189...\n",
            "I Finished training epoch 189 - loss: 0.005613\n",
            "I Training epoch 190...\n",
            "I Finished training epoch 190 - loss: 0.008169\n",
            "I Training epoch 191...\n",
            "I Finished training epoch 191 - loss: 0.008244\n",
            "I Training epoch 192...\n",
            "I Finished training epoch 192 - loss: 0.005989\n",
            "I Training epoch 193...\n",
            "I Finished training epoch 193 - loss: 0.006484\n",
            "I Training epoch 194...\n",
            "I Finished training epoch 194 - loss: 0.005829\n",
            "I Training epoch 195...\n",
            "I Finished training epoch 195 - loss: 0.005687\n",
            "I Training epoch 196...\n",
            "I Finished training epoch 196 - loss: 0.006483\n",
            "I Training epoch 197...\n",
            "I Finished training epoch 197 - loss: 0.008059\n",
            "I Training epoch 198...\n",
            "I Finished training epoch 198 - loss: 0.005850\n",
            "I Training epoch 199...\n",
            "I Finished training epoch 199 - loss: 0.006886\n",
            "I FINISHED optimization in 0:00:50.259035\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f925dc416a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f925dc416a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0530 22:36:55.671426 140266782492544 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f925dc416a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f925dc416a0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1600\n",
            "I0530 22:36:55.790268 140266782492544 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-1600\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-1600, step 1600\n",
            "Testing model on data/ldc93s1/ldc93s1.csv\n",
            "I Test epoch...\n",
            "Test on data/ldc93s1/ldc93s1.csv - WER: 0.000000, CER: 0.000000, loss: 0.001224\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.001224\n",
            " - wav: file:///content/drive/My Drive/deepspeech/DeepSpeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbs8ZjLmI_E1",
        "colab_type": "text"
      },
      "source": [
        "## Training a network using my own files\n",
        "The aim is to train a network to recognise the antibiotic names such as \"amikacin\", \"aztreonam\", etc...\n",
        "In this attempt I am only using one wav file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKES293OwTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech/bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COrO6vddS0zX",
        "colab_type": "text"
      },
      "source": [
        "## Clone `deepspeech-learning` repo\n",
        "After cloning `deepspeech-learning` repo, we transfer the files needed for training to the DeepSpeech repo. The aim is to mimic as much as possible the work done by `run-ldc93s1.sh` \n",
        "  * run-antibiotics.sh -> DeepSpeech/bin/run-antibiotics.sh\n",
        "  * antibiotics.csv -> DeepSpeech/data/antibiotics/antibiotics.csv\n",
        "  * amikacin.wav -> DeepSpeech/data/antibitics/amikacin.wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPjFoqC5S-WZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OqsvIGdS6fm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bdfdbb1c-161d-46ef-abb2-78b64448df8d"
      },
      "source": [
        "!git clone https://github.com/micheldanjou/deepspeech-basics.git"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deepspeech-basics'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 45 (delta 19), reused 9 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LXtz8VTTO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/deepspeech/deepspeech-basics/run-antibiotics.sh' '/content/drive/My Drive/deepspeech/DeepSpeech/bin/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WN1-YkTwov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p '/content/drive/My Drive/deepspeech/DeepSpeech/data/antibiotics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm6SD-beUKki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/deepspeech/deepspeech-basics/antibiotics.csv' '/content/drive/My Drive/deepspeech/DeepSpeech/data/antibiotics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cP8Q7sOcX8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/deepspeech/deepspeech-basics/antibiotics.txt' '/content/drive/My Drive/deepspeech/DeepSpeech/data/antibiotics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh8-TjRsUekR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/deepspeech/deepspeech-basics/amikacin.wav' '/content/drive/My Drive/deepspeech/DeepSpeech/data/antibiotics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-fqZJX8Ux6s",
        "colab_type": "text"
      },
      "source": [
        "## Start training on antibiotic wav file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJV2MyC_U2Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/deepspeech/DeepSpeech')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtGv5gCDU621",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "382db57e-fe68-4c75-853a-ca12a6260d59"
      },
      "source": [
        "!bash ./bin/run-antibiotics.sh"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ '[' '!' -f DeepSpeech.py ']'\n",
            "+ '[' -d '' ']'\n",
            "++ python -c 'from xdg import BaseDirectory as xdg; print(xdg.save_data_path(\"deepspeech/ldc93s1\"))'\n",
            "+ checkpoint_dir=/root/.local/share/deepspeech/ldc93s1\n",
            "+ export CUDA_VISIBLE_DEVICES=0\n",
            "+ CUDA_VISIBLE_DEVICES=0\n",
            "+ python -u DeepSpeech.py --noshow_progressbar --train_files data/antibiotics/antibiotics.csv --test_files data/antibiotics/antibiotics.csv --train_batch_size 1 --test_batch_size 1 --n_hidden 100 --epochs 2000 --checkpoint_dir /root/.local/share/deepspeech/ldc93s1\n",
            "/bin/sh: 1: sox: not found\n",
            "SoX could not be found!\n",
            "\n",
            "    If you do not have SoX, proceed here:\n",
            "     - - - http://sox.sourceforge.net/ - - -\n",
            "\n",
            "    If you do (or think that you should) have SoX, double-check your\n",
            "    path variables.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0531 01:58:01.338448 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "W0531 01:58:01.551680 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:348: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "W0531 01:58:01.552015 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:349: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "W0531 01:58:01.552236 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:351: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0531 01:58:01.683766 139837890242432 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e4625efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e4625efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0531 01:58:01.723901 139837890242432 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e4625efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e4625efd0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0531 01:58:01.826237 139837890242432 deprecation.py:323] From DeepSpeech.py:236: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0531 01:58:02.670933 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-3600\n",
            "I0531 01:58:02.673852 139837890242432 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-3600\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-3600, step 3600\n",
            "I STARTING Optimization\n",
            "I Training epoch 0...\n",
            "I Finished training epoch 0 - loss: 0.069603\n",
            "I Training epoch 1...\n",
            "I Finished training epoch 1 - loss: 0.035582\n",
            "I Training epoch 2...\n",
            "I Finished training epoch 2 - loss: 0.091477\n",
            "I Training epoch 3...\n",
            "I Finished training epoch 3 - loss: 0.060818\n",
            "I Training epoch 4...\n",
            "I Finished training epoch 4 - loss: 0.069982\n",
            "I Training epoch 5...\n",
            "I Finished training epoch 5 - loss: 0.048406\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0531 01:58:04.714136 139837890242432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I Training epoch 6...\n",
            "I Finished training epoch 6 - loss: 0.079879\n",
            "I Training epoch 7...\n",
            "I Finished training epoch 7 - loss: 0.065404\n",
            "I Training epoch 8...\n",
            "I Finished training epoch 8 - loss: 0.134263\n",
            "I Training epoch 9...\n",
            "I Finished training epoch 9 - loss: 0.963506\n",
            "I Training epoch 10...\n",
            "I Finished training epoch 10 - loss: 0.994108\n",
            "I Training epoch 11...\n",
            "I Finished training epoch 11 - loss: 0.026633\n",
            "I Training epoch 12...\n",
            "I Finished training epoch 12 - loss: 0.027496\n",
            "I Training epoch 13...\n",
            "I Finished training epoch 13 - loss: 0.039807\n",
            "I Training epoch 14...\n",
            "I Finished training epoch 14 - loss: 0.034278\n",
            "I Training epoch 15...\n",
            "I Finished training epoch 15 - loss: 0.072024\n",
            "I Training epoch 16...\n",
            "I Finished training epoch 16 - loss: 0.132842\n",
            "I Training epoch 17...\n",
            "I Finished training epoch 17 - loss: 0.706076\n",
            "I Training epoch 18...\n",
            "I Finished training epoch 18 - loss: 0.434009\n",
            "I Training epoch 19...\n",
            "I Finished training epoch 19 - loss: 0.156450\n",
            "I Training epoch 20...\n",
            "I Finished training epoch 20 - loss: 0.069812\n",
            "I Training epoch 21...\n",
            "I Finished training epoch 21 - loss: 0.152984\n",
            "I Training epoch 22...\n",
            "I Finished training epoch 22 - loss: 0.444590\n",
            "I Training epoch 23...\n",
            "I Finished training epoch 23 - loss: 0.169744\n",
            "I Training epoch 24...\n",
            "I Finished training epoch 24 - loss: 0.044435\n",
            "I Training epoch 25...\n",
            "I Finished training epoch 25 - loss: 0.608439\n",
            "I Training epoch 26...\n",
            "I Finished training epoch 26 - loss: 0.529518\n",
            "I Training epoch 27...\n",
            "I Finished training epoch 27 - loss: 0.057523\n",
            "I Training epoch 28...\n",
            "I Finished training epoch 28 - loss: 0.205921\n",
            "I Training epoch 29...\n",
            "I Finished training epoch 29 - loss: 0.103512\n",
            "I Training epoch 30...\n",
            "I Finished training epoch 30 - loss: 0.063708\n",
            "I Training epoch 31...\n",
            "I Finished training epoch 31 - loss: 0.164333\n",
            "I Training epoch 32...\n",
            "I Finished training epoch 32 - loss: 0.053893\n",
            "I Training epoch 33...\n",
            "I Finished training epoch 33 - loss: 0.062572\n",
            "I Training epoch 34...\n",
            "I Finished training epoch 34 - loss: 0.036844\n",
            "I Training epoch 35...\n",
            "I Finished training epoch 35 - loss: 0.221235\n",
            "I Training epoch 36...\n",
            "I Finished training epoch 36 - loss: 0.269586\n",
            "I Training epoch 37...\n",
            "I Finished training epoch 37 - loss: 0.061335\n",
            "I Training epoch 38...\n",
            "I Finished training epoch 38 - loss: 0.031148\n",
            "I Training epoch 39...\n",
            "I Finished training epoch 39 - loss: 0.173690\n",
            "I Training epoch 40...\n",
            "I Finished training epoch 40 - loss: 0.163408\n",
            "I Training epoch 41...\n",
            "I Finished training epoch 41 - loss: 0.324087\n",
            "I Training epoch 42...\n",
            "I Finished training epoch 42 - loss: 0.040671\n",
            "I Training epoch 43...\n",
            "I Finished training epoch 43 - loss: 0.952138\n",
            "I Training epoch 44...\n",
            "I Finished training epoch 44 - loss: 0.082163\n",
            "I Training epoch 45...\n",
            "I Finished training epoch 45 - loss: 0.259474\n",
            "I Training epoch 46...\n",
            "I Finished training epoch 46 - loss: 0.060309\n",
            "I Training epoch 47...\n",
            "I Finished training epoch 47 - loss: 0.031974\n",
            "I Training epoch 48...\n",
            "I Finished training epoch 48 - loss: 0.036836\n",
            "I Training epoch 49...\n",
            "I Finished training epoch 49 - loss: 0.028359\n",
            "I Training epoch 50...\n",
            "I Finished training epoch 50 - loss: 0.087131\n",
            "I Training epoch 51...\n",
            "I Finished training epoch 51 - loss: 0.025629\n",
            "I Training epoch 52...\n",
            "I Finished training epoch 52 - loss: 0.024228\n",
            "I Training epoch 53...\n",
            "I Finished training epoch 53 - loss: 0.080831\n",
            "I Training epoch 54...\n",
            "I Finished training epoch 54 - loss: 0.044750\n",
            "I Training epoch 55...\n",
            "I Finished training epoch 55 - loss: 0.020744\n",
            "I Training epoch 56...\n",
            "I Finished training epoch 56 - loss: 0.033631\n",
            "I Training epoch 57...\n",
            "I Finished training epoch 57 - loss: 0.244065\n",
            "I Training epoch 58...\n",
            "I Finished training epoch 58 - loss: 0.036560\n",
            "I Training epoch 59...\n",
            "I Finished training epoch 59 - loss: 0.026462\n",
            "I Training epoch 60...\n",
            "I Finished training epoch 60 - loss: 0.038705\n",
            "I Training epoch 61...\n",
            "I Finished training epoch 61 - loss: 0.090596\n",
            "I Training epoch 62...\n",
            "I Finished training epoch 62 - loss: 0.079604\n",
            "I Training epoch 63...\n",
            "I Finished training epoch 63 - loss: 0.025610\n",
            "I Training epoch 64...\n",
            "I Finished training epoch 64 - loss: 0.050698\n",
            "I Training epoch 65...\n",
            "I Finished training epoch 65 - loss: 0.119528\n",
            "I Training epoch 66...\n",
            "I Finished training epoch 66 - loss: 0.032432\n",
            "I Training epoch 67...\n",
            "I Finished training epoch 67 - loss: 0.059296\n",
            "I Training epoch 68...\n",
            "I Finished training epoch 68 - loss: 0.092548\n",
            "I Training epoch 69...\n",
            "I Finished training epoch 69 - loss: 0.061819\n",
            "I Training epoch 70...\n",
            "I Finished training epoch 70 - loss: 0.030477\n",
            "I Training epoch 71...\n",
            "I Finished training epoch 71 - loss: 0.014110\n",
            "I Training epoch 72...\n",
            "I Finished training epoch 72 - loss: 0.033667\n",
            "I Training epoch 73...\n",
            "I Finished training epoch 73 - loss: 0.046848\n",
            "I Training epoch 74...\n",
            "I Finished training epoch 74 - loss: 0.043364\n",
            "I Training epoch 75...\n",
            "I Finished training epoch 75 - loss: 0.015707\n",
            "I Training epoch 76...\n",
            "I Finished training epoch 76 - loss: 0.022152\n",
            "I Training epoch 77...\n",
            "I Finished training epoch 77 - loss: 0.032162\n",
            "I Training epoch 78...\n",
            "I Finished training epoch 78 - loss: 0.031471\n",
            "I Training epoch 79...\n",
            "I Finished training epoch 79 - loss: 0.029283\n",
            "I Training epoch 80...\n",
            "I Finished training epoch 80 - loss: 0.023893\n",
            "I Training epoch 81...\n",
            "I Finished training epoch 81 - loss: 0.023826\n",
            "I Training epoch 82...\n",
            "I Finished training epoch 82 - loss: 0.036786\n",
            "I Training epoch 83...\n",
            "I Finished training epoch 83 - loss: 0.030161\n",
            "I Training epoch 84...\n",
            "I Finished training epoch 84 - loss: 0.035991\n",
            "I Training epoch 85...\n",
            "I Finished training epoch 85 - loss: 0.020719\n",
            "I Training epoch 86...\n",
            "I Finished training epoch 86 - loss: 0.017149\n",
            "I Training epoch 87...\n",
            "I Finished training epoch 87 - loss: 0.028652\n",
            "I Training epoch 88...\n",
            "I Finished training epoch 88 - loss: 0.012819\n",
            "I Training epoch 89...\n",
            "I Finished training epoch 89 - loss: 0.033459\n",
            "I Training epoch 90...\n",
            "I Finished training epoch 90 - loss: 0.065926\n",
            "I Training epoch 91...\n",
            "I Finished training epoch 91 - loss: 0.035568\n",
            "I Training epoch 92...\n",
            "I Finished training epoch 92 - loss: 0.011362\n",
            "I Training epoch 93...\n",
            "I Finished training epoch 93 - loss: 0.052482\n",
            "I Training epoch 94...\n",
            "I Finished training epoch 94 - loss: 0.067480\n",
            "I Training epoch 95...\n",
            "I Finished training epoch 95 - loss: 0.026059\n",
            "I Training epoch 96...\n",
            "I Finished training epoch 96 - loss: 0.022420\n",
            "I Training epoch 97...\n",
            "I Finished training epoch 97 - loss: 0.011556\n",
            "I Training epoch 98...\n",
            "I Finished training epoch 98 - loss: 0.041886\n",
            "I Training epoch 99...\n",
            "I Finished training epoch 99 - loss: 0.343320\n",
            "I Training epoch 100...\n",
            "I Finished training epoch 100 - loss: 0.013376\n",
            "I Training epoch 101...\n",
            "I Finished training epoch 101 - loss: 0.036409\n",
            "I Training epoch 102...\n",
            "I Finished training epoch 102 - loss: 0.015523\n",
            "I Training epoch 103...\n",
            "I Finished training epoch 103 - loss: 0.011642\n",
            "I Training epoch 104...\n",
            "I Finished training epoch 104 - loss: 0.017762\n",
            "I Training epoch 105...\n",
            "I Finished training epoch 105 - loss: 2.088508\n",
            "I Training epoch 106...\n",
            "I Finished training epoch 106 - loss: 0.009783\n",
            "I Training epoch 107...\n",
            "I Finished training epoch 107 - loss: 0.016247\n",
            "I Training epoch 108...\n",
            "I Finished training epoch 108 - loss: 0.056980\n",
            "I Training epoch 109...\n",
            "I Finished training epoch 109 - loss: 0.029438\n",
            "I Training epoch 110...\n",
            "I Finished training epoch 110 - loss: 0.104121\n",
            "I Training epoch 111...\n",
            "I Finished training epoch 111 - loss: 0.080171\n",
            "I Training epoch 112...\n",
            "I Finished training epoch 112 - loss: 0.299832\n",
            "I Training epoch 113...\n",
            "I Finished training epoch 113 - loss: 5.601366\n",
            "I Training epoch 114...\n",
            "I Finished training epoch 114 - loss: 0.107970\n",
            "I Training epoch 115...\n",
            "I Finished training epoch 115 - loss: 0.484569\n",
            "I Training epoch 116...\n",
            "I Finished training epoch 116 - loss: 0.571334\n",
            "I Training epoch 117...\n",
            "I Finished training epoch 117 - loss: 0.093422\n",
            "I Training epoch 118...\n",
            "I Finished training epoch 118 - loss: 0.106176\n",
            "I Training epoch 119...\n",
            "I Finished training epoch 119 - loss: 1.513580\n",
            "I Training epoch 120...\n",
            "I Finished training epoch 120 - loss: 0.744137\n",
            "I Training epoch 121...\n",
            "I Finished training epoch 121 - loss: 0.050494\n",
            "I Training epoch 122...\n",
            "I Finished training epoch 122 - loss: 0.047353\n",
            "I Training epoch 123...\n",
            "I Finished training epoch 123 - loss: 0.096284\n",
            "I Training epoch 124...\n",
            "I Finished training epoch 124 - loss: 0.057974\n",
            "I Training epoch 125...\n",
            "I Finished training epoch 125 - loss: 0.227828\n",
            "I Training epoch 126...\n",
            "I Finished training epoch 126 - loss: 0.369163\n",
            "I Training epoch 127...\n",
            "I Finished training epoch 127 - loss: 0.069655\n",
            "I Training epoch 128...\n",
            "I Finished training epoch 128 - loss: 0.110443\n",
            "I Training epoch 129...\n",
            "I Finished training epoch 129 - loss: 0.379446\n",
            "I Training epoch 130...\n",
            "I Finished training epoch 130 - loss: 0.935989\n",
            "I Training epoch 131...\n",
            "I Finished training epoch 131 - loss: 0.329007\n",
            "I Training epoch 132...\n",
            "I Finished training epoch 132 - loss: 0.103692\n",
            "I Training epoch 133...\n",
            "I Finished training epoch 133 - loss: 0.179847\n",
            "I Training epoch 134...\n",
            "I Finished training epoch 134 - loss: 0.218487\n",
            "I Training epoch 135...\n",
            "I Finished training epoch 135 - loss: 0.176494\n",
            "I Training epoch 136...\n",
            "I Finished training epoch 136 - loss: 0.185810\n",
            "I Training epoch 137...\n",
            "I Finished training epoch 137 - loss: 0.200002\n",
            "I Training epoch 138...\n",
            "I Finished training epoch 138 - loss: 0.125772\n",
            "I Training epoch 139...\n",
            "I Finished training epoch 139 - loss: 0.129873\n",
            "I Training epoch 140...\n",
            "I Finished training epoch 140 - loss: 0.110379\n",
            "I Training epoch 141...\n",
            "I Finished training epoch 141 - loss: 0.091380\n",
            "I Training epoch 142...\n",
            "I Finished training epoch 142 - loss: 0.092664\n",
            "I Training epoch 143...\n",
            "I Finished training epoch 143 - loss: 0.076134\n",
            "I Training epoch 144...\n",
            "I Finished training epoch 144 - loss: 0.076478\n",
            "I Training epoch 145...\n",
            "I Finished training epoch 145 - loss: 0.088387\n",
            "I Training epoch 146...\n",
            "I Finished training epoch 146 - loss: 0.105964\n",
            "I Training epoch 147...\n",
            "I Finished training epoch 147 - loss: 0.052504\n",
            "I Training epoch 148...\n",
            "I Finished training epoch 148 - loss: 0.146054\n",
            "I Training epoch 149...\n",
            "I Finished training epoch 149 - loss: 0.072011\n",
            "I Training epoch 150...\n",
            "I Finished training epoch 150 - loss: 0.053017\n",
            "I Training epoch 151...\n",
            "I Finished training epoch 151 - loss: 0.067038\n",
            "I Training epoch 152...\n",
            "I Finished training epoch 152 - loss: 0.033219\n",
            "I Training epoch 153...\n",
            "I Finished training epoch 153 - loss: 0.046126\n",
            "I Training epoch 154...\n",
            "I Finished training epoch 154 - loss: 0.037628\n",
            "I Training epoch 155...\n",
            "I Finished training epoch 155 - loss: 0.109341\n",
            "I Training epoch 156...\n",
            "I Finished training epoch 156 - loss: 0.181110\n",
            "I Training epoch 157...\n",
            "I Finished training epoch 157 - loss: 0.032450\n",
            "I Training epoch 158...\n",
            "I Finished training epoch 158 - loss: 0.040625\n",
            "I Training epoch 159...\n",
            "I Finished training epoch 159 - loss: 0.049013\n",
            "I Training epoch 160...\n",
            "I Finished training epoch 160 - loss: 0.062888\n",
            "I Training epoch 161...\n",
            "I Finished training epoch 161 - loss: 0.053702\n",
            "I Training epoch 162...\n",
            "I Finished training epoch 162 - loss: 0.073083\n",
            "I Training epoch 163...\n",
            "I Finished training epoch 163 - loss: 0.036477\n",
            "I Training epoch 164...\n",
            "I Finished training epoch 164 - loss: 0.046526\n",
            "I Training epoch 165...\n",
            "I Finished training epoch 165 - loss: 0.069956\n",
            "I Training epoch 166...\n",
            "I Finished training epoch 166 - loss: 0.031514\n",
            "I Training epoch 167...\n",
            "I Finished training epoch 167 - loss: 0.053565\n",
            "I Training epoch 168...\n",
            "I Finished training epoch 168 - loss: 0.028459\n",
            "I Training epoch 169...\n",
            "I Finished training epoch 169 - loss: 0.074201\n",
            "I Training epoch 170...\n",
            "I Finished training epoch 170 - loss: 0.038108\n",
            "I Training epoch 171...\n",
            "I Finished training epoch 171 - loss: 0.020196\n",
            "I Training epoch 172...\n",
            "I Finished training epoch 172 - loss: 0.024918\n",
            "I Training epoch 173...\n",
            "I Finished training epoch 173 - loss: 0.913325\n",
            "I Training epoch 174...\n",
            "I Finished training epoch 174 - loss: 0.051002\n",
            "I Training epoch 175...\n",
            "I Finished training epoch 175 - loss: 0.029252\n",
            "I Training epoch 176...\n",
            "I Finished training epoch 176 - loss: 0.040846\n",
            "I Training epoch 177...\n",
            "I Finished training epoch 177 - loss: 0.047145\n",
            "I Training epoch 178...\n",
            "I Finished training epoch 178 - loss: 0.038988\n",
            "I Training epoch 179...\n",
            "I Finished training epoch 179 - loss: 0.082313\n",
            "I Training epoch 180...\n",
            "I Finished training epoch 180 - loss: 3.496757\n",
            "I Training epoch 181...\n",
            "I Finished training epoch 181 - loss: 0.234784\n",
            "I Training epoch 182...\n",
            "I Finished training epoch 182 - loss: 0.058617\n",
            "I Training epoch 183...\n",
            "I Finished training epoch 183 - loss: 0.045045\n",
            "I Training epoch 184...\n",
            "I Finished training epoch 184 - loss: 0.282611\n",
            "I Training epoch 185...\n",
            "I Finished training epoch 185 - loss: 0.035444\n",
            "I Training epoch 186...\n",
            "I Finished training epoch 186 - loss: 0.020841\n",
            "I Training epoch 187...\n",
            "I Finished training epoch 187 - loss: 0.063346\n",
            "I Training epoch 188...\n",
            "I Finished training epoch 188 - loss: 0.026358\n",
            "I Training epoch 189...\n",
            "I Finished training epoch 189 - loss: 0.021637\n",
            "I Training epoch 190...\n",
            "I Finished training epoch 190 - loss: 0.025284\n",
            "I Training epoch 191...\n",
            "I Finished training epoch 191 - loss: 0.036857\n",
            "I Training epoch 192...\n",
            "I Finished training epoch 192 - loss: 0.205599\n",
            "I Training epoch 193...\n",
            "I Finished training epoch 193 - loss: 0.043122\n",
            "I Training epoch 194...\n",
            "I Finished training epoch 194 - loss: 0.041757\n",
            "I Training epoch 195...\n",
            "I Finished training epoch 195 - loss: 0.036544\n",
            "I Training epoch 196...\n",
            "I Finished training epoch 196 - loss: 0.027599\n",
            "I Training epoch 197...\n",
            "I Finished training epoch 197 - loss: 2.221867\n",
            "I Training epoch 198...\n",
            "I Finished training epoch 198 - loss: 0.093479\n",
            "I Training epoch 199...\n",
            "I Finished training epoch 199 - loss: 0.163301\n",
            "I Training epoch 200...\n",
            "I Finished training epoch 200 - loss: 0.052677\n",
            "I Training epoch 201...\n",
            "I Finished training epoch 201 - loss: 0.047882\n",
            "I Training epoch 202...\n",
            "I Finished training epoch 202 - loss: 0.462885\n",
            "I Training epoch 203...\n",
            "I Finished training epoch 203 - loss: 0.023660\n",
            "I Training epoch 204...\n",
            "I Finished training epoch 204 - loss: 0.042324\n",
            "I Training epoch 205...\n",
            "I Finished training epoch 205 - loss: 0.065388\n",
            "I Training epoch 206...\n",
            "I Finished training epoch 206 - loss: 0.188701\n",
            "I Training epoch 207...\n",
            "I Finished training epoch 207 - loss: 0.067133\n",
            "I Training epoch 208...\n",
            "I Finished training epoch 208 - loss: 0.068052\n",
            "I Training epoch 209...\n",
            "I Finished training epoch 209 - loss: 0.040138\n",
            "I Training epoch 210...\n",
            "I Finished training epoch 210 - loss: 0.040528\n",
            "I Training epoch 211...\n",
            "I Finished training epoch 211 - loss: 0.024307\n",
            "I Training epoch 212...\n",
            "I Finished training epoch 212 - loss: 0.147295\n",
            "I Training epoch 213...\n",
            "I Finished training epoch 213 - loss: 0.075894\n",
            "I Training epoch 214...\n",
            "I Finished training epoch 214 - loss: 0.020081\n",
            "I Training epoch 215...\n",
            "I Finished training epoch 215 - loss: 0.039788\n",
            "I Training epoch 216...\n",
            "I Finished training epoch 216 - loss: 0.023330\n",
            "I Training epoch 217...\n",
            "I Finished training epoch 217 - loss: 0.050558\n",
            "I Training epoch 218...\n",
            "I Finished training epoch 218 - loss: 0.236681\n",
            "I Training epoch 219...\n",
            "I Finished training epoch 219 - loss: 0.176904\n",
            "I Training epoch 220...\n",
            "I Finished training epoch 220 - loss: 0.028084\n",
            "I Training epoch 221...\n",
            "I Finished training epoch 221 - loss: 0.015342\n",
            "I Training epoch 222...\n",
            "I Finished training epoch 222 - loss: 0.027131\n",
            "I Training epoch 223...\n",
            "I Finished training epoch 223 - loss: 0.053239\n",
            "I Training epoch 224...\n",
            "I Finished training epoch 224 - loss: 0.071967\n",
            "I Training epoch 225...\n",
            "I Finished training epoch 225 - loss: 0.075621\n",
            "I Training epoch 226...\n",
            "I Finished training epoch 226 - loss: 0.072791\n",
            "I Training epoch 227...\n",
            "I Finished training epoch 227 - loss: 0.163076\n",
            "I Training epoch 228...\n",
            "I Finished training epoch 228 - loss: 0.035007\n",
            "I Training epoch 229...\n",
            "I Finished training epoch 229 - loss: 0.027743\n",
            "I Training epoch 230...\n",
            "I Finished training epoch 230 - loss: 0.056008\n",
            "I Training epoch 231...\n",
            "I Finished training epoch 231 - loss: 0.077092\n",
            "I Training epoch 232...\n",
            "I Finished training epoch 232 - loss: 0.028883\n",
            "I Training epoch 233...\n",
            "I Finished training epoch 233 - loss: 0.019725\n",
            "I Training epoch 234...\n",
            "I Finished training epoch 234 - loss: 0.029552\n",
            "I Training epoch 235...\n",
            "I Finished training epoch 235 - loss: 0.038088\n",
            "I Training epoch 236...\n",
            "I Finished training epoch 236 - loss: 0.030152\n",
            "I Training epoch 237...\n",
            "I Finished training epoch 237 - loss: 0.015398\n",
            "I Training epoch 238...\n",
            "I Finished training epoch 238 - loss: 0.078968\n",
            "I Training epoch 239...\n",
            "I Finished training epoch 239 - loss: 0.021431\n",
            "I Training epoch 240...\n",
            "I Finished training epoch 240 - loss: 0.028534\n",
            "I Training epoch 241...\n",
            "I Finished training epoch 241 - loss: 0.040552\n",
            "I Training epoch 242...\n",
            "I Finished training epoch 242 - loss: 0.054841\n",
            "I Training epoch 243...\n",
            "I Finished training epoch 243 - loss: 0.020992\n",
            "I Training epoch 244...\n",
            "I Finished training epoch 244 - loss: 0.013446\n",
            "I Training epoch 245...\n",
            "I Finished training epoch 245 - loss: 0.030208\n",
            "I Training epoch 246...\n",
            "I Finished training epoch 246 - loss: 0.044099\n",
            "I Training epoch 247...\n",
            "I Finished training epoch 247 - loss: 0.022189\n",
            "I Training epoch 248...\n",
            "I Finished training epoch 248 - loss: 0.032370\n",
            "I Training epoch 249...\n",
            "I Finished training epoch 249 - loss: 0.027461\n",
            "I Training epoch 250...\n",
            "I Finished training epoch 250 - loss: 0.019643\n",
            "I Training epoch 251...\n",
            "I Finished training epoch 251 - loss: 0.028714\n",
            "I Training epoch 252...\n",
            "I Finished training epoch 252 - loss: 0.017931\n",
            "I Training epoch 253...\n",
            "I Finished training epoch 253 - loss: 0.047793\n",
            "I Training epoch 254...\n",
            "I Finished training epoch 254 - loss: 0.023390\n",
            "I Training epoch 255...\n",
            "I Finished training epoch 255 - loss: 0.049154\n",
            "I Training epoch 256...\n",
            "I Finished training epoch 256 - loss: 0.021619\n",
            "I Training epoch 257...\n",
            "I Finished training epoch 257 - loss: 0.019366\n",
            "I Training epoch 258...\n",
            "I Finished training epoch 258 - loss: 0.015273\n",
            "I Training epoch 259...\n",
            "I Finished training epoch 259 - loss: 0.018190\n",
            "I Training epoch 260...\n",
            "I Finished training epoch 260 - loss: 0.019169\n",
            "I Training epoch 261...\n",
            "I Finished training epoch 261 - loss: 0.015657\n",
            "I Training epoch 262...\n",
            "I Finished training epoch 262 - loss: 0.019225\n",
            "I Training epoch 263...\n",
            "I Finished training epoch 263 - loss: 0.020045\n",
            "I Training epoch 264...\n",
            "I Finished training epoch 264 - loss: 0.015382\n",
            "I Training epoch 265...\n",
            "I Finished training epoch 265 - loss: 0.026685\n",
            "I Training epoch 266...\n",
            "I Finished training epoch 266 - loss: 0.013839\n",
            "I Training epoch 267...\n",
            "I Finished training epoch 267 - loss: 0.015102\n",
            "I Training epoch 268...\n",
            "I Finished training epoch 268 - loss: 0.022462\n",
            "I Training epoch 269...\n",
            "I Finished training epoch 269 - loss: 0.016232\n",
            "I Training epoch 270...\n",
            "I Finished training epoch 270 - loss: 0.012261\n",
            "I Training epoch 271...\n",
            "I Finished training epoch 271 - loss: 0.016995\n",
            "I Training epoch 272...\n",
            "I Finished training epoch 272 - loss: 0.014820\n",
            "I Training epoch 273...\n",
            "I Finished training epoch 273 - loss: 0.032650\n",
            "I Training epoch 274...\n",
            "I Finished training epoch 274 - loss: 3.885591\n",
            "I Training epoch 275...\n",
            "I Finished training epoch 275 - loss: 0.019647\n",
            "I Training epoch 276...\n",
            "I Finished training epoch 276 - loss: 0.049220\n",
            "I Training epoch 277...\n",
            "I Finished training epoch 277 - loss: 0.023063\n",
            "I Training epoch 278...\n",
            "I Finished training epoch 278 - loss: 0.032059\n",
            "I Training epoch 279...\n",
            "I Finished training epoch 279 - loss: 0.021759\n",
            "I Training epoch 280...\n",
            "I Finished training epoch 280 - loss: 0.029042\n",
            "I Training epoch 281...\n",
            "I Finished training epoch 281 - loss: 0.087529\n",
            "I Training epoch 282...\n",
            "I Finished training epoch 282 - loss: 0.091407\n",
            "I Training epoch 283...\n",
            "I Finished training epoch 283 - loss: 0.034130\n",
            "I Training epoch 284...\n",
            "I Finished training epoch 284 - loss: 0.173462\n",
            "I Training epoch 285...\n",
            "I Finished training epoch 285 - loss: 0.027537\n",
            "I Training epoch 286...\n",
            "I Finished training epoch 286 - loss: 0.117616\n",
            "I Training epoch 287...\n",
            "I Finished training epoch 287 - loss: 0.035645\n",
            "I Training epoch 288...\n",
            "I Finished training epoch 288 - loss: 0.153181\n",
            "I Training epoch 289...\n",
            "I Finished training epoch 289 - loss: 0.272347\n",
            "I Training epoch 290...\n",
            "I Finished training epoch 290 - loss: 0.106708\n",
            "I Training epoch 291...\n",
            "I Finished training epoch 291 - loss: 0.110201\n",
            "I Training epoch 292...\n",
            "I Finished training epoch 292 - loss: 0.141783\n",
            "I Training epoch 293...\n",
            "I Finished training epoch 293 - loss: 2.918458\n",
            "I Training epoch 294...\n",
            "I Finished training epoch 294 - loss: 0.042169\n",
            "I Training epoch 295...\n",
            "I Finished training epoch 295 - loss: 0.109582\n",
            "I Training epoch 296...\n",
            "I Finished training epoch 296 - loss: 0.027418\n",
            "I Training epoch 297...\n",
            "I Finished training epoch 297 - loss: 0.192634\n",
            "I Training epoch 298...\n",
            "I Finished training epoch 298 - loss: 0.705464\n",
            "I Training epoch 299...\n",
            "I Finished training epoch 299 - loss: 0.046769\n",
            "I Training epoch 300...\n",
            "I Finished training epoch 300 - loss: 0.047895\n",
            "I Training epoch 301...\n",
            "I Finished training epoch 301 - loss: 0.038873\n",
            "I Training epoch 302...\n",
            "I Finished training epoch 302 - loss: 0.015671\n",
            "I Training epoch 303...\n",
            "I Finished training epoch 303 - loss: 0.016077\n",
            "I Training epoch 304...\n",
            "I Finished training epoch 304 - loss: 0.021657\n",
            "I Training epoch 305...\n",
            "I Finished training epoch 305 - loss: 0.040424\n",
            "I Training epoch 306...\n",
            "I Finished training epoch 306 - loss: 0.020603\n",
            "I Training epoch 307...\n",
            "I Finished training epoch 307 - loss: 0.052037\n",
            "I Training epoch 308...\n",
            "I Finished training epoch 308 - loss: 0.012975\n",
            "I Training epoch 309...\n",
            "I Finished training epoch 309 - loss: 0.035278\n",
            "I Training epoch 310...\n",
            "I Finished training epoch 310 - loss: 0.107458\n",
            "I Training epoch 311...\n",
            "I Finished training epoch 311 - loss: 0.020975\n",
            "I Training epoch 312...\n",
            "I Finished training epoch 312 - loss: 0.012179\n",
            "I Training epoch 313...\n",
            "I Finished training epoch 313 - loss: 0.024581\n",
            "I Training epoch 314...\n",
            "I Finished training epoch 314 - loss: 0.037632\n",
            "I Training epoch 315...\n",
            "I Finished training epoch 315 - loss: 0.433412\n",
            "I Training epoch 316...\n",
            "I Finished training epoch 316 - loss: 0.026657\n",
            "I Training epoch 317...\n",
            "I Finished training epoch 317 - loss: 0.010011\n",
            "I Training epoch 318...\n",
            "I Finished training epoch 318 - loss: 0.071688\n",
            "I Training epoch 319...\n",
            "I Finished training epoch 319 - loss: 0.101565\n",
            "I Training epoch 320...\n",
            "I Finished training epoch 320 - loss: 0.078872\n",
            "I Training epoch 321...\n",
            "I Finished training epoch 321 - loss: 0.086812\n",
            "I Training epoch 322...\n",
            "I Finished training epoch 322 - loss: 1.503486\n",
            "I Training epoch 323...\n",
            "I Finished training epoch 323 - loss: 0.081858\n",
            "I Training epoch 324...\n",
            "I Finished training epoch 324 - loss: 0.015291\n",
            "I Training epoch 325...\n",
            "I Finished training epoch 325 - loss: 0.012491\n",
            "I Training epoch 326...\n",
            "I Finished training epoch 326 - loss: 0.012932\n",
            "I Training epoch 327...\n",
            "I Finished training epoch 327 - loss: 0.019221\n",
            "I Training epoch 328...\n",
            "I Finished training epoch 328 - loss: 0.016270\n",
            "I Training epoch 329...\n",
            "I Finished training epoch 329 - loss: 0.017251\n",
            "I Training epoch 330...\n",
            "I Finished training epoch 330 - loss: 0.030408\n",
            "I Training epoch 331...\n",
            "I Finished training epoch 331 - loss: 0.060739\n",
            "I Training epoch 332...\n",
            "I Finished training epoch 332 - loss: 0.028447\n",
            "I Training epoch 333...\n",
            "I Finished training epoch 333 - loss: 0.018359\n",
            "I Training epoch 334...\n",
            "I Finished training epoch 334 - loss: 0.034703\n",
            "I Training epoch 335...\n",
            "I Finished training epoch 335 - loss: 0.018346\n",
            "I Training epoch 336...\n",
            "I Finished training epoch 336 - loss: 0.017840\n",
            "I Training epoch 337...\n",
            "I Finished training epoch 337 - loss: 0.031698\n",
            "I Training epoch 338...\n",
            "I Finished training epoch 338 - loss: 0.022240\n",
            "I Training epoch 339...\n",
            "I Finished training epoch 339 - loss: 0.016366\n",
            "I Training epoch 340...\n",
            "I Finished training epoch 340 - loss: 0.021819\n",
            "I Training epoch 341...\n",
            "I Finished training epoch 341 - loss: 0.033917\n",
            "I Training epoch 342...\n",
            "I Finished training epoch 342 - loss: 0.029546\n",
            "I Training epoch 343...\n",
            "I Finished training epoch 343 - loss: 0.021042\n",
            "I Training epoch 344...\n",
            "I Finished training epoch 344 - loss: 0.020467\n",
            "I Training epoch 345...\n",
            "I Finished training epoch 345 - loss: 1.056703\n",
            "I Training epoch 346...\n",
            "I Finished training epoch 346 - loss: 0.032223\n",
            "I Training epoch 347...\n",
            "I Finished training epoch 347 - loss: 0.021957\n",
            "I Training epoch 348...\n",
            "I Finished training epoch 348 - loss: 0.035810\n",
            "I Training epoch 349...\n",
            "I Finished training epoch 349 - loss: 0.177010\n",
            "I Training epoch 350...\n",
            "I Finished training epoch 350 - loss: 0.216995\n",
            "I Training epoch 351...\n",
            "I Finished training epoch 351 - loss: 0.102370\n",
            "I Training epoch 352...\n",
            "I Finished training epoch 352 - loss: 0.074010\n",
            "I Training epoch 353...\n",
            "I Finished training epoch 353 - loss: 0.239008\n",
            "I Training epoch 354...\n",
            "I Finished training epoch 354 - loss: 0.076832\n",
            "I Training epoch 355...\n",
            "I Finished training epoch 355 - loss: 0.065712\n",
            "I Training epoch 356...\n",
            "I Finished training epoch 356 - loss: 0.061195\n",
            "I Training epoch 357...\n",
            "I Finished training epoch 357 - loss: 0.144273\n",
            "I Training epoch 358...\n",
            "I Finished training epoch 358 - loss: 0.079624\n",
            "I Training epoch 359...\n",
            "I Finished training epoch 359 - loss: 0.041783\n",
            "I Training epoch 360...\n",
            "I Finished training epoch 360 - loss: 0.022944\n",
            "I Training epoch 361...\n",
            "I Finished training epoch 361 - loss: 0.034895\n",
            "I Training epoch 362...\n",
            "I Finished training epoch 362 - loss: 0.044729\n",
            "I Training epoch 363...\n",
            "I Finished training epoch 363 - loss: 0.028515\n",
            "I Training epoch 364...\n",
            "I Finished training epoch 364 - loss: 0.050059\n",
            "I Training epoch 365...\n",
            "I Finished training epoch 365 - loss: 0.037965\n",
            "I Training epoch 366...\n",
            "I Finished training epoch 366 - loss: 0.085994\n",
            "I Training epoch 367...\n",
            "I Finished training epoch 367 - loss: 0.053888\n",
            "I Training epoch 368...\n",
            "I Finished training epoch 368 - loss: 0.656705\n",
            "I Training epoch 369...\n",
            "I Finished training epoch 369 - loss: 0.026353\n",
            "I Training epoch 370...\n",
            "I Finished training epoch 370 - loss: 0.021012\n",
            "I Training epoch 371...\n",
            "I Finished training epoch 371 - loss: 0.024537\n",
            "I Training epoch 372...\n",
            "I Finished training epoch 372 - loss: 0.028682\n",
            "I Training epoch 373...\n",
            "I Finished training epoch 373 - loss: 0.044700\n",
            "I Training epoch 374...\n",
            "I Finished training epoch 374 - loss: 0.014721\n",
            "I Training epoch 375...\n",
            "I Finished training epoch 375 - loss: 0.011526\n",
            "I Training epoch 376...\n",
            "I Finished training epoch 376 - loss: 0.018424\n",
            "I Training epoch 377...\n",
            "I Finished training epoch 377 - loss: 0.023894\n",
            "I Training epoch 378...\n",
            "I Finished training epoch 378 - loss: 0.037783\n",
            "I Training epoch 379...\n",
            "I Finished training epoch 379 - loss: 0.020570\n",
            "I Training epoch 380...\n",
            "I Finished training epoch 380 - loss: 0.022189\n",
            "I Training epoch 381...\n",
            "I Finished training epoch 381 - loss: 0.021736\n",
            "I Training epoch 382...\n",
            "I Finished training epoch 382 - loss: 0.020528\n",
            "I Training epoch 383...\n",
            "I Finished training epoch 383 - loss: 0.019263\n",
            "I Training epoch 384...\n",
            "I Finished training epoch 384 - loss: 0.057272\n",
            "I Training epoch 385...\n",
            "I Finished training epoch 385 - loss: 0.171986\n",
            "I Training epoch 386...\n",
            "I Finished training epoch 386 - loss: 0.022561\n",
            "I Training epoch 387...\n",
            "I Finished training epoch 387 - loss: 0.021390\n",
            "I Training epoch 388...\n",
            "I Finished training epoch 388 - loss: 0.010000\n",
            "I Training epoch 389...\n",
            "I Finished training epoch 389 - loss: 0.019719\n",
            "I Training epoch 390...\n",
            "I Finished training epoch 390 - loss: 0.010586\n",
            "I Training epoch 391...\n",
            "I Finished training epoch 391 - loss: 0.010749\n",
            "I Training epoch 392...\n",
            "I Finished training epoch 392 - loss: 0.012445\n",
            "I Training epoch 393...\n",
            "I Finished training epoch 393 - loss: 0.019691\n",
            "I Training epoch 394...\n",
            "I Finished training epoch 394 - loss: 0.013923\n",
            "I Training epoch 395...\n",
            "I Finished training epoch 395 - loss: 0.021226\n",
            "I Training epoch 396...\n",
            "I Finished training epoch 396 - loss: 0.008126\n",
            "I Training epoch 397...\n",
            "I Finished training epoch 397 - loss: 0.018696\n",
            "I Training epoch 398...\n",
            "I Finished training epoch 398 - loss: 0.014573\n",
            "I Training epoch 399...\n",
            "I Finished training epoch 399 - loss: 0.012712\n",
            "I Training epoch 400...\n",
            "I Finished training epoch 400 - loss: 0.009771\n",
            "I Training epoch 401...\n",
            "I Finished training epoch 401 - loss: 0.018352\n",
            "I Training epoch 402...\n",
            "I Finished training epoch 402 - loss: 0.017525\n",
            "I Training epoch 403...\n",
            "I Finished training epoch 403 - loss: 0.008629\n",
            "I Training epoch 404...\n",
            "I Finished training epoch 404 - loss: 0.082674\n",
            "I Training epoch 405...\n",
            "I Finished training epoch 405 - loss: 0.008213\n",
            "I Training epoch 406...\n",
            "I Finished training epoch 406 - loss: 0.009809\n",
            "I Training epoch 407...\n",
            "I Finished training epoch 407 - loss: 0.012728\n",
            "I Training epoch 408...\n",
            "I Finished training epoch 408 - loss: 0.017658\n",
            "I Training epoch 409...\n",
            "I Finished training epoch 409 - loss: 0.008866\n",
            "I Training epoch 410...\n",
            "I Finished training epoch 410 - loss: 0.011208\n",
            "I Training epoch 411...\n",
            "I Finished training epoch 411 - loss: 0.026834\n",
            "I Training epoch 412...\n",
            "I Finished training epoch 412 - loss: 0.009252\n",
            "I Training epoch 413...\n",
            "I Finished training epoch 413 - loss: 0.022853\n",
            "I Training epoch 414...\n",
            "I Finished training epoch 414 - loss: 0.026733\n",
            "I Training epoch 415...\n",
            "I Finished training epoch 415 - loss: 0.010025\n",
            "I Training epoch 416...\n",
            "I Finished training epoch 416 - loss: 0.011265\n",
            "I Training epoch 417...\n",
            "I Finished training epoch 417 - loss: 0.011243\n",
            "I Training epoch 418...\n",
            "I Finished training epoch 418 - loss: 0.013742\n",
            "I Training epoch 419...\n",
            "I Finished training epoch 419 - loss: 0.007644\n",
            "I Training epoch 420...\n",
            "I Finished training epoch 420 - loss: 0.010522\n",
            "I Training epoch 421...\n",
            "I Finished training epoch 421 - loss: 0.008015\n",
            "I Training epoch 422...\n",
            "I Finished training epoch 422 - loss: 0.009485\n",
            "I Training epoch 423...\n",
            "I Finished training epoch 423 - loss: 0.012942\n",
            "I Training epoch 424...\n",
            "I Finished training epoch 424 - loss: 0.025378\n",
            "I Training epoch 425...\n",
            "I Finished training epoch 425 - loss: 0.006179\n",
            "I Training epoch 426...\n",
            "I Finished training epoch 426 - loss: 0.011555\n",
            "I Training epoch 427...\n",
            "I Finished training epoch 427 - loss: 0.012247\n",
            "I Training epoch 428...\n",
            "I Finished training epoch 428 - loss: 0.011543\n",
            "I Training epoch 429...\n",
            "I Finished training epoch 429 - loss: 0.009825\n",
            "I Training epoch 430...\n",
            "I Finished training epoch 430 - loss: 0.010410\n",
            "I Training epoch 431...\n",
            "I Finished training epoch 431 - loss: 0.013462\n",
            "I Training epoch 432...\n",
            "I Finished training epoch 432 - loss: 0.019315\n",
            "I Training epoch 433...\n",
            "I Finished training epoch 433 - loss: 0.009142\n",
            "I Training epoch 434...\n",
            "I Finished training epoch 434 - loss: 0.010036\n",
            "I Training epoch 435...\n",
            "I Finished training epoch 435 - loss: 0.007085\n",
            "I Training epoch 436...\n",
            "I Finished training epoch 436 - loss: 0.012060\n",
            "I Training epoch 437...\n",
            "I Finished training epoch 437 - loss: 0.035207\n",
            "I Training epoch 438...\n",
            "I Finished training epoch 438 - loss: 0.018327\n",
            "I Training epoch 439...\n",
            "I Finished training epoch 439 - loss: 0.012742\n",
            "I Training epoch 440...\n",
            "I Finished training epoch 440 - loss: 0.007772\n",
            "I Training epoch 441...\n",
            "I Finished training epoch 441 - loss: 0.014069\n",
            "I Training epoch 442...\n",
            "I Finished training epoch 442 - loss: 0.013165\n",
            "I Training epoch 443...\n",
            "I Finished training epoch 443 - loss: 0.009324\n",
            "I Training epoch 444...\n",
            "I Finished training epoch 444 - loss: 0.012777\n",
            "I Training epoch 445...\n",
            "I Finished training epoch 445 - loss: 0.007267\n",
            "I Training epoch 446...\n",
            "I Finished training epoch 446 - loss: 0.008303\n",
            "I Training epoch 447...\n",
            "I Finished training epoch 447 - loss: 0.016115\n",
            "I Training epoch 448...\n",
            "I Finished training epoch 448 - loss: 0.022850\n",
            "I Training epoch 449...\n",
            "I Finished training epoch 449 - loss: 0.012503\n",
            "I Training epoch 450...\n",
            "I Finished training epoch 450 - loss: 0.010214\n",
            "I Training epoch 451...\n",
            "I Finished training epoch 451 - loss: 0.032671\n",
            "I Training epoch 452...\n",
            "I Finished training epoch 452 - loss: 0.006753\n",
            "I Training epoch 453...\n",
            "I Finished training epoch 453 - loss: 0.007600\n",
            "I Training epoch 454...\n",
            "I Finished training epoch 454 - loss: 0.007014\n",
            "I Training epoch 455...\n",
            "I Finished training epoch 455 - loss: 0.025579\n",
            "I Training epoch 456...\n",
            "I Finished training epoch 456 - loss: 0.017347\n",
            "I Training epoch 457...\n",
            "I Finished training epoch 457 - loss: 0.010530\n",
            "I Training epoch 458...\n",
            "I Finished training epoch 458 - loss: 0.010222\n",
            "I Training epoch 459...\n",
            "I Finished training epoch 459 - loss: 0.007316\n",
            "I Training epoch 460...\n",
            "I Finished training epoch 460 - loss: 0.019343\n",
            "I Training epoch 461...\n",
            "I Finished training epoch 461 - loss: 0.014836\n",
            "I Training epoch 462...\n",
            "I Finished training epoch 462 - loss: 0.018269\n",
            "I Training epoch 463...\n",
            "I Finished training epoch 463 - loss: 0.008444\n",
            "I Training epoch 464...\n",
            "I Finished training epoch 464 - loss: 0.015712\n",
            "I Training epoch 465...\n",
            "I Finished training epoch 465 - loss: 0.013067\n",
            "I Training epoch 466...\n",
            "I Finished training epoch 466 - loss: 0.006424\n",
            "I Training epoch 467...\n",
            "I Finished training epoch 467 - loss: 0.006621\n",
            "I Training epoch 468...\n",
            "I Finished training epoch 468 - loss: 0.006534\n",
            "I Training epoch 469...\n",
            "I Finished training epoch 469 - loss: 0.007978\n",
            "I Training epoch 470...\n",
            "I Finished training epoch 470 - loss: 0.018799\n",
            "I Training epoch 471...\n",
            "I Finished training epoch 471 - loss: 0.012289\n",
            "I Training epoch 472...\n",
            "I Finished training epoch 472 - loss: 0.009402\n",
            "I Training epoch 473...\n",
            "I Finished training epoch 473 - loss: 0.009800\n",
            "I Training epoch 474...\n",
            "I Finished training epoch 474 - loss: 0.009811\n",
            "I Training epoch 475...\n",
            "I Finished training epoch 475 - loss: 0.007758\n",
            "I Training epoch 476...\n",
            "I Finished training epoch 476 - loss: 0.011052\n",
            "I Training epoch 477...\n",
            "I Finished training epoch 477 - loss: 0.016440\n",
            "I Training epoch 478...\n",
            "I Finished training epoch 478 - loss: 0.009603\n",
            "I Training epoch 479...\n",
            "I Finished training epoch 479 - loss: 0.007258\n",
            "I Training epoch 480...\n",
            "I Finished training epoch 480 - loss: 0.034126\n",
            "I Training epoch 481...\n",
            "I Finished training epoch 481 - loss: 0.008230\n",
            "I Training epoch 482...\n",
            "I Finished training epoch 482 - loss: 0.009790\n",
            "I Training epoch 483...\n",
            "I Finished training epoch 483 - loss: 0.024847\n",
            "I Training epoch 484...\n",
            "I Finished training epoch 484 - loss: 0.018231\n",
            "I Training epoch 485...\n",
            "I Finished training epoch 485 - loss: 0.019997\n",
            "I Training epoch 486...\n",
            "I Finished training epoch 486 - loss: 0.007903\n",
            "I Training epoch 487...\n",
            "I Finished training epoch 487 - loss: 0.009933\n",
            "I Training epoch 488...\n",
            "I Finished training epoch 488 - loss: 0.012932\n",
            "I Training epoch 489...\n",
            "I Finished training epoch 489 - loss: 0.011263\n",
            "I Training epoch 490...\n",
            "I Finished training epoch 490 - loss: 0.005472\n",
            "I Training epoch 491...\n",
            "I Finished training epoch 491 - loss: 0.008290\n",
            "I Training epoch 492...\n",
            "I Finished training epoch 492 - loss: 0.006413\n",
            "I Training epoch 493...\n",
            "I Finished training epoch 493 - loss: 0.011156\n",
            "I Training epoch 494...\n",
            "I Finished training epoch 494 - loss: 0.007650\n",
            "I Training epoch 495...\n",
            "I Finished training epoch 495 - loss: 0.010554\n",
            "I Training epoch 496...\n",
            "I Finished training epoch 496 - loss: 0.006431\n",
            "I Training epoch 497...\n",
            "I Finished training epoch 497 - loss: 0.049427\n",
            "I Training epoch 498...\n",
            "I Finished training epoch 498 - loss: 0.034301\n",
            "I Training epoch 499...\n",
            "I Finished training epoch 499 - loss: 0.005901\n",
            "I Training epoch 500...\n",
            "I Finished training epoch 500 - loss: 0.013455\n",
            "I Training epoch 501...\n",
            "I Finished training epoch 501 - loss: 0.010693\n",
            "I Training epoch 502...\n",
            "I Finished training epoch 502 - loss: 0.023524\n",
            "I Training epoch 503...\n",
            "I Finished training epoch 503 - loss: 0.012657\n",
            "I Training epoch 504...\n",
            "I Finished training epoch 504 - loss: 0.013138\n",
            "I Training epoch 505...\n",
            "I Finished training epoch 505 - loss: 0.011449\n",
            "I Training epoch 506...\n",
            "I Finished training epoch 506 - loss: 0.097952\n",
            "I Training epoch 507...\n",
            "I Finished training epoch 507 - loss: 0.005455\n",
            "I Training epoch 508...\n",
            "I Finished training epoch 508 - loss: 0.005768\n",
            "I Training epoch 509...\n",
            "I Finished training epoch 509 - loss: 0.020174\n",
            "I Training epoch 510...\n",
            "I Finished training epoch 510 - loss: 0.009903\n",
            "I Training epoch 511...\n",
            "I Finished training epoch 511 - loss: 0.012694\n",
            "I Training epoch 512...\n",
            "I Finished training epoch 512 - loss: 0.005501\n",
            "I Training epoch 513...\n",
            "I Finished training epoch 513 - loss: 0.004864\n",
            "I Training epoch 514...\n",
            "I Finished training epoch 514 - loss: 0.008293\n",
            "I Training epoch 515...\n",
            "I Finished training epoch 515 - loss: 0.014206\n",
            "I Training epoch 516...\n",
            "I Finished training epoch 516 - loss: 0.005517\n",
            "I Training epoch 517...\n",
            "I Finished training epoch 517 - loss: 0.007854\n",
            "I Training epoch 518...\n",
            "I Finished training epoch 518 - loss: 0.039669\n",
            "I Training epoch 519...\n",
            "I Finished training epoch 519 - loss: 0.005098\n",
            "I Training epoch 520...\n",
            "I Finished training epoch 520 - loss: 0.011705\n",
            "I Training epoch 521...\n",
            "I Finished training epoch 521 - loss: 0.006747\n",
            "I Training epoch 522...\n",
            "I Finished training epoch 522 - loss: 0.006641\n",
            "I Training epoch 523...\n",
            "I Finished training epoch 523 - loss: 0.007314\n",
            "I Training epoch 524...\n",
            "I Finished training epoch 524 - loss: 0.010949\n",
            "I Training epoch 525...\n",
            "I Finished training epoch 525 - loss: 0.007191\n",
            "I Training epoch 526...\n",
            "I Finished training epoch 526 - loss: 0.005641\n",
            "I Training epoch 527...\n",
            "I Finished training epoch 527 - loss: 0.011990\n",
            "I Training epoch 528...\n",
            "I Finished training epoch 528 - loss: 0.005899\n",
            "I Training epoch 529...\n",
            "I Finished training epoch 529 - loss: 0.026217\n",
            "I Training epoch 530...\n",
            "I Finished training epoch 530 - loss: 0.006499\n",
            "I Training epoch 531...\n",
            "I Finished training epoch 531 - loss: 0.004980\n",
            "I Training epoch 532...\n",
            "I Finished training epoch 532 - loss: 0.005361\n",
            "I Training epoch 533...\n",
            "I Finished training epoch 533 - loss: 0.012237\n",
            "I Training epoch 534...\n",
            "I Finished training epoch 534 - loss: 0.009838\n",
            "I Training epoch 535...\n",
            "I Finished training epoch 535 - loss: 0.015540\n",
            "I Training epoch 536...\n",
            "I Finished training epoch 536 - loss: 0.006223\n",
            "I Training epoch 537...\n",
            "I Finished training epoch 537 - loss: 0.005825\n",
            "I Training epoch 538...\n",
            "I Finished training epoch 538 - loss: 0.008154\n",
            "I Training epoch 539...\n",
            "I Finished training epoch 539 - loss: 0.005350\n",
            "I Training epoch 540...\n",
            "I Finished training epoch 540 - loss: 0.010914\n",
            "I Training epoch 541...\n",
            "I Finished training epoch 541 - loss: 0.007279\n",
            "I Training epoch 542...\n",
            "I Finished training epoch 542 - loss: 0.014708\n",
            "I Training epoch 543...\n",
            "I Finished training epoch 543 - loss: 0.005291\n",
            "I Training epoch 544...\n",
            "I Finished training epoch 544 - loss: 0.009439\n",
            "I Training epoch 545...\n",
            "I Finished training epoch 545 - loss: 0.009544\n",
            "I Training epoch 546...\n",
            "I Finished training epoch 546 - loss: 0.009088\n",
            "I Training epoch 547...\n",
            "I Finished training epoch 547 - loss: 0.007620\n",
            "I Training epoch 548...\n",
            "I Finished training epoch 548 - loss: 0.005824\n",
            "I Training epoch 549...\n",
            "I Finished training epoch 549 - loss: 0.007905\n",
            "I Training epoch 550...\n",
            "I Finished training epoch 550 - loss: 0.009055\n",
            "I Training epoch 551...\n",
            "I Finished training epoch 551 - loss: 0.008065\n",
            "I Training epoch 552...\n",
            "I Finished training epoch 552 - loss: 0.006126\n",
            "I Training epoch 553...\n",
            "I Finished training epoch 553 - loss: 0.004707\n",
            "I Training epoch 554...\n",
            "I Finished training epoch 554 - loss: 0.009277\n",
            "I Training epoch 555...\n",
            "I Finished training epoch 555 - loss: 0.026809\n",
            "I Training epoch 556...\n",
            "I Finished training epoch 556 - loss: 0.007142\n",
            "I Training epoch 557...\n",
            "I Finished training epoch 557 - loss: 0.008866\n",
            "I Training epoch 558...\n",
            "I Finished training epoch 558 - loss: 0.012227\n",
            "I Training epoch 559...\n",
            "I Finished training epoch 559 - loss: 0.010105\n",
            "I Training epoch 560...\n",
            "I Finished training epoch 560 - loss: 0.010167\n",
            "I Training epoch 561...\n",
            "I Finished training epoch 561 - loss: 0.010701\n",
            "I Training epoch 562...\n",
            "I Finished training epoch 562 - loss: 0.005387\n",
            "I Training epoch 563...\n",
            "I Finished training epoch 563 - loss: 0.007905\n",
            "I Training epoch 564...\n",
            "I Finished training epoch 564 - loss: 0.005889\n",
            "I Training epoch 565...\n",
            "I Finished training epoch 565 - loss: 0.035999\n",
            "I Training epoch 566...\n",
            "I Finished training epoch 566 - loss: 0.005464\n",
            "I Training epoch 567...\n",
            "I Finished training epoch 567 - loss: 0.017614\n",
            "I Training epoch 568...\n",
            "I Finished training epoch 568 - loss: 0.006848\n",
            "I Training epoch 569...\n",
            "I Finished training epoch 569 - loss: 0.005540\n",
            "I Training epoch 570...\n",
            "I Finished training epoch 570 - loss: 0.008074\n",
            "I Training epoch 571...\n",
            "I Finished training epoch 571 - loss: 0.007940\n",
            "I Training epoch 572...\n",
            "I Finished training epoch 572 - loss: 0.008474\n",
            "I Training epoch 573...\n",
            "I Finished training epoch 573 - loss: 0.005463\n",
            "I Training epoch 574...\n",
            "I Finished training epoch 574 - loss: 0.006265\n",
            "I Training epoch 575...\n",
            "I Finished training epoch 575 - loss: 0.004910\n",
            "I Training epoch 576...\n",
            "I Finished training epoch 576 - loss: 0.021026\n",
            "I Training epoch 577...\n",
            "I Finished training epoch 577 - loss: 0.014714\n",
            "I Training epoch 578...\n",
            "I Finished training epoch 578 - loss: 0.004332\n",
            "I Training epoch 579...\n",
            "I Finished training epoch 579 - loss: 0.005988\n",
            "I Training epoch 580...\n",
            "I Finished training epoch 580 - loss: 0.004705\n",
            "I Training epoch 581...\n",
            "I Finished training epoch 581 - loss: 0.087054\n",
            "I Training epoch 582...\n",
            "I Finished training epoch 582 - loss: 0.009368\n",
            "I Training epoch 583...\n",
            "I Finished training epoch 583 - loss: 0.006346\n",
            "I Training epoch 584...\n",
            "I Finished training epoch 584 - loss: 0.006873\n",
            "I Training epoch 585...\n",
            "I Finished training epoch 585 - loss: 0.005883\n",
            "I Training epoch 586...\n",
            "I Finished training epoch 586 - loss: 0.009478\n",
            "I Training epoch 587...\n",
            "I Finished training epoch 587 - loss: 0.021493\n",
            "I Training epoch 588...\n",
            "I Finished training epoch 588 - loss: 0.006995\n",
            "I Training epoch 589...\n",
            "I Finished training epoch 589 - loss: 0.004574\n",
            "I Training epoch 590...\n",
            "I Finished training epoch 590 - loss: 0.019785\n",
            "I Training epoch 591...\n",
            "I Finished training epoch 591 - loss: 0.009724\n",
            "I Training epoch 592...\n",
            "I Finished training epoch 592 - loss: 0.007119\n",
            "I Training epoch 593...\n",
            "I Finished training epoch 593 - loss: 0.007322\n",
            "I Training epoch 594...\n",
            "I Finished training epoch 594 - loss: 0.007369\n",
            "I Training epoch 595...\n",
            "I Finished training epoch 595 - loss: 0.010270\n",
            "I Training epoch 596...\n",
            "I Finished training epoch 596 - loss: 0.053785\n",
            "I Training epoch 597...\n",
            "I Finished training epoch 597 - loss: 0.020923\n",
            "I Training epoch 598...\n",
            "I Finished training epoch 598 - loss: 0.007570\n",
            "I Training epoch 599...\n",
            "I Finished training epoch 599 - loss: 0.005537\n",
            "I Training epoch 600...\n",
            "I Finished training epoch 600 - loss: 0.007989\n",
            "I Training epoch 601...\n",
            "I Finished training epoch 601 - loss: 0.008771\n",
            "I Training epoch 602...\n",
            "I Finished training epoch 602 - loss: 0.011351\n",
            "I Training epoch 603...\n",
            "I Finished training epoch 603 - loss: 0.007647\n",
            "I Training epoch 604...\n",
            "I Finished training epoch 604 - loss: 0.008012\n",
            "I Training epoch 605...\n",
            "I Finished training epoch 605 - loss: 0.004835\n",
            "I Training epoch 606...\n",
            "I Finished training epoch 606 - loss: 0.012265\n",
            "I Training epoch 607...\n",
            "I Finished training epoch 607 - loss: 0.003954\n",
            "I Training epoch 608...\n",
            "I Finished training epoch 608 - loss: 0.007388\n",
            "I Training epoch 609...\n",
            "I Finished training epoch 609 - loss: 0.008206\n",
            "I Training epoch 610...\n",
            "I Finished training epoch 610 - loss: 0.007217\n",
            "I Training epoch 611...\n",
            "I Finished training epoch 611 - loss: 0.005930\n",
            "I Training epoch 612...\n",
            "I Finished training epoch 612 - loss: 0.009664\n",
            "I Training epoch 613...\n",
            "I Finished training epoch 613 - loss: 0.005687\n",
            "I Training epoch 614...\n",
            "I Finished training epoch 614 - loss: 0.008925\n",
            "I Training epoch 615...\n",
            "I Finished training epoch 615 - loss: 0.025677\n",
            "I Training epoch 616...\n",
            "I Finished training epoch 616 - loss: 0.004418\n",
            "I Training epoch 617...\n",
            "I Finished training epoch 617 - loss: 0.007922\n",
            "I Training epoch 618...\n",
            "I Finished training epoch 618 - loss: 0.005165\n",
            "I Training epoch 619...\n",
            "I Finished training epoch 619 - loss: 0.013402\n",
            "I Training epoch 620...\n",
            "I Finished training epoch 620 - loss: 0.011943\n",
            "I Training epoch 621...\n",
            "I Finished training epoch 621 - loss: 0.006417\n",
            "I Training epoch 622...\n",
            "I Finished training epoch 622 - loss: 0.009814\n",
            "I Training epoch 623...\n",
            "I Finished training epoch 623 - loss: 0.005856\n",
            "I Training epoch 624...\n",
            "I Finished training epoch 624 - loss: 0.005987\n",
            "I Training epoch 625...\n",
            "I Finished training epoch 625 - loss: 0.008179\n",
            "I Training epoch 626...\n",
            "I Finished training epoch 626 - loss: 0.006741\n",
            "I Training epoch 627...\n",
            "I Finished training epoch 627 - loss: 0.270506\n",
            "I Training epoch 628...\n",
            "I Finished training epoch 628 - loss: 0.010019\n",
            "I Training epoch 629...\n",
            "I Finished training epoch 629 - loss: 0.005023\n",
            "I Training epoch 630...\n",
            "I Finished training epoch 630 - loss: 0.006534\n",
            "I Training epoch 631...\n",
            "I Finished training epoch 631 - loss: 0.007761\n",
            "I Training epoch 632...\n",
            "I Finished training epoch 632 - loss: 0.007798\n",
            "I Training epoch 633...\n",
            "I Finished training epoch 633 - loss: 0.009714\n",
            "I Training epoch 634...\n",
            "I Finished training epoch 634 - loss: 0.005847\n",
            "I Training epoch 635...\n",
            "I Finished training epoch 635 - loss: 0.013152\n",
            "I Training epoch 636...\n",
            "I Finished training epoch 636 - loss: 0.057480\n",
            "I Training epoch 637...\n",
            "I Finished training epoch 637 - loss: 2.315390\n",
            "I Training epoch 638...\n",
            "I Finished training epoch 638 - loss: 0.005314\n",
            "I Training epoch 639...\n",
            "I Finished training epoch 639 - loss: 0.038242\n",
            "I Training epoch 640...\n",
            "I Finished training epoch 640 - loss: 0.008191\n",
            "I Training epoch 641...\n",
            "I Finished training epoch 641 - loss: 0.007525\n",
            "I Training epoch 642...\n",
            "I Finished training epoch 642 - loss: 0.009556\n",
            "I Training epoch 643...\n",
            "I Finished training epoch 643 - loss: 0.006082\n",
            "I Training epoch 644...\n",
            "I Finished training epoch 644 - loss: 0.016764\n",
            "I Training epoch 645...\n",
            "I Finished training epoch 645 - loss: 0.012724\n",
            "I Training epoch 646...\n",
            "I Finished training epoch 646 - loss: 0.155457\n",
            "I Training epoch 647...\n",
            "I Finished training epoch 647 - loss: 0.036638\n",
            "I Training epoch 648...\n",
            "I Finished training epoch 648 - loss: 0.040221\n",
            "I Training epoch 649...\n",
            "I Finished training epoch 649 - loss: 0.123449\n",
            "I Training epoch 650...\n",
            "I Finished training epoch 650 - loss: 0.019745\n",
            "I Training epoch 651...\n",
            "I Finished training epoch 651 - loss: 0.107125\n",
            "I Training epoch 652...\n",
            "I Finished training epoch 652 - loss: 0.027485\n",
            "I Training epoch 653...\n",
            "I Finished training epoch 653 - loss: 0.025136\n",
            "I Training epoch 654...\n",
            "I Finished training epoch 654 - loss: 0.897729\n",
            "I Training epoch 655...\n",
            "I Finished training epoch 655 - loss: 0.059778\n",
            "I Training epoch 656...\n",
            "I Finished training epoch 656 - loss: 0.046368\n",
            "I Training epoch 657...\n",
            "I Finished training epoch 657 - loss: 1.028466\n",
            "I Training epoch 658...\n",
            "I Finished training epoch 658 - loss: 0.011199\n",
            "I Training epoch 659...\n",
            "I Finished training epoch 659 - loss: 0.016607\n",
            "I Training epoch 660...\n",
            "I Finished training epoch 660 - loss: 0.010023\n",
            "I Training epoch 661...\n",
            "I Finished training epoch 661 - loss: 0.028109\n",
            "I Training epoch 662...\n",
            "I Finished training epoch 662 - loss: 0.073223\n",
            "I Training epoch 663...\n",
            "I Finished training epoch 663 - loss: 0.044180\n",
            "I Training epoch 664...\n",
            "I Finished training epoch 664 - loss: 0.032439\n",
            "I Training epoch 665...\n",
            "I Finished training epoch 665 - loss: 0.033705\n",
            "I Training epoch 666...\n",
            "I Finished training epoch 666 - loss: 0.178890\n",
            "I Training epoch 667...\n",
            "I Finished training epoch 667 - loss: 0.071598\n",
            "I Training epoch 668...\n",
            "I Finished training epoch 668 - loss: 0.048342\n",
            "I Training epoch 669...\n",
            "I Finished training epoch 669 - loss: 0.084020\n",
            "I Training epoch 670...\n",
            "I Finished training epoch 670 - loss: 0.074539\n",
            "I Training epoch 671...\n",
            "I Finished training epoch 671 - loss: 0.082328\n",
            "I Training epoch 672...\n",
            "I Finished training epoch 672 - loss: 0.028577\n",
            "I Training epoch 673...\n",
            "I Finished training epoch 673 - loss: 0.020203\n",
            "I Training epoch 674...\n",
            "I Finished training epoch 674 - loss: 0.019685\n",
            "I Training epoch 675...\n",
            "I Finished training epoch 675 - loss: 0.027228\n",
            "I Training epoch 676...\n",
            "I Finished training epoch 676 - loss: 0.035021\n",
            "I Training epoch 677...\n",
            "I Finished training epoch 677 - loss: 0.020010\n",
            "I Training epoch 678...\n",
            "I Finished training epoch 678 - loss: 0.013086\n",
            "I Training epoch 679...\n",
            "I Finished training epoch 679 - loss: 0.028446\n",
            "I Training epoch 680...\n",
            "I Finished training epoch 680 - loss: 0.011141\n",
            "I Training epoch 681...\n",
            "I Finished training epoch 681 - loss: 0.009451\n",
            "I Training epoch 682...\n",
            "I Finished training epoch 682 - loss: 0.007674\n",
            "I Training epoch 683...\n",
            "I Finished training epoch 683 - loss: 0.007941\n",
            "I Training epoch 684...\n",
            "I Finished training epoch 684 - loss: 0.016702\n",
            "I Training epoch 685...\n",
            "I Finished training epoch 685 - loss: 0.010110\n",
            "I Training epoch 686...\n",
            "I Finished training epoch 686 - loss: 0.011395\n",
            "I Training epoch 687...\n",
            "I Finished training epoch 687 - loss: 0.009048\n",
            "I Training epoch 688...\n",
            "I Finished training epoch 688 - loss: 0.018170\n",
            "I Training epoch 689...\n",
            "I Finished training epoch 689 - loss: 0.020212\n",
            "I Training epoch 690...\n",
            "I Finished training epoch 690 - loss: 0.014278\n",
            "I Training epoch 691...\n",
            "I Finished training epoch 691 - loss: 0.006820\n",
            "I Training epoch 692...\n",
            "I Finished training epoch 692 - loss: 0.005253\n",
            "I Training epoch 693...\n",
            "I Finished training epoch 693 - loss: 0.006078\n",
            "I Training epoch 694...\n",
            "I Finished training epoch 694 - loss: 0.007108\n",
            "I Training epoch 695...\n",
            "I Finished training epoch 695 - loss: 0.008911\n",
            "I Training epoch 696...\n",
            "I Finished training epoch 696 - loss: 0.011153\n",
            "I Training epoch 697...\n",
            "I Finished training epoch 697 - loss: 0.009944\n",
            "I Training epoch 698...\n",
            "I Finished training epoch 698 - loss: 0.007567\n",
            "I Training epoch 699...\n",
            "I Finished training epoch 699 - loss: 0.008580\n",
            "I Training epoch 700...\n",
            "I Finished training epoch 700 - loss: 0.010661\n",
            "I Training epoch 701...\n",
            "I Finished training epoch 701 - loss: 0.007977\n",
            "I Training epoch 702...\n",
            "I Finished training epoch 702 - loss: 0.006344\n",
            "I Training epoch 703...\n",
            "I Finished training epoch 703 - loss: 0.006428\n",
            "I Training epoch 704...\n",
            "I Finished training epoch 704 - loss: 0.005353\n",
            "I Training epoch 705...\n",
            "I Finished training epoch 705 - loss: 0.007984\n",
            "I Training epoch 706...\n",
            "I Finished training epoch 706 - loss: 0.015839\n",
            "I Training epoch 707...\n",
            "I Finished training epoch 707 - loss: 0.007220\n",
            "I Training epoch 708...\n",
            "I Finished training epoch 708 - loss: 0.006660\n",
            "I Training epoch 709...\n",
            "I Finished training epoch 709 - loss: 0.014424\n",
            "I Training epoch 710...\n",
            "I Finished training epoch 710 - loss: 0.008826\n",
            "I Training epoch 711...\n",
            "I Finished training epoch 711 - loss: 0.012572\n",
            "I Training epoch 712...\n",
            "I Finished training epoch 712 - loss: 0.024506\n",
            "I Training epoch 713...\n",
            "I Finished training epoch 713 - loss: 0.009144\n",
            "I Training epoch 714...\n",
            "I Finished training epoch 714 - loss: 0.008585\n",
            "I Training epoch 715...\n",
            "I Finished training epoch 715 - loss: 0.014436\n",
            "I Training epoch 716...\n",
            "I Finished training epoch 716 - loss: 0.008715\n",
            "I Training epoch 717...\n",
            "I Finished training epoch 717 - loss: 0.030003\n",
            "I Training epoch 718...\n",
            "I Finished training epoch 718 - loss: 0.006807\n",
            "I Training epoch 719...\n",
            "I Finished training epoch 719 - loss: 0.004236\n",
            "I Training epoch 720...\n",
            "I Finished training epoch 720 - loss: 0.007329\n",
            "I Training epoch 721...\n",
            "I Finished training epoch 721 - loss: 0.012491\n",
            "I Training epoch 722...\n",
            "I Finished training epoch 722 - loss: 0.009556\n",
            "I Training epoch 723...\n",
            "I Finished training epoch 723 - loss: 0.015566\n",
            "I Training epoch 724...\n",
            "I Finished training epoch 724 - loss: 0.008187\n",
            "I Training epoch 725...\n",
            "I Finished training epoch 725 - loss: 0.006044\n",
            "I Training epoch 726...\n",
            "I Finished training epoch 726 - loss: 0.005434\n",
            "I Training epoch 727...\n",
            "I Finished training epoch 727 - loss: 0.005917\n",
            "I Training epoch 728...\n",
            "I Finished training epoch 728 - loss: 0.010725\n",
            "I Training epoch 729...\n",
            "I Finished training epoch 729 - loss: 0.008686\n",
            "I Training epoch 730...\n",
            "I Finished training epoch 730 - loss: 0.007434\n",
            "I Training epoch 731...\n",
            "I Finished training epoch 731 - loss: 0.008091\n",
            "I Training epoch 732...\n",
            "I Finished training epoch 732 - loss: 0.005267\n",
            "I Training epoch 733...\n",
            "I Finished training epoch 733 - loss: 0.029111\n",
            "I Training epoch 734...\n",
            "I Finished training epoch 734 - loss: 0.009826\n",
            "I Training epoch 735...\n",
            "I Finished training epoch 735 - loss: 0.013828\n",
            "I Training epoch 736...\n",
            "I Finished training epoch 736 - loss: 0.010716\n",
            "I Training epoch 737...\n",
            "I Finished training epoch 737 - loss: 0.005223\n",
            "I Training epoch 738...\n",
            "I Finished training epoch 738 - loss: 0.019635\n",
            "I Training epoch 739...\n",
            "I Finished training epoch 739 - loss: 0.049370\n",
            "I Training epoch 740...\n",
            "I Finished training epoch 740 - loss: 0.031732\n",
            "I Training epoch 741...\n",
            "I Finished training epoch 741 - loss: 0.023218\n",
            "I Training epoch 742...\n",
            "I Finished training epoch 742 - loss: 0.003712\n",
            "I Training epoch 743...\n",
            "I Finished training epoch 743 - loss: 0.011235\n",
            "I Training epoch 744...\n",
            "I Finished training epoch 744 - loss: 0.007179\n",
            "I Training epoch 745...\n",
            "I Finished training epoch 745 - loss: 0.009804\n",
            "I Training epoch 746...\n",
            "I Finished training epoch 746 - loss: 0.007672\n",
            "I Training epoch 747...\n",
            "I Finished training epoch 747 - loss: 0.016328\n",
            "I Training epoch 748...\n",
            "I Finished training epoch 748 - loss: 0.004622\n",
            "I Training epoch 749...\n",
            "I Finished training epoch 749 - loss: 0.010912\n",
            "I Training epoch 750...\n",
            "I Finished training epoch 750 - loss: 0.009884\n",
            "I Training epoch 751...\n",
            "I Finished training epoch 751 - loss: 0.012146\n",
            "I Training epoch 752...\n",
            "I Finished training epoch 752 - loss: 0.015671\n",
            "I Training epoch 753...\n",
            "I Finished training epoch 753 - loss: 0.009552\n",
            "I Training epoch 754...\n",
            "I Finished training epoch 754 - loss: 0.007021\n",
            "I Training epoch 755...\n",
            "I Finished training epoch 755 - loss: 0.006053\n",
            "I Training epoch 756...\n",
            "I Finished training epoch 756 - loss: 0.005829\n",
            "I Training epoch 757...\n",
            "I Finished training epoch 757 - loss: 0.011282\n",
            "I Training epoch 758...\n",
            "I Finished training epoch 758 - loss: 0.006103\n",
            "I Training epoch 759...\n",
            "I Finished training epoch 759 - loss: 0.006393\n",
            "I Training epoch 760...\n",
            "I Finished training epoch 760 - loss: 0.006642\n",
            "I Training epoch 761...\n",
            "I Finished training epoch 761 - loss: 0.009252\n",
            "I Training epoch 762...\n",
            "I Finished training epoch 762 - loss: 0.012881\n",
            "I Training epoch 763...\n",
            "I Finished training epoch 763 - loss: 0.004356\n",
            "I Training epoch 764...\n",
            "I Finished training epoch 764 - loss: 0.007901\n",
            "I Training epoch 765...\n",
            "I Finished training epoch 765 - loss: 0.005253\n",
            "I Training epoch 766...\n",
            "I Finished training epoch 766 - loss: 0.065896\n",
            "I Training epoch 767...\n",
            "I Finished training epoch 767 - loss: 0.008289\n",
            "I Training epoch 768...\n",
            "I Finished training epoch 768 - loss: 0.004377\n",
            "I Training epoch 769...\n",
            "I Finished training epoch 769 - loss: 0.005297\n",
            "I Training epoch 770...\n",
            "I Finished training epoch 770 - loss: 0.014748\n",
            "I Training epoch 771...\n",
            "I Finished training epoch 771 - loss: 0.018016\n",
            "I Training epoch 772...\n",
            "I Finished training epoch 772 - loss: 0.006838\n",
            "I Training epoch 773...\n",
            "I Finished training epoch 773 - loss: 0.009045\n",
            "I Training epoch 774...\n",
            "I Finished training epoch 774 - loss: 0.008145\n",
            "I Training epoch 775...\n",
            "I Finished training epoch 775 - loss: 0.004227\n",
            "I Training epoch 776...\n",
            "I Finished training epoch 776 - loss: 0.006434\n",
            "I Training epoch 777...\n",
            "I Finished training epoch 777 - loss: 0.007162\n",
            "I Training epoch 778...\n",
            "I Finished training epoch 778 - loss: 0.006680\n",
            "I Training epoch 779...\n",
            "I Finished training epoch 779 - loss: 0.005238\n",
            "I Training epoch 780...\n",
            "I Finished training epoch 780 - loss: 0.007025\n",
            "I Training epoch 781...\n",
            "I Finished training epoch 781 - loss: 0.017322\n",
            "I Training epoch 782...\n",
            "I Finished training epoch 782 - loss: 0.004067\n",
            "I Training epoch 783...\n",
            "I Finished training epoch 783 - loss: 0.006201\n",
            "I Training epoch 784...\n",
            "I Finished training epoch 784 - loss: 0.005235\n",
            "I Training epoch 785...\n",
            "I Finished training epoch 785 - loss: 0.007645\n",
            "I Training epoch 786...\n",
            "I Finished training epoch 786 - loss: 0.010843\n",
            "I Training epoch 787...\n",
            "I Finished training epoch 787 - loss: 0.006631\n",
            "I Training epoch 788...\n",
            "I Finished training epoch 788 - loss: 0.005551\n",
            "I Training epoch 789...\n",
            "I Finished training epoch 789 - loss: 0.015031\n",
            "I Training epoch 790...\n",
            "I Finished training epoch 790 - loss: 0.011301\n",
            "I Training epoch 791...\n",
            "I Finished training epoch 791 - loss: 0.006713\n",
            "I Training epoch 792...\n",
            "I Finished training epoch 792 - loss: 0.077137\n",
            "I Training epoch 793...\n",
            "I Finished training epoch 793 - loss: 0.004731\n",
            "I Training epoch 794...\n",
            "I Finished training epoch 794 - loss: 0.005246\n",
            "I Training epoch 795...\n",
            "I Finished training epoch 795 - loss: 0.006126\n",
            "I Training epoch 796...\n",
            "I Finished training epoch 796 - loss: 0.004452\n",
            "I Training epoch 797...\n",
            "I Finished training epoch 797 - loss: 0.010980\n",
            "I Training epoch 798...\n",
            "I Finished training epoch 798 - loss: 0.004344\n",
            "I Training epoch 799...\n",
            "I Finished training epoch 799 - loss: 0.005087\n",
            "I Training epoch 800...\n",
            "I Finished training epoch 800 - loss: 0.006129\n",
            "I Training epoch 801...\n",
            "I Finished training epoch 801 - loss: 0.007526\n",
            "I Training epoch 802...\n",
            "I Finished training epoch 802 - loss: 0.003035\n",
            "I Training epoch 803...\n",
            "I Finished training epoch 803 - loss: 0.005170\n",
            "I Training epoch 804...\n",
            "I Finished training epoch 804 - loss: 0.008607\n",
            "I Training epoch 805...\n",
            "I Finished training epoch 805 - loss: 0.004354\n",
            "I Training epoch 806...\n",
            "I Finished training epoch 806 - loss: 0.008968\n",
            "I Training epoch 807...\n",
            "I Finished training epoch 807 - loss: 0.008299\n",
            "I Training epoch 808...\n",
            "I Finished training epoch 808 - loss: 0.014184\n",
            "I Training epoch 809...\n",
            "I Finished training epoch 809 - loss: 0.018527\n",
            "I Training epoch 810...\n",
            "I Finished training epoch 810 - loss: 0.006358\n",
            "I Training epoch 811...\n",
            "I Finished training epoch 811 - loss: 0.014436\n",
            "I Training epoch 812...\n",
            "I Finished training epoch 812 - loss: 0.005605\n",
            "I Training epoch 813...\n",
            "I Finished training epoch 813 - loss: 0.004654\n",
            "I Training epoch 814...\n",
            "I Finished training epoch 814 - loss: 0.010893\n",
            "I Training epoch 815...\n",
            "I Finished training epoch 815 - loss: 0.011229\n",
            "I Training epoch 816...\n",
            "I Finished training epoch 816 - loss: 0.004540\n",
            "I Training epoch 817...\n",
            "I Finished training epoch 817 - loss: 0.005164\n",
            "I Training epoch 818...\n",
            "I Finished training epoch 818 - loss: 0.009930\n",
            "I Training epoch 819...\n",
            "I Finished training epoch 819 - loss: 0.007109\n",
            "I Training epoch 820...\n",
            "I Finished training epoch 820 - loss: 0.005406\n",
            "I Training epoch 821...\n",
            "I Finished training epoch 821 - loss: 0.024370\n",
            "I Training epoch 822...\n",
            "I Finished training epoch 822 - loss: 0.006274\n",
            "I Training epoch 823...\n",
            "I Finished training epoch 823 - loss: 0.009160\n",
            "I Training epoch 824...\n",
            "I Finished training epoch 824 - loss: 0.004595\n",
            "I Training epoch 825...\n",
            "I Finished training epoch 825 - loss: 0.012221\n",
            "I Training epoch 826...\n",
            "I Finished training epoch 826 - loss: 0.007748\n",
            "I Training epoch 827...\n",
            "I Finished training epoch 827 - loss: 0.009887\n",
            "I Training epoch 828...\n",
            "I Finished training epoch 828 - loss: 0.019931\n",
            "I Training epoch 829...\n",
            "I Finished training epoch 829 - loss: 0.007530\n",
            "I Training epoch 830...\n",
            "I Finished training epoch 830 - loss: 0.006481\n",
            "I Training epoch 831...\n",
            "I Finished training epoch 831 - loss: 0.007898\n",
            "I Training epoch 832...\n",
            "I Finished training epoch 832 - loss: 0.003899\n",
            "I Training epoch 833...\n",
            "I Finished training epoch 833 - loss: 0.004004\n",
            "I Training epoch 834...\n",
            "I Finished training epoch 834 - loss: 0.004756\n",
            "I Training epoch 835...\n",
            "I Finished training epoch 835 - loss: 0.014085\n",
            "I Training epoch 836...\n",
            "I Finished training epoch 836 - loss: 0.005620\n",
            "I Training epoch 837...\n",
            "I Finished training epoch 837 - loss: 0.008969\n",
            "I Training epoch 838...\n",
            "I Finished training epoch 838 - loss: 0.004467\n",
            "I Training epoch 839...\n",
            "I Finished training epoch 839 - loss: 0.005800\n",
            "I Training epoch 840...\n",
            "I Finished training epoch 840 - loss: 0.005755\n",
            "I Training epoch 841...\n",
            "I Finished training epoch 841 - loss: 0.003782\n",
            "I Training epoch 842...\n",
            "I Finished training epoch 842 - loss: 0.004980\n",
            "I Training epoch 843...\n",
            "I Finished training epoch 843 - loss: 0.004841\n",
            "I Training epoch 844...\n",
            "I Finished training epoch 844 - loss: 0.011164\n",
            "I Training epoch 845...\n",
            "I Finished training epoch 845 - loss: 0.005404\n",
            "I Training epoch 846...\n",
            "I Finished training epoch 846 - loss: 0.005194\n",
            "I Training epoch 847...\n",
            "I Finished training epoch 847 - loss: 0.006026\n",
            "I Training epoch 848...\n",
            "I Finished training epoch 848 - loss: 0.006155\n",
            "I Training epoch 849...\n",
            "I Finished training epoch 849 - loss: 0.007263\n",
            "I Training epoch 850...\n",
            "I Finished training epoch 850 - loss: 0.018095\n",
            "I Training epoch 851...\n",
            "I Finished training epoch 851 - loss: 0.003551\n",
            "I Training epoch 852...\n",
            "I Finished training epoch 852 - loss: 0.010727\n",
            "I Training epoch 853...\n",
            "I Finished training epoch 853 - loss: 0.009889\n",
            "I Training epoch 854...\n",
            "I Finished training epoch 854 - loss: 0.005114\n",
            "I Training epoch 855...\n",
            "I Finished training epoch 855 - loss: 0.011932\n",
            "I Training epoch 856...\n",
            "I Finished training epoch 856 - loss: 0.003648\n",
            "I Training epoch 857...\n",
            "I Finished training epoch 857 - loss: 0.003774\n",
            "I Training epoch 858...\n",
            "I Finished training epoch 858 - loss: 0.005980\n",
            "I Training epoch 859...\n",
            "I Finished training epoch 859 - loss: 0.004837\n",
            "I Training epoch 860...\n",
            "I Finished training epoch 860 - loss: 0.007012\n",
            "I Training epoch 861...\n",
            "I Finished training epoch 861 - loss: 0.003672\n",
            "I Training epoch 862...\n",
            "I Finished training epoch 862 - loss: 0.005535\n",
            "I Training epoch 863...\n",
            "I Finished training epoch 863 - loss: 0.008655\n",
            "I Training epoch 864...\n",
            "I Finished training epoch 864 - loss: 0.004284\n",
            "I Training epoch 865...\n",
            "I Finished training epoch 865 - loss: 0.007413\n",
            "I Training epoch 866...\n",
            "I Finished training epoch 866 - loss: 0.009078\n",
            "I Training epoch 867...\n",
            "I Finished training epoch 867 - loss: 0.018231\n",
            "I Training epoch 868...\n",
            "I Finished training epoch 868 - loss: 0.005376\n",
            "I Training epoch 869...\n",
            "I Finished training epoch 869 - loss: 0.005520\n",
            "I Training epoch 870...\n",
            "I Finished training epoch 870 - loss: 0.014617\n",
            "I Training epoch 871...\n",
            "I Finished training epoch 871 - loss: 0.004089\n",
            "I Training epoch 872...\n",
            "I Finished training epoch 872 - loss: 0.005574\n",
            "I Training epoch 873...\n",
            "I Finished training epoch 873 - loss: 0.008046\n",
            "I Training epoch 874...\n",
            "I Finished training epoch 874 - loss: 0.005334\n",
            "I Training epoch 875...\n",
            "I Finished training epoch 875 - loss: 0.011049\n",
            "I Training epoch 876...\n",
            "I Finished training epoch 876 - loss: 0.004323\n",
            "I Training epoch 877...\n",
            "I Finished training epoch 877 - loss: 0.007906\n",
            "I Training epoch 878...\n",
            "I Finished training epoch 878 - loss: 0.004796\n",
            "I Training epoch 879...\n",
            "I Finished training epoch 879 - loss: 0.003613\n",
            "I Training epoch 880...\n",
            "I Finished training epoch 880 - loss: 0.023384\n",
            "I Training epoch 881...\n",
            "I Finished training epoch 881 - loss: 0.008432\n",
            "I Training epoch 882...\n",
            "I Finished training epoch 882 - loss: 0.009244\n",
            "I Training epoch 883...\n",
            "I Finished training epoch 883 - loss: 0.004048\n",
            "I Training epoch 884...\n",
            "I Finished training epoch 884 - loss: 0.004551\n",
            "I Training epoch 885...\n",
            "I Finished training epoch 885 - loss: 0.004978\n",
            "I Training epoch 886...\n",
            "I Finished training epoch 886 - loss: 0.012772\n",
            "I Training epoch 887...\n",
            "I Finished training epoch 887 - loss: 0.006779\n",
            "I Training epoch 888...\n",
            "I Finished training epoch 888 - loss: 0.009938\n",
            "I Training epoch 889...\n",
            "I Finished training epoch 889 - loss: 0.046907\n",
            "I Training epoch 890...\n",
            "I Finished training epoch 890 - loss: 0.007341\n",
            "I Training epoch 891...\n",
            "I Finished training epoch 891 - loss: 0.007780\n",
            "I Training epoch 892...\n",
            "I Finished training epoch 892 - loss: 0.007392\n",
            "I Training epoch 893...\n",
            "I Finished training epoch 893 - loss: 0.010444\n",
            "I Training epoch 894...\n",
            "I Finished training epoch 894 - loss: 0.003390\n",
            "I Training epoch 895...\n",
            "I Finished training epoch 895 - loss: 1.305338\n",
            "I Training epoch 896...\n",
            "I Finished training epoch 896 - loss: 0.029273\n",
            "I Training epoch 897...\n",
            "I Finished training epoch 897 - loss: 0.021368\n",
            "I Training epoch 898...\n",
            "I Finished training epoch 898 - loss: 0.072774\n",
            "I Training epoch 899...\n",
            "I Finished training epoch 899 - loss: 0.011863\n",
            "I Training epoch 900...\n",
            "I Finished training epoch 900 - loss: 0.007861\n",
            "I Training epoch 901...\n",
            "I Finished training epoch 901 - loss: 0.007907\n",
            "I Training epoch 902...\n",
            "I Finished training epoch 902 - loss: 0.026335\n",
            "I Training epoch 903...\n",
            "I Finished training epoch 903 - loss: 0.119502\n",
            "I Training epoch 904...\n",
            "I Finished training epoch 904 - loss: 0.249980\n",
            "I Training epoch 905...\n",
            "I Finished training epoch 905 - loss: 10.451483\n",
            "I Training epoch 906...\n",
            "I Finished training epoch 906 - loss: 0.042777\n",
            "I Training epoch 907...\n",
            "I Finished training epoch 907 - loss: 0.108049\n",
            "I Training epoch 908...\n",
            "I Finished training epoch 908 - loss: 0.340181\n",
            "I Training epoch 909...\n",
            "I Finished training epoch 909 - loss: 0.006633\n",
            "I Training epoch 910...\n",
            "I Finished training epoch 910 - loss: 0.009592\n",
            "I Training epoch 911...\n",
            "I Finished training epoch 911 - loss: 0.014853\n",
            "I Training epoch 912...\n",
            "I Finished training epoch 912 - loss: 0.025775\n",
            "I Training epoch 913...\n",
            "I Finished training epoch 913 - loss: 0.048952\n",
            "I Training epoch 914...\n",
            "I Finished training epoch 914 - loss: 0.004298\n",
            "I Training epoch 915...\n",
            "I Finished training epoch 915 - loss: 0.261405\n",
            "I Training epoch 916...\n",
            "I Finished training epoch 916 - loss: 0.007267\n",
            "I Training epoch 917...\n",
            "I Finished training epoch 917 - loss: 0.040053\n",
            "I Training epoch 918...\n",
            "I Finished training epoch 918 - loss: 0.033823\n",
            "I Training epoch 919...\n",
            "I Finished training epoch 919 - loss: 0.006295\n",
            "I Training epoch 920...\n",
            "I Finished training epoch 920 - loss: 0.006343\n",
            "I Training epoch 921...\n",
            "I Finished training epoch 921 - loss: 0.020244\n",
            "I Training epoch 922...\n",
            "I Finished training epoch 922 - loss: 0.014937\n",
            "I Training epoch 923...\n",
            "I Finished training epoch 923 - loss: 0.005420\n",
            "I Training epoch 924...\n",
            "I Finished training epoch 924 - loss: 0.020776\n",
            "I Training epoch 925...\n",
            "I Finished training epoch 925 - loss: 0.009383\n",
            "I Training epoch 926...\n",
            "I Finished training epoch 926 - loss: 0.012374\n",
            "I Training epoch 927...\n",
            "I Finished training epoch 927 - loss: 0.009128\n",
            "I Training epoch 928...\n",
            "I Finished training epoch 928 - loss: 0.042469\n",
            "I Training epoch 929...\n",
            "I Finished training epoch 929 - loss: 0.009097\n",
            "I Training epoch 930...\n",
            "I Finished training epoch 930 - loss: 0.008304\n",
            "I Training epoch 931...\n",
            "I Finished training epoch 931 - loss: 0.007896\n",
            "I Training epoch 932...\n",
            "I Finished training epoch 932 - loss: 0.031842\n",
            "I Training epoch 933...\n",
            "I Finished training epoch 933 - loss: 0.327615\n",
            "I Training epoch 934...\n",
            "I Finished training epoch 934 - loss: 0.022179\n",
            "I Training epoch 935...\n",
            "I Finished training epoch 935 - loss: 0.014211\n",
            "I Training epoch 936...\n",
            "I Finished training epoch 936 - loss: 0.024438\n",
            "I Training epoch 937...\n",
            "I Finished training epoch 937 - loss: 0.005827\n",
            "I Training epoch 938...\n",
            "I Finished training epoch 938 - loss: 0.006303\n",
            "I Training epoch 939...\n",
            "I Finished training epoch 939 - loss: 0.008062\n",
            "I Training epoch 940...\n",
            "I Finished training epoch 940 - loss: 0.009812\n",
            "I Training epoch 941...\n",
            "I Finished training epoch 941 - loss: 0.006960\n",
            "I Training epoch 942...\n",
            "I Finished training epoch 942 - loss: 0.018711\n",
            "I Training epoch 943...\n",
            "I Finished training epoch 943 - loss: 0.188954\n",
            "I Training epoch 944...\n",
            "I Finished training epoch 944 - loss: 0.011701\n",
            "I Training epoch 945...\n",
            "I Finished training epoch 945 - loss: 0.018212\n",
            "I Training epoch 946...\n",
            "I Finished training epoch 946 - loss: 0.007313\n",
            "I Training epoch 947...\n",
            "I Finished training epoch 947 - loss: 0.009259\n",
            "I Training epoch 948...\n",
            "I Finished training epoch 948 - loss: 0.020035\n",
            "I Training epoch 949...\n",
            "I Finished training epoch 949 - loss: 0.007729\n",
            "I Training epoch 950...\n",
            "I Finished training epoch 950 - loss: 0.010435\n",
            "I Training epoch 951...\n",
            "I Finished training epoch 951 - loss: 0.011287\n",
            "I Training epoch 952...\n",
            "I Finished training epoch 952 - loss: 0.007678\n",
            "I Training epoch 953...\n",
            "I Finished training epoch 953 - loss: 0.014938\n",
            "I Training epoch 954...\n",
            "I Finished training epoch 954 - loss: 0.019592\n",
            "I Training epoch 955...\n",
            "I Finished training epoch 955 - loss: 0.006048\n",
            "I Training epoch 956...\n",
            "I Finished training epoch 956 - loss: 0.218866\n",
            "I Training epoch 957...\n",
            "I Finished training epoch 957 - loss: 0.007544\n",
            "I Training epoch 958...\n",
            "I Finished training epoch 958 - loss: 0.008105\n",
            "I Training epoch 959...\n",
            "I Finished training epoch 959 - loss: 0.016799\n",
            "I Training epoch 960...\n",
            "I Finished training epoch 960 - loss: 0.005980\n",
            "I Training epoch 961...\n",
            "I Finished training epoch 961 - loss: 0.009208\n",
            "I Training epoch 962...\n",
            "I Finished training epoch 962 - loss: 0.008525\n",
            "I Training epoch 963...\n",
            "I Finished training epoch 963 - loss: 0.011439\n",
            "I Training epoch 964...\n",
            "I Finished training epoch 964 - loss: 0.017589\n",
            "I Training epoch 965...\n",
            "I Finished training epoch 965 - loss: 0.006596\n",
            "I Training epoch 966...\n",
            "I Finished training epoch 966 - loss: 0.007593\n",
            "I Training epoch 967...\n",
            "I Finished training epoch 967 - loss: 0.020861\n",
            "I Training epoch 968...\n",
            "I Finished training epoch 968 - loss: 0.012958\n",
            "I Training epoch 969...\n",
            "I Finished training epoch 969 - loss: 0.006247\n",
            "I Training epoch 970...\n",
            "I Finished training epoch 970 - loss: 0.005374\n",
            "I Training epoch 971...\n",
            "I Finished training epoch 971 - loss: 0.006554\n",
            "I Training epoch 972...\n",
            "I Finished training epoch 972 - loss: 0.005197\n",
            "I Training epoch 973...\n",
            "I Finished training epoch 973 - loss: 0.007007\n",
            "I Training epoch 974...\n",
            "I Finished training epoch 974 - loss: 0.008860\n",
            "I Training epoch 975...\n",
            "I Finished training epoch 975 - loss: 0.020946\n",
            "I Training epoch 976...\n",
            "I Finished training epoch 976 - loss: 0.121159\n",
            "I Training epoch 977...\n",
            "I Finished training epoch 977 - loss: 0.049342\n",
            "I Training epoch 978...\n",
            "I Finished training epoch 978 - loss: 0.008991\n",
            "I Training epoch 979...\n",
            "I Finished training epoch 979 - loss: 0.006907\n",
            "I Training epoch 980...\n",
            "I Finished training epoch 980 - loss: 0.009513\n",
            "I Training epoch 981...\n",
            "I Finished training epoch 981 - loss: 0.006834\n",
            "I Training epoch 982...\n",
            "I Finished training epoch 982 - loss: 0.009767\n",
            "I Training epoch 983...\n",
            "I Finished training epoch 983 - loss: 0.010469\n",
            "I Training epoch 984...\n",
            "I Finished training epoch 984 - loss: 0.025454\n",
            "I Training epoch 985...\n",
            "I Finished training epoch 985 - loss: 0.009586\n",
            "I Training epoch 986...\n",
            "I Finished training epoch 986 - loss: 0.006647\n",
            "I Training epoch 987...\n",
            "I Finished training epoch 987 - loss: 0.007582\n",
            "I Training epoch 988...\n",
            "I Finished training epoch 988 - loss: 0.006305\n",
            "I Training epoch 989...\n",
            "I Finished training epoch 989 - loss: 0.007699\n",
            "I Training epoch 990...\n",
            "I Finished training epoch 990 - loss: 0.005346\n",
            "I Training epoch 991...\n",
            "I Finished training epoch 991 - loss: 0.008151\n",
            "I Training epoch 992...\n",
            "I Finished training epoch 992 - loss: 0.006946\n",
            "I Training epoch 993...\n",
            "I Finished training epoch 993 - loss: 0.006155\n",
            "I Training epoch 994...\n",
            "I Finished training epoch 994 - loss: 0.006161\n",
            "I Training epoch 995...\n",
            "I Finished training epoch 995 - loss: 0.007334\n",
            "I Training epoch 996...\n",
            "I Finished training epoch 996 - loss: 0.009632\n",
            "I Training epoch 997...\n",
            "I Finished training epoch 997 - loss: 0.009440\n",
            "I Training epoch 998...\n",
            "I Finished training epoch 998 - loss: 0.008231\n",
            "I Training epoch 999...\n",
            "I Finished training epoch 999 - loss: 0.005526\n",
            "I Training epoch 1000...\n",
            "I Finished training epoch 1000 - loss: 0.006189\n",
            "I Training epoch 1001...\n",
            "I Finished training epoch 1001 - loss: 0.021090\n",
            "I Training epoch 1002...\n",
            "I Finished training epoch 1002 - loss: 0.018482\n",
            "I Training epoch 1003...\n",
            "I Finished training epoch 1003 - loss: 0.006370\n",
            "I Training epoch 1004...\n",
            "I Finished training epoch 1004 - loss: 0.015415\n",
            "I Training epoch 1005...\n",
            "I Finished training epoch 1005 - loss: 0.013281\n",
            "I Training epoch 1006...\n",
            "I Finished training epoch 1006 - loss: 0.012338\n",
            "I Training epoch 1007...\n",
            "I Finished training epoch 1007 - loss: 0.010350\n",
            "I Training epoch 1008...\n",
            "I Finished training epoch 1008 - loss: 0.009133\n",
            "I Training epoch 1009...\n",
            "I Finished training epoch 1009 - loss: 0.013434\n",
            "I Training epoch 1010...\n",
            "I Finished training epoch 1010 - loss: 0.006908\n",
            "I Training epoch 1011...\n",
            "I Finished training epoch 1011 - loss: 0.832224\n",
            "I Training epoch 1012...\n",
            "I Finished training epoch 1012 - loss: 0.006958\n",
            "I Training epoch 1013...\n",
            "I Finished training epoch 1013 - loss: 0.016610\n",
            "I Training epoch 1014...\n",
            "I Finished training epoch 1014 - loss: 0.009967\n",
            "I Training epoch 1015...\n",
            "I Finished training epoch 1015 - loss: 0.011873\n",
            "I Training epoch 1016...\n",
            "I Finished training epoch 1016 - loss: 0.015300\n",
            "I Training epoch 1017...\n",
            "I Finished training epoch 1017 - loss: 0.008241\n",
            "I Training epoch 1018...\n",
            "I Finished training epoch 1018 - loss: 0.008753\n",
            "I Training epoch 1019...\n",
            "I Finished training epoch 1019 - loss: 0.009356\n",
            "I Training epoch 1020...\n",
            "I Finished training epoch 1020 - loss: 0.007766\n",
            "I Training epoch 1021...\n",
            "I Finished training epoch 1021 - loss: 0.010329\n",
            "I Training epoch 1022...\n",
            "I Finished training epoch 1022 - loss: 0.017983\n",
            "I Training epoch 1023...\n",
            "I Finished training epoch 1023 - loss: 0.008193\n",
            "I Training epoch 1024...\n",
            "I Finished training epoch 1024 - loss: 0.161845\n",
            "I Training epoch 1025...\n",
            "I Finished training epoch 1025 - loss: 0.049111\n",
            "I Training epoch 1026...\n",
            "I Finished training epoch 1026 - loss: 0.009215\n",
            "I Training epoch 1027...\n",
            "I Finished training epoch 1027 - loss: 0.009850\n",
            "I Training epoch 1028...\n",
            "I Finished training epoch 1028 - loss: 0.039359\n",
            "I Training epoch 1029...\n",
            "I Finished training epoch 1029 - loss: 0.009458\n",
            "I Training epoch 1030...\n",
            "I Finished training epoch 1030 - loss: 0.022116\n",
            "I Training epoch 1031...\n",
            "I Finished training epoch 1031 - loss: 0.006500\n",
            "I Training epoch 1032...\n",
            "I Finished training epoch 1032 - loss: 0.074622\n",
            "I Training epoch 1033...\n",
            "I Finished training epoch 1033 - loss: 0.007545\n",
            "I Training epoch 1034...\n",
            "I Finished training epoch 1034 - loss: 0.014946\n",
            "I Training epoch 1035...\n",
            "I Finished training epoch 1035 - loss: 0.097498\n",
            "I Training epoch 1036...\n",
            "I Finished training epoch 1036 - loss: 0.006283\n",
            "I Training epoch 1037...\n",
            "I Finished training epoch 1037 - loss: 0.006660\n",
            "I Training epoch 1038...\n",
            "I Finished training epoch 1038 - loss: 0.019108\n",
            "I Training epoch 1039...\n",
            "I Finished training epoch 1039 - loss: 0.009180\n",
            "I Training epoch 1040...\n",
            "I Finished training epoch 1040 - loss: 0.014407\n",
            "I Training epoch 1041...\n",
            "I Finished training epoch 1041 - loss: 0.005121\n",
            "I Training epoch 1042...\n",
            "I Finished training epoch 1042 - loss: 0.009341\n",
            "I Training epoch 1043...\n",
            "I Finished training epoch 1043 - loss: 0.005170\n",
            "I Training epoch 1044...\n",
            "I Finished training epoch 1044 - loss: 0.008583\n",
            "I Training epoch 1045...\n",
            "I Finished training epoch 1045 - loss: 0.014509\n",
            "I Training epoch 1046...\n",
            "I Finished training epoch 1046 - loss: 0.009935\n",
            "I Training epoch 1047...\n",
            "I Finished training epoch 1047 - loss: 0.008988\n",
            "I Training epoch 1048...\n",
            "I Finished training epoch 1048 - loss: 0.007879\n",
            "I Training epoch 1049...\n",
            "I Finished training epoch 1049 - loss: 0.006240\n",
            "I Training epoch 1050...\n",
            "I Finished training epoch 1050 - loss: 0.007153\n",
            "I Training epoch 1051...\n",
            "I Finished training epoch 1051 - loss: 0.004940\n",
            "I Training epoch 1052...\n",
            "I Finished training epoch 1052 - loss: 0.011592\n",
            "I Training epoch 1053...\n",
            "I Finished training epoch 1053 - loss: 0.007986\n",
            "I Training epoch 1054...\n",
            "I Finished training epoch 1054 - loss: 0.005128\n",
            "I Training epoch 1055...\n",
            "I Finished training epoch 1055 - loss: 0.009929\n",
            "I Training epoch 1056...\n",
            "I Finished training epoch 1056 - loss: 0.005634\n",
            "I Training epoch 1057...\n",
            "I Finished training epoch 1057 - loss: 0.011026\n",
            "I Training epoch 1058...\n",
            "I Finished training epoch 1058 - loss: 0.013953\n",
            "I Training epoch 1059...\n",
            "I Finished training epoch 1059 - loss: 0.009283\n",
            "I Training epoch 1060...\n",
            "I Finished training epoch 1060 - loss: 0.028374\n",
            "I Training epoch 1061...\n",
            "I Finished training epoch 1061 - loss: 0.429067\n",
            "I Training epoch 1062...\n",
            "I Finished training epoch 1062 - loss: 0.006385\n",
            "I Training epoch 1063...\n",
            "I Finished training epoch 1063 - loss: 0.003507\n",
            "I Training epoch 1064...\n",
            "I Finished training epoch 1064 - loss: 0.005095\n",
            "I Training epoch 1065...\n",
            "I Finished training epoch 1065 - loss: 0.003818\n",
            "I Training epoch 1066...\n",
            "I Finished training epoch 1066 - loss: 0.004062\n",
            "I Training epoch 1067...\n",
            "I Finished training epoch 1067 - loss: 0.010584\n",
            "I Training epoch 1068...\n",
            "I Finished training epoch 1068 - loss: 0.005882\n",
            "I Training epoch 1069...\n",
            "I Finished training epoch 1069 - loss: 0.005157\n",
            "I Training epoch 1070...\n",
            "I Finished training epoch 1070 - loss: 1.219768\n",
            "I Training epoch 1071...\n",
            "I Finished training epoch 1071 - loss: 0.008583\n",
            "I Training epoch 1072...\n",
            "I Finished training epoch 1072 - loss: 0.015244\n",
            "I Training epoch 1073...\n",
            "I Finished training epoch 1073 - loss: 0.103228\n",
            "I Training epoch 1074...\n",
            "I Finished training epoch 1074 - loss: 3.428559\n",
            "I Training epoch 1075...\n",
            "I Finished training epoch 1075 - loss: 0.039253\n",
            "I Training epoch 1076...\n",
            "I Finished training epoch 1076 - loss: 4.876190\n",
            "I Training epoch 1077...\n",
            "I Finished training epoch 1077 - loss: 2.013117\n",
            "I Training epoch 1078...\n",
            "I Finished training epoch 1078 - loss: 0.027920\n",
            "I Training epoch 1079...\n",
            "I Finished training epoch 1079 - loss: 0.014308\n",
            "I Training epoch 1080...\n",
            "I Finished training epoch 1080 - loss: 0.048988\n",
            "I Training epoch 1081...\n",
            "I Finished training epoch 1081 - loss: 0.070480\n",
            "I Training epoch 1082...\n",
            "I Finished training epoch 1082 - loss: 1.452972\n",
            "I Training epoch 1083...\n",
            "I Finished training epoch 1083 - loss: 0.031219\n",
            "I Training epoch 1084...\n",
            "I Finished training epoch 1084 - loss: 0.067624\n",
            "I Training epoch 1085...\n",
            "I Finished training epoch 1085 - loss: 0.124869\n",
            "I Training epoch 1086...\n",
            "I Finished training epoch 1086 - loss: 0.039255\n",
            "I Training epoch 1087...\n",
            "I Finished training epoch 1087 - loss: 0.037432\n",
            "I Training epoch 1088...\n",
            "I Finished training epoch 1088 - loss: 1.034043\n",
            "I Training epoch 1089...\n",
            "I Finished training epoch 1089 - loss: 0.052609\n",
            "I Training epoch 1090...\n",
            "I Finished training epoch 1090 - loss: 0.108159\n",
            "I Training epoch 1091...\n",
            "I Finished training epoch 1091 - loss: 0.664698\n",
            "I Training epoch 1092...\n",
            "I Finished training epoch 1092 - loss: 0.115808\n",
            "I Training epoch 1093...\n",
            "I Finished training epoch 1093 - loss: 0.748503\n",
            "I Training epoch 1094...\n",
            "I Finished training epoch 1094 - loss: 0.053843\n",
            "I Training epoch 1095...\n",
            "I Finished training epoch 1095 - loss: 0.069336\n",
            "I Training epoch 1096...\n",
            "I Finished training epoch 1096 - loss: 0.169126\n",
            "I Training epoch 1097...\n",
            "I Finished training epoch 1097 - loss: 0.090580\n",
            "I Training epoch 1098...\n",
            "I Finished training epoch 1098 - loss: 0.039779\n",
            "I Training epoch 1099...\n",
            "I Finished training epoch 1099 - loss: 0.278664\n",
            "I Training epoch 1100...\n",
            "I Finished training epoch 1100 - loss: 0.053517\n",
            "I Training epoch 1101...\n",
            "I Finished training epoch 1101 - loss: 0.047578\n",
            "I Training epoch 1102...\n",
            "I Finished training epoch 1102 - loss: 0.038822\n",
            "I Training epoch 1103...\n",
            "I Finished training epoch 1103 - loss: 0.030342\n",
            "I Training epoch 1104...\n",
            "I Finished training epoch 1104 - loss: 0.035606\n",
            "I Training epoch 1105...\n",
            "I Finished training epoch 1105 - loss: 0.150177\n",
            "I Training epoch 1106...\n",
            "I Finished training epoch 1106 - loss: 0.040909\n",
            "I Training epoch 1107...\n",
            "I Finished training epoch 1107 - loss: 0.152563\n",
            "I Training epoch 1108...\n",
            "I Finished training epoch 1108 - loss: 0.311776\n",
            "I Training epoch 1109...\n",
            "I Finished training epoch 1109 - loss: 2.743932\n",
            "I Training epoch 1110...\n",
            "I Finished training epoch 1110 - loss: 0.042738\n",
            "I Training epoch 1111...\n",
            "I Finished training epoch 1111 - loss: 0.338813\n",
            "I Training epoch 1112...\n",
            "I Finished training epoch 1112 - loss: 0.119058\n",
            "I Training epoch 1113...\n",
            "I Finished training epoch 1113 - loss: 2.156697\n",
            "I Training epoch 1114...\n",
            "I Finished training epoch 1114 - loss: 0.089856\n",
            "I Training epoch 1115...\n",
            "I Finished training epoch 1115 - loss: 0.172493\n",
            "I Training epoch 1116...\n",
            "I Finished training epoch 1116 - loss: 0.044890\n",
            "I Training epoch 1117...\n",
            "I Finished training epoch 1117 - loss: 0.025169\n",
            "I Training epoch 1118...\n",
            "I Finished training epoch 1118 - loss: 0.073152\n",
            "I Training epoch 1119...\n",
            "I Finished training epoch 1119 - loss: 0.048196\n",
            "I Training epoch 1120...\n",
            "I Finished training epoch 1120 - loss: 0.202041\n",
            "I Training epoch 1121...\n",
            "I Finished training epoch 1121 - loss: 0.021213\n",
            "I Training epoch 1122...\n",
            "I Finished training epoch 1122 - loss: 0.027599\n",
            "I Training epoch 1123...\n",
            "I Finished training epoch 1123 - loss: 0.101214\n",
            "I Training epoch 1124...\n",
            "I Finished training epoch 1124 - loss: 0.027150\n",
            "I Training epoch 1125...\n",
            "I Finished training epoch 1125 - loss: 0.183821\n",
            "I Training epoch 1126...\n",
            "I Finished training epoch 1126 - loss: 0.130032\n",
            "I Training epoch 1127...\n",
            "I Finished training epoch 1127 - loss: 0.018170\n",
            "I Training epoch 1128...\n",
            "I Finished training epoch 1128 - loss: 0.027466\n",
            "I Training epoch 1129...\n",
            "I Finished training epoch 1129 - loss: 0.107975\n",
            "I Training epoch 1130...\n",
            "I Finished training epoch 1130 - loss: 0.088583\n",
            "I Training epoch 1131...\n",
            "I Finished training epoch 1131 - loss: 0.028314\n",
            "I Training epoch 1132...\n",
            "I Finished training epoch 1132 - loss: 0.020510\n",
            "I Training epoch 1133...\n",
            "I Finished training epoch 1133 - loss: 0.059616\n",
            "I Training epoch 1134...\n",
            "I Finished training epoch 1134 - loss: 0.033444\n",
            "I Training epoch 1135...\n",
            "I Finished training epoch 1135 - loss: 0.030425\n",
            "I Training epoch 1136...\n",
            "I Finished training epoch 1136 - loss: 0.038233\n",
            "I Training epoch 1137...\n",
            "I Finished training epoch 1137 - loss: 0.379448\n",
            "I Training epoch 1138...\n",
            "I Finished training epoch 1138 - loss: 0.041641\n",
            "I Training epoch 1139...\n",
            "I Finished training epoch 1139 - loss: 0.030760\n",
            "I Training epoch 1140...\n",
            "I Finished training epoch 1140 - loss: 0.020762\n",
            "I Training epoch 1141...\n",
            "I Finished training epoch 1141 - loss: 0.128792\n",
            "I Training epoch 1142...\n",
            "I Finished training epoch 1142 - loss: 0.017331\n",
            "I Training epoch 1143...\n",
            "I Finished training epoch 1143 - loss: 0.011873\n",
            "I Training epoch 1144...\n",
            "I Finished training epoch 1144 - loss: 0.013400\n",
            "I Training epoch 1145...\n",
            "I Finished training epoch 1145 - loss: 0.013384\n",
            "I Training epoch 1146...\n",
            "I Finished training epoch 1146 - loss: 0.012963\n",
            "I Training epoch 1147...\n",
            "I Finished training epoch 1147 - loss: 0.050670\n",
            "I Training epoch 1148...\n",
            "I Finished training epoch 1148 - loss: 0.013215\n",
            "I Training epoch 1149...\n",
            "I Finished training epoch 1149 - loss: 0.054265\n",
            "I Training epoch 1150...\n",
            "I Finished training epoch 1150 - loss: 0.014865\n",
            "I Training epoch 1151...\n",
            "I Finished training epoch 1151 - loss: 0.014891\n",
            "I Training epoch 1152...\n",
            "I Finished training epoch 1152 - loss: 0.014251\n",
            "I Training epoch 1153...\n",
            "I Finished training epoch 1153 - loss: 0.013846\n",
            "I Training epoch 1154...\n",
            "I Finished training epoch 1154 - loss: 0.021296\n",
            "I Training epoch 1155...\n",
            "I Finished training epoch 1155 - loss: 0.023260\n",
            "I Training epoch 1156...\n",
            "I Finished training epoch 1156 - loss: 0.038794\n",
            "I Training epoch 1157...\n",
            "I Finished training epoch 1157 - loss: 0.025669\n",
            "I Training epoch 1158...\n",
            "I Finished training epoch 1158 - loss: 0.021375\n",
            "I Training epoch 1159...\n",
            "I Finished training epoch 1159 - loss: 0.006453\n",
            "I Training epoch 1160...\n",
            "I Finished training epoch 1160 - loss: 0.010216\n",
            "I Training epoch 1161...\n",
            "I Finished training epoch 1161 - loss: 0.066810\n",
            "I Training epoch 1162...\n",
            "I Finished training epoch 1162 - loss: 0.013956\n",
            "I Training epoch 1163...\n",
            "I Finished training epoch 1163 - loss: 0.037747\n",
            "I Training epoch 1164...\n",
            "I Finished training epoch 1164 - loss: 0.009257\n",
            "I Training epoch 1165...\n",
            "I Finished training epoch 1165 - loss: 0.037637\n",
            "I Training epoch 1166...\n",
            "I Finished training epoch 1166 - loss: 0.009467\n",
            "I Training epoch 1167...\n",
            "I Finished training epoch 1167 - loss: 0.019539\n",
            "I Training epoch 1168...\n",
            "I Finished training epoch 1168 - loss: 0.011450\n",
            "I Training epoch 1169...\n",
            "I Finished training epoch 1169 - loss: 0.018882\n",
            "I Training epoch 1170...\n",
            "I Finished training epoch 1170 - loss: 0.025316\n",
            "I Training epoch 1171...\n",
            "I Finished training epoch 1171 - loss: 0.015382\n",
            "I Training epoch 1172...\n",
            "I Finished training epoch 1172 - loss: 0.015521\n",
            "I Training epoch 1173...\n",
            "I Finished training epoch 1173 - loss: 0.011617\n",
            "I Training epoch 1174...\n",
            "I Finished training epoch 1174 - loss: 0.006896\n",
            "I Training epoch 1175...\n",
            "I Finished training epoch 1175 - loss: 0.027804\n",
            "I Training epoch 1176...\n",
            "I Finished training epoch 1176 - loss: 0.007161\n",
            "I Training epoch 1177...\n",
            "I Finished training epoch 1177 - loss: 0.009722\n",
            "I Training epoch 1178...\n",
            "I Finished training epoch 1178 - loss: 0.040547\n",
            "I Training epoch 1179...\n",
            "I Finished training epoch 1179 - loss: 0.013350\n",
            "I Training epoch 1180...\n",
            "I Finished training epoch 1180 - loss: 0.008642\n",
            "I Training epoch 1181...\n",
            "I Finished training epoch 1181 - loss: 0.028081\n",
            "I Training epoch 1182...\n",
            "I Finished training epoch 1182 - loss: 0.013178\n",
            "I Training epoch 1183...\n",
            "I Finished training epoch 1183 - loss: 0.008302\n",
            "I Training epoch 1184...\n",
            "I Finished training epoch 1184 - loss: 0.022110\n",
            "I Training epoch 1185...\n",
            "I Finished training epoch 1185 - loss: 0.010225\n",
            "I Training epoch 1186...\n",
            "I Finished training epoch 1186 - loss: 0.302036\n",
            "I Training epoch 1187...\n",
            "I Finished training epoch 1187 - loss: 0.005694\n",
            "I Training epoch 1188...\n",
            "I Finished training epoch 1188 - loss: 0.046698\n",
            "I Training epoch 1189...\n",
            "I Finished training epoch 1189 - loss: 0.018267\n",
            "I Training epoch 1190...\n",
            "I Finished training epoch 1190 - loss: 0.007601\n",
            "I Training epoch 1191...\n",
            "I Finished training epoch 1191 - loss: 0.013957\n",
            "I Training epoch 1192...\n",
            "I Finished training epoch 1192 - loss: 0.012986\n",
            "I Training epoch 1193...\n",
            "I Finished training epoch 1193 - loss: 0.006381\n",
            "I Training epoch 1194...\n",
            "I Finished training epoch 1194 - loss: 0.017624\n",
            "I Training epoch 1195...\n",
            "I Finished training epoch 1195 - loss: 0.006438\n",
            "I Training epoch 1196...\n",
            "I Finished training epoch 1196 - loss: 0.014588\n",
            "I Training epoch 1197...\n",
            "I Finished training epoch 1197 - loss: 0.035992\n",
            "I Training epoch 1198...\n",
            "I Finished training epoch 1198 - loss: 0.013430\n",
            "I Training epoch 1199...\n",
            "I Finished training epoch 1199 - loss: 0.015181\n",
            "I Training epoch 1200...\n",
            "I Finished training epoch 1200 - loss: 0.006415\n",
            "I Training epoch 1201...\n",
            "I Finished training epoch 1201 - loss: 0.020529\n",
            "I Training epoch 1202...\n",
            "I Finished training epoch 1202 - loss: 0.013638\n",
            "I Training epoch 1203...\n",
            "I Finished training epoch 1203 - loss: 0.008235\n",
            "I Training epoch 1204...\n",
            "I Finished training epoch 1204 - loss: 0.006333\n",
            "I Training epoch 1205...\n",
            "I Finished training epoch 1205 - loss: 0.005554\n",
            "I Training epoch 1206...\n",
            "I Finished training epoch 1206 - loss: 0.012727\n",
            "I Training epoch 1207...\n",
            "I Finished training epoch 1207 - loss: 0.013112\n",
            "I Training epoch 1208...\n",
            "I Finished training epoch 1208 - loss: 0.004412\n",
            "I Training epoch 1209...\n",
            "I Finished training epoch 1209 - loss: 0.013257\n",
            "I Training epoch 1210...\n",
            "I Finished training epoch 1210 - loss: 0.016792\n",
            "I Training epoch 1211...\n",
            "I Finished training epoch 1211 - loss: 0.014564\n",
            "I Training epoch 1212...\n",
            "I Finished training epoch 1212 - loss: 0.019605\n",
            "I Training epoch 1213...\n",
            "I Finished training epoch 1213 - loss: 0.006025\n",
            "I Training epoch 1214...\n",
            "I Finished training epoch 1214 - loss: 0.007721\n",
            "I Training epoch 1215...\n",
            "I Finished training epoch 1215 - loss: 0.010874\n",
            "I Training epoch 1216...\n",
            "I Finished training epoch 1216 - loss: 0.007814\n",
            "I Training epoch 1217...\n",
            "I Finished training epoch 1217 - loss: 0.009384\n",
            "I Training epoch 1218...\n",
            "I Finished training epoch 1218 - loss: 0.023257\n",
            "I Training epoch 1219...\n",
            "I Finished training epoch 1219 - loss: 0.010947\n",
            "I Training epoch 1220...\n",
            "I Finished training epoch 1220 - loss: 0.009931\n",
            "I Training epoch 1221...\n",
            "I Finished training epoch 1221 - loss: 0.010689\n",
            "I Training epoch 1222...\n",
            "I Finished training epoch 1222 - loss: 0.010789\n",
            "I Training epoch 1223...\n",
            "I Finished training epoch 1223 - loss: 0.007618\n",
            "I Training epoch 1224...\n",
            "I Finished training epoch 1224 - loss: 0.010024\n",
            "I Training epoch 1225...\n",
            "I Finished training epoch 1225 - loss: 0.007759\n",
            "I Training epoch 1226...\n",
            "I Finished training epoch 1226 - loss: 0.010383\n",
            "I Training epoch 1227...\n",
            "I Finished training epoch 1227 - loss: 0.008411\n",
            "I Training epoch 1228...\n",
            "I Finished training epoch 1228 - loss: 0.013997\n",
            "I Training epoch 1229...\n",
            "I Finished training epoch 1229 - loss: 0.012838\n",
            "I Training epoch 1230...\n",
            "I Finished training epoch 1230 - loss: 0.008082\n",
            "I Training epoch 1231...\n",
            "I Finished training epoch 1231 - loss: 0.006450\n",
            "I Training epoch 1232...\n",
            "I Finished training epoch 1232 - loss: 0.007057\n",
            "I Training epoch 1233...\n",
            "I Finished training epoch 1233 - loss: 0.005006\n",
            "I Training epoch 1234...\n",
            "I Finished training epoch 1234 - loss: 0.005834\n",
            "I Training epoch 1235...\n",
            "I Finished training epoch 1235 - loss: 0.005929\n",
            "I Training epoch 1236...\n",
            "I Finished training epoch 1236 - loss: 0.004541\n",
            "I Training epoch 1237...\n",
            "I Finished training epoch 1237 - loss: 0.013875\n",
            "I Training epoch 1238...\n",
            "I Finished training epoch 1238 - loss: 0.004641\n",
            "I Training epoch 1239...\n",
            "I Finished training epoch 1239 - loss: 0.007896\n",
            "I Training epoch 1240...\n",
            "I Finished training epoch 1240 - loss: 0.009586\n",
            "I Training epoch 1241...\n",
            "I Finished training epoch 1241 - loss: 0.007164\n",
            "I Training epoch 1242...\n",
            "I Finished training epoch 1242 - loss: 0.011140\n",
            "I Training epoch 1243...\n",
            "I Finished training epoch 1243 - loss: 0.006959\n",
            "I Training epoch 1244...\n",
            "I Finished training epoch 1244 - loss: 0.008390\n",
            "I Training epoch 1245...\n",
            "I Finished training epoch 1245 - loss: 0.012099\n",
            "I Training epoch 1246...\n",
            "I Finished training epoch 1246 - loss: 0.005232\n",
            "I Training epoch 1247...\n",
            "I Finished training epoch 1247 - loss: 0.005662\n",
            "I Training epoch 1248...\n",
            "I Finished training epoch 1248 - loss: 0.013442\n",
            "I Training epoch 1249...\n",
            "I Finished training epoch 1249 - loss: 0.013136\n",
            "I Training epoch 1250...\n",
            "I Finished training epoch 1250 - loss: 0.004336\n",
            "I Training epoch 1251...\n",
            "I Finished training epoch 1251 - loss: 0.006715\n",
            "I Training epoch 1252...\n",
            "I Finished training epoch 1252 - loss: 0.004802\n",
            "I Training epoch 1253...\n",
            "I Finished training epoch 1253 - loss: 0.015492\n",
            "I Training epoch 1254...\n",
            "I Finished training epoch 1254 - loss: 0.010317\n",
            "I Training epoch 1255...\n",
            "I Finished training epoch 1255 - loss: 0.010374\n",
            "I Training epoch 1256...\n",
            "I Finished training epoch 1256 - loss: 0.011345\n",
            "I Training epoch 1257...\n",
            "I Finished training epoch 1257 - loss: 0.006947\n",
            "I Training epoch 1258...\n",
            "I Finished training epoch 1258 - loss: 0.015567\n",
            "I Training epoch 1259...\n",
            "I Finished training epoch 1259 - loss: 0.009396\n",
            "I Training epoch 1260...\n",
            "I Finished training epoch 1260 - loss: 0.026097\n",
            "I Training epoch 1261...\n",
            "I Finished training epoch 1261 - loss: 0.028429\n",
            "I Training epoch 1262...\n",
            "I Finished training epoch 1262 - loss: 0.009287\n",
            "I Training epoch 1263...\n",
            "I Finished training epoch 1263 - loss: 0.062082\n",
            "I Training epoch 1264...\n",
            "I Finished training epoch 1264 - loss: 0.007459\n",
            "I Training epoch 1265...\n",
            "I Finished training epoch 1265 - loss: 0.008094\n",
            "I Training epoch 1266...\n",
            "I Finished training epoch 1266 - loss: 0.008127\n",
            "I Training epoch 1267...\n",
            "I Finished training epoch 1267 - loss: 0.007414\n",
            "I Training epoch 1268...\n",
            "I Finished training epoch 1268 - loss: 0.006940\n",
            "I Training epoch 1269...\n",
            "I Finished training epoch 1269 - loss: 0.007306\n",
            "I Training epoch 1270...\n",
            "I Finished training epoch 1270 - loss: 0.010512\n",
            "I Training epoch 1271...\n",
            "I Finished training epoch 1271 - loss: 0.006896\n",
            "I Training epoch 1272...\n",
            "I Finished training epoch 1272 - loss: 0.006740\n",
            "I Training epoch 1273...\n",
            "I Finished training epoch 1273 - loss: 0.008223\n",
            "I Training epoch 1274...\n",
            "I Finished training epoch 1274 - loss: 0.005336\n",
            "I Training epoch 1275...\n",
            "I Finished training epoch 1275 - loss: 0.013584\n",
            "I Training epoch 1276...\n",
            "I Finished training epoch 1276 - loss: 0.005608\n",
            "I Training epoch 1277...\n",
            "I Finished training epoch 1277 - loss: 0.019566\n",
            "I Training epoch 1278...\n",
            "I Finished training epoch 1278 - loss: 0.009366\n",
            "I Training epoch 1279...\n",
            "I Finished training epoch 1279 - loss: 0.010402\n",
            "I Training epoch 1280...\n",
            "I Finished training epoch 1280 - loss: 0.013645\n",
            "I Training epoch 1281...\n",
            "I Finished training epoch 1281 - loss: 0.010298\n",
            "I Training epoch 1282...\n",
            "I Finished training epoch 1282 - loss: 0.014016\n",
            "I Training epoch 1283...\n",
            "I Finished training epoch 1283 - loss: 0.010795\n",
            "I Training epoch 1284...\n",
            "I Finished training epoch 1284 - loss: 0.008333\n",
            "I Training epoch 1285...\n",
            "I Finished training epoch 1285 - loss: 0.008710\n",
            "I Training epoch 1286...\n",
            "I Finished training epoch 1286 - loss: 0.004511\n",
            "I Training epoch 1287...\n",
            "I Finished training epoch 1287 - loss: 0.004283\n",
            "I Training epoch 1288...\n",
            "I Finished training epoch 1288 - loss: 0.004085\n",
            "I Training epoch 1289...\n",
            "I Finished training epoch 1289 - loss: 0.006590\n",
            "I Training epoch 1290...\n",
            "I Finished training epoch 1290 - loss: 0.004566\n",
            "I Training epoch 1291...\n",
            "I Finished training epoch 1291 - loss: 0.005321\n",
            "I Training epoch 1292...\n",
            "I Finished training epoch 1292 - loss: 0.013049\n",
            "I Training epoch 1293...\n",
            "I Finished training epoch 1293 - loss: 0.012499\n",
            "I Training epoch 1294...\n",
            "I Finished training epoch 1294 - loss: 0.008345\n",
            "I Training epoch 1295...\n",
            "I Finished training epoch 1295 - loss: 0.007966\n",
            "I Training epoch 1296...\n",
            "I Finished training epoch 1296 - loss: 0.004377\n",
            "I Training epoch 1297...\n",
            "I Finished training epoch 1297 - loss: 0.003871\n",
            "I Training epoch 1298...\n",
            "I Finished training epoch 1298 - loss: 0.010684\n",
            "I Training epoch 1299...\n",
            "I Finished training epoch 1299 - loss: 0.015177\n",
            "I Training epoch 1300...\n",
            "I Finished training epoch 1300 - loss: 0.015781\n",
            "I Training epoch 1301...\n",
            "I Finished training epoch 1301 - loss: 0.006877\n",
            "I Training epoch 1302...\n",
            "I Finished training epoch 1302 - loss: 0.005843\n",
            "I Training epoch 1303...\n",
            "I Finished training epoch 1303 - loss: 0.018752\n",
            "I Training epoch 1304...\n",
            "I Finished training epoch 1304 - loss: 0.003929\n",
            "I Training epoch 1305...\n",
            "I Finished training epoch 1305 - loss: 0.005498\n",
            "I Training epoch 1306...\n",
            "I Finished training epoch 1306 - loss: 0.004197\n",
            "I Training epoch 1307...\n",
            "I Finished training epoch 1307 - loss: 0.009508\n",
            "I Training epoch 1308...\n",
            "I Finished training epoch 1308 - loss: 0.004253\n",
            "I Training epoch 1309...\n",
            "I Finished training epoch 1309 - loss: 0.004647\n",
            "I Training epoch 1310...\n",
            "I Finished training epoch 1310 - loss: 0.004655\n",
            "I Training epoch 1311...\n",
            "I Finished training epoch 1311 - loss: 0.011767\n",
            "I Training epoch 1312...\n",
            "I Finished training epoch 1312 - loss: 0.004767\n",
            "I Training epoch 1313...\n",
            "I Finished training epoch 1313 - loss: 0.004579\n",
            "I Training epoch 1314...\n",
            "I Finished training epoch 1314 - loss: 0.005623\n",
            "I Training epoch 1315...\n",
            "I Finished training epoch 1315 - loss: 0.004231\n",
            "I Training epoch 1316...\n",
            "I Finished training epoch 1316 - loss: 0.005485\n",
            "I Training epoch 1317...\n",
            "I Finished training epoch 1317 - loss: 0.054339\n",
            "I Training epoch 1318...\n",
            "I Finished training epoch 1318 - loss: 0.007180\n",
            "I Training epoch 1319...\n",
            "I Finished training epoch 1319 - loss: 0.008339\n",
            "I Training epoch 1320...\n",
            "I Finished training epoch 1320 - loss: 0.007166\n",
            "I Training epoch 1321...\n",
            "I Finished training epoch 1321 - loss: 0.021488\n",
            "I Training epoch 1322...\n",
            "I Finished training epoch 1322 - loss: 0.044140\n",
            "I Training epoch 1323...\n",
            "I Finished training epoch 1323 - loss: 0.003896\n",
            "I Training epoch 1324...\n",
            "I Finished training epoch 1324 - loss: 0.009667\n",
            "I Training epoch 1325...\n",
            "I Finished training epoch 1325 - loss: 0.019603\n",
            "I Training epoch 1326...\n",
            "I Finished training epoch 1326 - loss: 0.004492\n",
            "I Training epoch 1327...\n",
            "I Finished training epoch 1327 - loss: 0.003172\n",
            "I Training epoch 1328...\n",
            "I Finished training epoch 1328 - loss: 0.007100\n",
            "I Training epoch 1329...\n",
            "I Finished training epoch 1329 - loss: 0.004708\n",
            "I Training epoch 1330...\n",
            "I Finished training epoch 1330 - loss: 0.004757\n",
            "I Training epoch 1331...\n",
            "I Finished training epoch 1331 - loss: 0.010914\n",
            "I Training epoch 1332...\n",
            "I Finished training epoch 1332 - loss: 0.013339\n",
            "I Training epoch 1333...\n",
            "I Finished training epoch 1333 - loss: 0.003665\n",
            "I Training epoch 1334...\n",
            "I Finished training epoch 1334 - loss: 0.004869\n",
            "I Training epoch 1335...\n",
            "I Finished training epoch 1335 - loss: 0.005871\n",
            "I Training epoch 1336...\n",
            "I Finished training epoch 1336 - loss: 0.006219\n",
            "I Training epoch 1337...\n",
            "I Finished training epoch 1337 - loss: 0.004173\n",
            "I Training epoch 1338...\n",
            "I Finished training epoch 1338 - loss: 0.010301\n",
            "I Training epoch 1339...\n",
            "I Finished training epoch 1339 - loss: 0.014957\n",
            "I Training epoch 1340...\n",
            "I Finished training epoch 1340 - loss: 0.015507\n",
            "I Training epoch 1341...\n",
            "I Finished training epoch 1341 - loss: 0.008309\n",
            "I Training epoch 1342...\n",
            "I Finished training epoch 1342 - loss: 0.010458\n",
            "I Training epoch 1343...\n",
            "I Finished training epoch 1343 - loss: 0.034850\n",
            "I Training epoch 1344...\n",
            "I Finished training epoch 1344 - loss: 0.007191\n",
            "I Training epoch 1345...\n",
            "I Finished training epoch 1345 - loss: 0.006993\n",
            "I Training epoch 1346...\n",
            "I Finished training epoch 1346 - loss: 0.007927\n",
            "I Training epoch 1347...\n",
            "I Finished training epoch 1347 - loss: 0.004633\n",
            "I Training epoch 1348...\n",
            "I Finished training epoch 1348 - loss: 0.006051\n",
            "I Training epoch 1349...\n",
            "I Finished training epoch 1349 - loss: 0.009096\n",
            "I Training epoch 1350...\n",
            "I Finished training epoch 1350 - loss: 0.005064\n",
            "I Training epoch 1351...\n",
            "I Finished training epoch 1351 - loss: 0.006667\n",
            "I Training epoch 1352...\n",
            "I Finished training epoch 1352 - loss: 0.005920\n",
            "I Training epoch 1353...\n",
            "I Finished training epoch 1353 - loss: 0.013804\n",
            "I Training epoch 1354...\n",
            "I Finished training epoch 1354 - loss: 0.021509\n",
            "I Training epoch 1355...\n",
            "I Finished training epoch 1355 - loss: 0.022964\n",
            "I Training epoch 1356...\n",
            "I Finished training epoch 1356 - loss: 0.004646\n",
            "I Training epoch 1357...\n",
            "I Finished training epoch 1357 - loss: 0.005511\n",
            "I Training epoch 1358...\n",
            "I Finished training epoch 1358 - loss: 0.004036\n",
            "I Training epoch 1359...\n",
            "I Finished training epoch 1359 - loss: 0.003291\n",
            "I Training epoch 1360...\n",
            "I Finished training epoch 1360 - loss: 0.006398\n",
            "I Training epoch 1361...\n",
            "I Finished training epoch 1361 - loss: 0.003765\n",
            "I Training epoch 1362...\n",
            "I Finished training epoch 1362 - loss: 0.004066\n",
            "I Training epoch 1363...\n",
            "I Finished training epoch 1363 - loss: 0.004959\n",
            "I Training epoch 1364...\n",
            "I Finished training epoch 1364 - loss: 0.005616\n",
            "I Training epoch 1365...\n",
            "I Finished training epoch 1365 - loss: 0.004972\n",
            "I Training epoch 1366...\n",
            "I Finished training epoch 1366 - loss: 0.004286\n",
            "I Training epoch 1367...\n",
            "I Finished training epoch 1367 - loss: 0.004239\n",
            "I Training epoch 1368...\n",
            "I Finished training epoch 1368 - loss: 0.069162\n",
            "I Training epoch 1369...\n",
            "I Finished training epoch 1369 - loss: 0.003772\n",
            "I Training epoch 1370...\n",
            "I Finished training epoch 1370 - loss: 0.006987\n",
            "I Training epoch 1371...\n",
            "I Finished training epoch 1371 - loss: 0.006999\n",
            "I Training epoch 1372...\n",
            "I Finished training epoch 1372 - loss: 0.005852\n",
            "I Training epoch 1373...\n",
            "I Finished training epoch 1373 - loss: 0.167304\n",
            "I Training epoch 1374...\n",
            "I Finished training epoch 1374 - loss: 0.006680\n",
            "I Training epoch 1375...\n",
            "I Finished training epoch 1375 - loss: 0.009553\n",
            "I Training epoch 1376...\n",
            "I Finished training epoch 1376 - loss: 0.004504\n",
            "I Training epoch 1377...\n",
            "I Finished training epoch 1377 - loss: 0.039738\n",
            "I Training epoch 1378...\n",
            "I Finished training epoch 1378 - loss: 0.004852\n",
            "I Training epoch 1379...\n",
            "I Finished training epoch 1379 - loss: 0.003200\n",
            "I Training epoch 1380...\n",
            "I Finished training epoch 1380 - loss: 0.009424\n",
            "I Training epoch 1381...\n",
            "I Finished training epoch 1381 - loss: 0.006038\n",
            "I Training epoch 1382...\n",
            "I Finished training epoch 1382 - loss: 0.005463\n",
            "I Training epoch 1383...\n",
            "I Finished training epoch 1383 - loss: 0.043436\n",
            "I Training epoch 1384...\n",
            "I Finished training epoch 1384 - loss: 0.009691\n",
            "I Training epoch 1385...\n",
            "I Finished training epoch 1385 - loss: 0.032307\n",
            "I Training epoch 1386...\n",
            "I Finished training epoch 1386 - loss: 0.012152\n",
            "I Training epoch 1387...\n",
            "I Finished training epoch 1387 - loss: 0.004848\n",
            "I Training epoch 1388...\n",
            "I Finished training epoch 1388 - loss: 0.005121\n",
            "I Training epoch 1389...\n",
            "I Finished training epoch 1389 - loss: 0.014525\n",
            "I Training epoch 1390...\n",
            "I Finished training epoch 1390 - loss: 0.004823\n",
            "I Training epoch 1391...\n",
            "I Finished training epoch 1391 - loss: 0.019431\n",
            "I Training epoch 1392...\n",
            "I Finished training epoch 1392 - loss: 0.004067\n",
            "I Training epoch 1393...\n",
            "I Finished training epoch 1393 - loss: 0.007938\n",
            "I Training epoch 1394...\n",
            "I Finished training epoch 1394 - loss: 0.008836\n",
            "I Training epoch 1395...\n",
            "I Finished training epoch 1395 - loss: 0.007764\n",
            "I Training epoch 1396...\n",
            "I Finished training epoch 1396 - loss: 0.006327\n",
            "I Training epoch 1397...\n",
            "I Finished training epoch 1397 - loss: 0.006533\n",
            "I Training epoch 1398...\n",
            "I Finished training epoch 1398 - loss: 0.003633\n",
            "I Training epoch 1399...\n",
            "I Finished training epoch 1399 - loss: 0.008576\n",
            "I Training epoch 1400...\n",
            "I Finished training epoch 1400 - loss: 0.002612\n",
            "I Training epoch 1401...\n",
            "I Finished training epoch 1401 - loss: 0.005398\n",
            "I Training epoch 1402...\n",
            "I Finished training epoch 1402 - loss: 0.003724\n",
            "I Training epoch 1403...\n",
            "I Finished training epoch 1403 - loss: 0.010171\n",
            "I Training epoch 1404...\n",
            "I Finished training epoch 1404 - loss: 0.007004\n",
            "I Training epoch 1405...\n",
            "I Finished training epoch 1405 - loss: 0.006486\n",
            "I Training epoch 1406...\n",
            "I Finished training epoch 1406 - loss: 0.007288\n",
            "I Training epoch 1407...\n",
            "I Finished training epoch 1407 - loss: 0.006719\n",
            "I Training epoch 1408...\n",
            "I Finished training epoch 1408 - loss: 0.003133\n",
            "I Training epoch 1409...\n",
            "I Finished training epoch 1409 - loss: 0.010773\n",
            "I Training epoch 1410...\n",
            "I Finished training epoch 1410 - loss: 0.006815\n",
            "I Training epoch 1411...\n",
            "I Finished training epoch 1411 - loss: 0.012658\n",
            "I Training epoch 1412...\n",
            "I Finished training epoch 1412 - loss: 0.006314\n",
            "I Training epoch 1413...\n",
            "I Finished training epoch 1413 - loss: 0.097982\n",
            "I Training epoch 1414...\n",
            "I Finished training epoch 1414 - loss: 0.005382\n",
            "I Training epoch 1415...\n",
            "I Finished training epoch 1415 - loss: 0.007464\n",
            "I Training epoch 1416...\n",
            "I Finished training epoch 1416 - loss: 0.014870\n",
            "I Training epoch 1417...\n",
            "I Finished training epoch 1417 - loss: 0.014394\n",
            "I Training epoch 1418...\n",
            "I Finished training epoch 1418 - loss: 0.004903\n",
            "I Training epoch 1419...\n",
            "I Finished training epoch 1419 - loss: 0.003551\n",
            "I Training epoch 1420...\n",
            "I Finished training epoch 1420 - loss: 0.006849\n",
            "I Training epoch 1421...\n",
            "I Finished training epoch 1421 - loss: 0.009906\n",
            "I Training epoch 1422...\n",
            "I Finished training epoch 1422 - loss: 0.004490\n",
            "I Training epoch 1423...\n",
            "I Finished training epoch 1423 - loss: 0.006460\n",
            "I Training epoch 1424...\n",
            "I Finished training epoch 1424 - loss: 0.008798\n",
            "I Training epoch 1425...\n",
            "I Finished training epoch 1425 - loss: 0.006047\n",
            "I Training epoch 1426...\n",
            "I Finished training epoch 1426 - loss: 0.040544\n",
            "I Training epoch 1427...\n",
            "I Finished training epoch 1427 - loss: 0.003358\n",
            "I Training epoch 1428...\n",
            "I Finished training epoch 1428 - loss: 0.007509\n",
            "I Training epoch 1429...\n",
            "I Finished training epoch 1429 - loss: 0.004584\n",
            "I Training epoch 1430...\n",
            "I Finished training epoch 1430 - loss: 0.005868\n",
            "I Training epoch 1431...\n",
            "I Finished training epoch 1431 - loss: 0.004770\n",
            "I Training epoch 1432...\n",
            "I Finished training epoch 1432 - loss: 0.006417\n",
            "I Training epoch 1433...\n",
            "I Finished training epoch 1433 - loss: 0.004797\n",
            "I Training epoch 1434...\n",
            "I Finished training epoch 1434 - loss: 0.004717\n",
            "I Training epoch 1435...\n",
            "I Finished training epoch 1435 - loss: 0.015117\n",
            "I Training epoch 1436...\n",
            "I Finished training epoch 1436 - loss: 0.003538\n",
            "I Training epoch 1437...\n",
            "I Finished training epoch 1437 - loss: 0.003669\n",
            "I Training epoch 1438...\n",
            "I Finished training epoch 1438 - loss: 0.004894\n",
            "I Training epoch 1439...\n",
            "I Finished training epoch 1439 - loss: 0.003891\n",
            "I Training epoch 1440...\n",
            "I Finished training epoch 1440 - loss: 0.039811\n",
            "I Training epoch 1441...\n",
            "I Finished training epoch 1441 - loss: 0.003936\n",
            "I Training epoch 1442...\n",
            "I Finished training epoch 1442 - loss: 0.047384\n",
            "I Training epoch 1443...\n",
            "I Finished training epoch 1443 - loss: 0.019913\n",
            "I Training epoch 1444...\n",
            "I Finished training epoch 1444 - loss: 0.004853\n",
            "I Training epoch 1445...\n",
            "I Finished training epoch 1445 - loss: 0.005450\n",
            "I Training epoch 1446...\n",
            "I Finished training epoch 1446 - loss: 0.017873\n",
            "I Training epoch 1447...\n",
            "I Finished training epoch 1447 - loss: 0.015319\n",
            "I Training epoch 1448...\n",
            "I Finished training epoch 1448 - loss: 0.004797\n",
            "I Training epoch 1449...\n",
            "I Finished training epoch 1449 - loss: 0.005835\n",
            "I Training epoch 1450...\n",
            "I Finished training epoch 1450 - loss: 0.004693\n",
            "I Training epoch 1451...\n",
            "I Finished training epoch 1451 - loss: 0.006275\n",
            "I Training epoch 1452...\n",
            "I Finished training epoch 1452 - loss: 0.004349\n",
            "I Training epoch 1453...\n",
            "I Finished training epoch 1453 - loss: 0.003961\n",
            "I Training epoch 1454...\n",
            "I Finished training epoch 1454 - loss: 0.006155\n",
            "I Training epoch 1455...\n",
            "I Finished training epoch 1455 - loss: 0.002048\n",
            "I Training epoch 1456...\n",
            "I Finished training epoch 1456 - loss: 0.004314\n",
            "I Training epoch 1457...\n",
            "I Finished training epoch 1457 - loss: 0.004883\n",
            "I Training epoch 1458...\n",
            "I Finished training epoch 1458 - loss: 0.005900\n",
            "I Training epoch 1459...\n",
            "I Finished training epoch 1459 - loss: 0.002178\n",
            "I Training epoch 1460...\n",
            "I Finished training epoch 1460 - loss: 0.025615\n",
            "I Training epoch 1461...\n",
            "I Finished training epoch 1461 - loss: 0.010836\n",
            "I Training epoch 1462...\n",
            "I Finished training epoch 1462 - loss: 0.003970\n",
            "I Training epoch 1463...\n",
            "I Finished training epoch 1463 - loss: 0.002661\n",
            "I Training epoch 1464...\n",
            "I Finished training epoch 1464 - loss: 0.004850\n",
            "I Training epoch 1465...\n",
            "I Finished training epoch 1465 - loss: 0.006594\n",
            "I Training epoch 1466...\n",
            "I Finished training epoch 1466 - loss: 0.010432\n",
            "I Training epoch 1467...\n",
            "I Finished training epoch 1467 - loss: 0.016317\n",
            "I Training epoch 1468...\n",
            "I Finished training epoch 1468 - loss: 0.005738\n",
            "I Training epoch 1469...\n",
            "I Finished training epoch 1469 - loss: 0.079787\n",
            "I Training epoch 1470...\n",
            "I Finished training epoch 1470 - loss: 0.009852\n",
            "I Training epoch 1471...\n",
            "I Finished training epoch 1471 - loss: 0.005518\n",
            "I Training epoch 1472...\n",
            "I Finished training epoch 1472 - loss: 0.004002\n",
            "I Training epoch 1473...\n",
            "I Finished training epoch 1473 - loss: 0.005794\n",
            "I Training epoch 1474...\n",
            "I Finished training epoch 1474 - loss: 0.008983\n",
            "I Training epoch 1475...\n",
            "I Finished training epoch 1475 - loss: 0.003633\n",
            "I Training epoch 1476...\n",
            "I Finished training epoch 1476 - loss: 0.005504\n",
            "I Training epoch 1477...\n",
            "I Finished training epoch 1477 - loss: 0.006168\n",
            "I Training epoch 1478...\n",
            "I Finished training epoch 1478 - loss: 0.006205\n",
            "I Training epoch 1479...\n",
            "I Finished training epoch 1479 - loss: 0.006244\n",
            "I Training epoch 1480...\n",
            "I Finished training epoch 1480 - loss: 0.003853\n",
            "I Training epoch 1481...\n",
            "I Finished training epoch 1481 - loss: 0.003763\n",
            "I Training epoch 1482...\n",
            "I Finished training epoch 1482 - loss: 0.003018\n",
            "I Training epoch 1483...\n",
            "I Finished training epoch 1483 - loss: 0.005129\n",
            "I Training epoch 1484...\n",
            "I Finished training epoch 1484 - loss: 2.815468\n",
            "I Training epoch 1485...\n",
            "I Finished training epoch 1485 - loss: 0.009725\n",
            "I Training epoch 1486...\n",
            "I Finished training epoch 1486 - loss: 4.569840\n",
            "I Training epoch 1487...\n",
            "I Finished training epoch 1487 - loss: 2.903309\n",
            "I Training epoch 1488...\n",
            "I Finished training epoch 1488 - loss: 1.510344\n",
            "I Training epoch 1489...\n",
            "I Finished training epoch 1489 - loss: 3.847553\n",
            "I Training epoch 1490...\n",
            "I Finished training epoch 1490 - loss: 1.671667\n",
            "I Training epoch 1491...\n",
            "I Finished training epoch 1491 - loss: 4.187746\n",
            "I Training epoch 1492...\n",
            "I Finished training epoch 1492 - loss: 3.702822\n",
            "I Training epoch 1493...\n",
            "I Finished training epoch 1493 - loss: 0.287608\n",
            "I Training epoch 1494...\n",
            "I Finished training epoch 1494 - loss: 1.149445\n",
            "I Training epoch 1495...\n",
            "I Finished training epoch 1495 - loss: 0.578602\n",
            "I Training epoch 1496...\n",
            "I Finished training epoch 1496 - loss: 3.691423\n",
            "I Training epoch 1497...\n",
            "I Finished training epoch 1497 - loss: 0.627383\n",
            "I Training epoch 1498...\n",
            "I Finished training epoch 1498 - loss: 0.746971\n",
            "I Training epoch 1499...\n",
            "I Finished training epoch 1499 - loss: 0.796207\n",
            "I Training epoch 1500...\n",
            "I Finished training epoch 1500 - loss: 0.517414\n",
            "I Training epoch 1501...\n",
            "I Finished training epoch 1501 - loss: 2.266197\n",
            "I Training epoch 1502...\n",
            "I Finished training epoch 1502 - loss: 0.486423\n",
            "I Training epoch 1503...\n",
            "I Finished training epoch 1503 - loss: 0.340888\n",
            "I Training epoch 1504...\n",
            "I Finished training epoch 1504 - loss: 0.932022\n",
            "I Training epoch 1505...\n",
            "I Finished training epoch 1505 - loss: 1.074968\n",
            "I Training epoch 1506...\n",
            "I Finished training epoch 1506 - loss: 0.212480\n",
            "I Training epoch 1507...\n",
            "I Finished training epoch 1507 - loss: 0.280212\n",
            "I Training epoch 1508...\n",
            "I Finished training epoch 1508 - loss: 0.391560\n",
            "I Training epoch 1509...\n",
            "I Finished training epoch 1509 - loss: 0.529136\n",
            "I Training epoch 1510...\n",
            "I Finished training epoch 1510 - loss: 0.172223\n",
            "I Training epoch 1511...\n",
            "I Finished training epoch 1511 - loss: 0.267823\n",
            "I Training epoch 1512...\n",
            "I Finished training epoch 1512 - loss: 0.263864\n",
            "I Training epoch 1513...\n",
            "I Finished training epoch 1513 - loss: 0.126831\n",
            "I Training epoch 1514...\n",
            "I Finished training epoch 1514 - loss: 0.185634\n",
            "I Training epoch 1515...\n",
            "I Finished training epoch 1515 - loss: 0.099783\n",
            "I Training epoch 1516...\n",
            "I Finished training epoch 1516 - loss: 0.133428\n",
            "I Training epoch 1517...\n",
            "I Finished training epoch 1517 - loss: 0.631388\n",
            "I Training epoch 1518...\n",
            "I Finished training epoch 1518 - loss: 0.121646\n",
            "I Training epoch 1519...\n",
            "I Finished training epoch 1519 - loss: 0.188089\n",
            "I Training epoch 1520...\n",
            "I Finished training epoch 1520 - loss: 0.093110\n",
            "I Training epoch 1521...\n",
            "I Finished training epoch 1521 - loss: 0.289190\n",
            "I Training epoch 1522...\n",
            "I Finished training epoch 1522 - loss: 0.063475\n",
            "I Training epoch 1523...\n",
            "I Finished training epoch 1523 - loss: 0.087391\n",
            "I Training epoch 1524...\n",
            "I Finished training epoch 1524 - loss: 0.199374\n",
            "I Training epoch 1525...\n",
            "I Finished training epoch 1525 - loss: 0.158016\n",
            "I Training epoch 1526...\n",
            "I Finished training epoch 1526 - loss: 0.131162\n",
            "I Training epoch 1527...\n",
            "I Finished training epoch 1527 - loss: 0.140376\n",
            "I Training epoch 1528...\n",
            "I Finished training epoch 1528 - loss: 0.059024\n",
            "I Training epoch 1529...\n",
            "I Finished training epoch 1529 - loss: 0.032738\n",
            "I Training epoch 1530...\n",
            "I Finished training epoch 1530 - loss: 0.065315\n",
            "I Training epoch 1531...\n",
            "I Finished training epoch 1531 - loss: 0.046760\n",
            "I Training epoch 1532...\n",
            "I Finished training epoch 1532 - loss: 0.034888\n",
            "I Training epoch 1533...\n",
            "I Finished training epoch 1533 - loss: 0.088325\n",
            "I Training epoch 1534...\n",
            "I Finished training epoch 1534 - loss: 0.059565\n",
            "I Training epoch 1535...\n",
            "I Finished training epoch 1535 - loss: 0.042030\n",
            "I Training epoch 1536...\n",
            "I Finished training epoch 1536 - loss: 0.094269\n",
            "I Training epoch 1537...\n",
            "I Finished training epoch 1537 - loss: 0.063584\n",
            "I Training epoch 1538...\n",
            "I Finished training epoch 1538 - loss: 0.116160\n",
            "I Training epoch 1539...\n",
            "I Finished training epoch 1539 - loss: 0.043223\n",
            "I Training epoch 1540...\n",
            "I Finished training epoch 1540 - loss: 0.026117\n",
            "I Training epoch 1541...\n",
            "I Finished training epoch 1541 - loss: 0.021791\n",
            "I Training epoch 1542...\n",
            "I Finished training epoch 1542 - loss: 0.037730\n",
            "I Training epoch 1543...\n",
            "I Finished training epoch 1543 - loss: 0.036336\n",
            "I Training epoch 1544...\n",
            "I Finished training epoch 1544 - loss: 0.019644\n",
            "I Training epoch 1545...\n",
            "I Finished training epoch 1545 - loss: 0.055664\n",
            "I Training epoch 1546...\n",
            "I Finished training epoch 1546 - loss: 0.021910\n",
            "I Training epoch 1547...\n",
            "I Finished training epoch 1547 - loss: 0.029207\n",
            "I Training epoch 1548...\n",
            "I Finished training epoch 1548 - loss: 0.024768\n",
            "I Training epoch 1549...\n",
            "I Finished training epoch 1549 - loss: 0.032156\n",
            "I Training epoch 1550...\n",
            "I Finished training epoch 1550 - loss: 0.027725\n",
            "I Training epoch 1551...\n",
            "I Finished training epoch 1551 - loss: 0.025363\n",
            "I Training epoch 1552...\n",
            "I Finished training epoch 1552 - loss: 0.015106\n",
            "I Training epoch 1553...\n",
            "I Finished training epoch 1553 - loss: 0.021530\n",
            "I Training epoch 1554...\n",
            "I Finished training epoch 1554 - loss: 0.153247\n",
            "I Training epoch 1555...\n",
            "I Finished training epoch 1555 - loss: 0.026138\n",
            "I Training epoch 1556...\n",
            "I Finished training epoch 1556 - loss: 0.029621\n",
            "I Training epoch 1557...\n",
            "I Finished training epoch 1557 - loss: 0.015717\n",
            "I Training epoch 1558...\n",
            "I Finished training epoch 1558 - loss: 0.022541\n",
            "I Training epoch 1559...\n",
            "I Finished training epoch 1559 - loss: 0.026811\n",
            "I Training epoch 1560...\n",
            "I Finished training epoch 1560 - loss: 0.018091\n",
            "I Training epoch 1561...\n",
            "I Finished training epoch 1561 - loss: 0.036067\n",
            "I Training epoch 1562...\n",
            "I Finished training epoch 1562 - loss: 0.138642\n",
            "I Training epoch 1563...\n",
            "I Finished training epoch 1563 - loss: 0.018575\n",
            "I Training epoch 1564...\n",
            "I Finished training epoch 1564 - loss: 0.019288\n",
            "I Training epoch 1565...\n",
            "I Finished training epoch 1565 - loss: 0.031781\n",
            "I Training epoch 1566...\n",
            "I Finished training epoch 1566 - loss: 0.024210\n",
            "I Training epoch 1567...\n",
            "I Finished training epoch 1567 - loss: 0.014984\n",
            "I Training epoch 1568...\n",
            "I Finished training epoch 1568 - loss: 0.019513\n",
            "I Training epoch 1569...\n",
            "I Finished training epoch 1569 - loss: 0.015605\n",
            "I Training epoch 1570...\n",
            "I Finished training epoch 1570 - loss: 0.263457\n",
            "I Training epoch 1571...\n",
            "I Finished training epoch 1571 - loss: 0.031561\n",
            "I Training epoch 1572...\n",
            "I Finished training epoch 1572 - loss: 0.033331\n",
            "I Training epoch 1573...\n",
            "I Finished training epoch 1573 - loss: 0.018095\n",
            "I Training epoch 1574...\n",
            "I Finished training epoch 1574 - loss: 0.016471\n",
            "I Training epoch 1575...\n",
            "I Finished training epoch 1575 - loss: 0.015681\n",
            "I Training epoch 1576...\n",
            "I Finished training epoch 1576 - loss: 0.032263\n",
            "I Training epoch 1577...\n",
            "I Finished training epoch 1577 - loss: 0.021801\n",
            "I Training epoch 1578...\n",
            "I Finished training epoch 1578 - loss: 0.012368\n",
            "I Training epoch 1579...\n",
            "I Finished training epoch 1579 - loss: 0.030469\n",
            "I Training epoch 1580...\n",
            "I Finished training epoch 1580 - loss: 0.016199\n",
            "I Training epoch 1581...\n",
            "I Finished training epoch 1581 - loss: 0.030862\n",
            "I Training epoch 1582...\n",
            "I Finished training epoch 1582 - loss: 0.064437\n",
            "I Training epoch 1583...\n",
            "I Finished training epoch 1583 - loss: 0.059323\n",
            "I Training epoch 1584...\n",
            "I Finished training epoch 1584 - loss: 0.829716\n",
            "I Training epoch 1585...\n",
            "I Finished training epoch 1585 - loss: 0.021075\n",
            "I Training epoch 1586...\n",
            "I Finished training epoch 1586 - loss: 0.118574\n",
            "I Training epoch 1587...\n",
            "I Finished training epoch 1587 - loss: 0.027723\n",
            "I Training epoch 1588...\n",
            "I Finished training epoch 1588 - loss: 0.024259\n",
            "I Training epoch 1589...\n",
            "I Finished training epoch 1589 - loss: 0.034437\n",
            "I Training epoch 1590...\n",
            "I Finished training epoch 1590 - loss: 0.018156\n",
            "I Training epoch 1591...\n",
            "I Finished training epoch 1591 - loss: 0.065918\n",
            "I Training epoch 1592...\n",
            "I Finished training epoch 1592 - loss: 0.025131\n",
            "I Training epoch 1593...\n",
            "I Finished training epoch 1593 - loss: 0.018734\n",
            "I Training epoch 1594...\n",
            "I Finished training epoch 1594 - loss: 0.029584\n",
            "I Training epoch 1595...\n",
            "I Finished training epoch 1595 - loss: 0.023069\n",
            "I Training epoch 1596...\n",
            "I Finished training epoch 1596 - loss: 0.028767\n",
            "I Training epoch 1597...\n",
            "I Finished training epoch 1597 - loss: 0.027605\n",
            "I Training epoch 1598...\n",
            "I Finished training epoch 1598 - loss: 0.033742\n",
            "I Training epoch 1599...\n",
            "I Finished training epoch 1599 - loss: 0.085909\n",
            "I Training epoch 1600...\n",
            "I Finished training epoch 1600 - loss: 0.043057\n",
            "I Training epoch 1601...\n",
            "I Finished training epoch 1601 - loss: 0.130019\n",
            "I Training epoch 1602...\n",
            "I Finished training epoch 1602 - loss: 0.025401\n",
            "I Training epoch 1603...\n",
            "I Finished training epoch 1603 - loss: 0.061705\n",
            "I Training epoch 1604...\n",
            "I Finished training epoch 1604 - loss: 0.171916\n",
            "I Training epoch 1605...\n",
            "I Finished training epoch 1605 - loss: 0.049667\n",
            "I Training epoch 1606...\n",
            "I Finished training epoch 1606 - loss: 0.045946\n",
            "I Training epoch 1607...\n",
            "I Finished training epoch 1607 - loss: 0.024234\n",
            "I Training epoch 1608...\n",
            "I Finished training epoch 1608 - loss: 0.023851\n",
            "I Training epoch 1609...\n",
            "I Finished training epoch 1609 - loss: 0.020132\n",
            "I Training epoch 1610...\n",
            "I Finished training epoch 1610 - loss: 0.058898\n",
            "I Training epoch 1611...\n",
            "I Finished training epoch 1611 - loss: 0.018143\n",
            "I Training epoch 1612...\n",
            "I Finished training epoch 1612 - loss: 0.022047\n",
            "I Training epoch 1613...\n",
            "I Finished training epoch 1613 - loss: 0.017703\n",
            "I Training epoch 1614...\n",
            "I Finished training epoch 1614 - loss: 0.047606\n",
            "I Training epoch 1615...\n",
            "I Finished training epoch 1615 - loss: 0.018745\n",
            "I Training epoch 1616...\n",
            "I Finished training epoch 1616 - loss: 0.010413\n",
            "I Training epoch 1617...\n",
            "I Finished training epoch 1617 - loss: 0.010355\n",
            "I Training epoch 1618...\n",
            "I Finished training epoch 1618 - loss: 0.013643\n",
            "I Training epoch 1619...\n",
            "I Finished training epoch 1619 - loss: 0.011178\n",
            "I Training epoch 1620...\n",
            "I Finished training epoch 1620 - loss: 0.026714\n",
            "I Training epoch 1621...\n",
            "I Finished training epoch 1621 - loss: 0.010347\n",
            "I Training epoch 1622...\n",
            "I Finished training epoch 1622 - loss: 0.019461\n",
            "I Training epoch 1623...\n",
            "I Finished training epoch 1623 - loss: 0.011055\n",
            "I Training epoch 1624...\n",
            "I Finished training epoch 1624 - loss: 0.032370\n",
            "I Training epoch 1625...\n",
            "I Finished training epoch 1625 - loss: 0.010291\n",
            "I Training epoch 1626...\n",
            "I Finished training epoch 1626 - loss: 0.010462\n",
            "I Training epoch 1627...\n",
            "I Finished training epoch 1627 - loss: 0.012234\n",
            "I Training epoch 1628...\n",
            "I Finished training epoch 1628 - loss: 0.020351\n",
            "I Training epoch 1629...\n",
            "I Finished training epoch 1629 - loss: 0.012520\n",
            "I Training epoch 1630...\n",
            "I Finished training epoch 1630 - loss: 0.013671\n",
            "I Training epoch 1631...\n",
            "I Finished training epoch 1631 - loss: 0.010606\n",
            "I Training epoch 1632...\n",
            "I Finished training epoch 1632 - loss: 0.018673\n",
            "I Training epoch 1633...\n",
            "I Finished training epoch 1633 - loss: 0.007301\n",
            "I Training epoch 1634...\n",
            "I Finished training epoch 1634 - loss: 0.008114\n",
            "I Training epoch 1635...\n",
            "I Finished training epoch 1635 - loss: 0.015648\n",
            "I Training epoch 1636...\n",
            "I Finished training epoch 1636 - loss: 0.057255\n",
            "I Training epoch 1637...\n",
            "I Finished training epoch 1637 - loss: 0.023000\n",
            "I Training epoch 1638...\n",
            "I Finished training epoch 1638 - loss: 0.021087\n",
            "I Training epoch 1639...\n",
            "I Finished training epoch 1639 - loss: 0.017486\n",
            "I Training epoch 1640...\n",
            "I Finished training epoch 1640 - loss: 0.024786\n",
            "I Training epoch 1641...\n",
            "I Finished training epoch 1641 - loss: 0.015519\n",
            "I Training epoch 1642...\n",
            "I Finished training epoch 1642 - loss: 0.012075\n",
            "I Training epoch 1643...\n",
            "I Finished training epoch 1643 - loss: 0.015241\n",
            "I Training epoch 1644...\n",
            "I Finished training epoch 1644 - loss: 0.006102\n",
            "I Training epoch 1645...\n",
            "I Finished training epoch 1645 - loss: 0.006433\n",
            "I Training epoch 1646...\n",
            "I Finished training epoch 1646 - loss: 0.014917\n",
            "I Training epoch 1647...\n",
            "I Finished training epoch 1647 - loss: 0.016405\n",
            "I Training epoch 1648...\n",
            "I Finished training epoch 1648 - loss: 0.005305\n",
            "I Training epoch 1649...\n",
            "I Finished training epoch 1649 - loss: 0.014738\n",
            "I Training epoch 1650...\n",
            "I Finished training epoch 1650 - loss: 0.010605\n",
            "I Training epoch 1651...\n",
            "I Finished training epoch 1651 - loss: 0.016516\n",
            "I Training epoch 1652...\n",
            "I Finished training epoch 1652 - loss: 0.019503\n",
            "I Training epoch 1653...\n",
            "I Finished training epoch 1653 - loss: 0.027553\n",
            "I Training epoch 1654...\n",
            "I Finished training epoch 1654 - loss: 0.010119\n",
            "I Training epoch 1655...\n",
            "I Finished training epoch 1655 - loss: 0.014705\n",
            "I Training epoch 1656...\n",
            "I Finished training epoch 1656 - loss: 0.018962\n",
            "I Training epoch 1657...\n",
            "I Finished training epoch 1657 - loss: 0.009473\n",
            "I Training epoch 1658...\n",
            "I Finished training epoch 1658 - loss: 0.008639\n",
            "I Training epoch 1659...\n",
            "I Finished training epoch 1659 - loss: 0.006851\n",
            "I Training epoch 1660...\n",
            "I Finished training epoch 1660 - loss: 0.005579\n",
            "I Training epoch 1661...\n",
            "I Finished training epoch 1661 - loss: 0.006208\n",
            "I Training epoch 1662...\n",
            "I Finished training epoch 1662 - loss: 0.012245\n",
            "I Training epoch 1663...\n",
            "I Finished training epoch 1663 - loss: 0.007233\n",
            "I Training epoch 1664...\n",
            "I Finished training epoch 1664 - loss: 0.009990\n",
            "I Training epoch 1665...\n",
            "I Finished training epoch 1665 - loss: 0.005643\n",
            "I Training epoch 1666...\n",
            "I Finished training epoch 1666 - loss: 0.014057\n",
            "I Training epoch 1667...\n",
            "I Finished training epoch 1667 - loss: 0.008151\n",
            "I Training epoch 1668...\n",
            "I Finished training epoch 1668 - loss: 0.006292\n",
            "I Training epoch 1669...\n",
            "I Finished training epoch 1669 - loss: 0.007772\n",
            "I Training epoch 1670...\n",
            "I Finished training epoch 1670 - loss: 0.008061\n",
            "I Training epoch 1671...\n",
            "I Finished training epoch 1671 - loss: 0.012069\n",
            "I Training epoch 1672...\n",
            "I Finished training epoch 1672 - loss: 0.014481\n",
            "I Training epoch 1673...\n",
            "I Finished training epoch 1673 - loss: 0.006903\n",
            "I Training epoch 1674...\n",
            "I Finished training epoch 1674 - loss: 0.006849\n",
            "I Training epoch 1675...\n",
            "I Finished training epoch 1675 - loss: 0.012521\n",
            "I Training epoch 1676...\n",
            "I Finished training epoch 1676 - loss: 0.005692\n",
            "I Training epoch 1677...\n",
            "I Finished training epoch 1677 - loss: 0.016908\n",
            "I Training epoch 1678...\n",
            "I Finished training epoch 1678 - loss: 0.011581\n",
            "I Training epoch 1679...\n",
            "I Finished training epoch 1679 - loss: 0.005384\n",
            "I Training epoch 1680...\n",
            "I Finished training epoch 1680 - loss: 0.006615\n",
            "I Training epoch 1681...\n",
            "I Finished training epoch 1681 - loss: 0.059666\n",
            "I Training epoch 1682...\n",
            "I Finished training epoch 1682 - loss: 0.006979\n",
            "I Training epoch 1683...\n",
            "I Finished training epoch 1683 - loss: 0.013769\n",
            "I Training epoch 1684...\n",
            "I Finished training epoch 1684 - loss: 0.011378\n",
            "I Training epoch 1685...\n",
            "I Finished training epoch 1685 - loss: 0.013495\n",
            "I Training epoch 1686...\n",
            "I Finished training epoch 1686 - loss: 0.005987\n",
            "I Training epoch 1687...\n",
            "I Finished training epoch 1687 - loss: 0.006303\n",
            "I Training epoch 1688...\n",
            "I Finished training epoch 1688 - loss: 0.007441\n",
            "I Training epoch 1689...\n",
            "I Finished training epoch 1689 - loss: 0.006880\n",
            "I Training epoch 1690...\n",
            "I Finished training epoch 1690 - loss: 0.005533\n",
            "I Training epoch 1691...\n",
            "I Finished training epoch 1691 - loss: 0.007553\n",
            "I Training epoch 1692...\n",
            "I Finished training epoch 1692 - loss: 0.015589\n",
            "I Training epoch 1693...\n",
            "I Finished training epoch 1693 - loss: 0.013946\n",
            "I Training epoch 1694...\n",
            "I Finished training epoch 1694 - loss: 0.007980\n",
            "I Training epoch 1695...\n",
            "I Finished training epoch 1695 - loss: 0.008620\n",
            "I Training epoch 1696...\n",
            "I Finished training epoch 1696 - loss: 0.011459\n",
            "I Training epoch 1697...\n",
            "I Finished training epoch 1697 - loss: 0.006280\n",
            "I Training epoch 1698...\n",
            "I Finished training epoch 1698 - loss: 0.012210\n",
            "I Training epoch 1699...\n",
            "I Finished training epoch 1699 - loss: 0.007412\n",
            "I Training epoch 1700...\n",
            "I Finished training epoch 1700 - loss: 0.024702\n",
            "I Training epoch 1701...\n",
            "I Finished training epoch 1701 - loss: 0.011184\n",
            "I Training epoch 1702...\n",
            "I Finished training epoch 1702 - loss: 0.048903\n",
            "I Training epoch 1703...\n",
            "I Finished training epoch 1703 - loss: 0.005766\n",
            "I Training epoch 1704...\n",
            "I Finished training epoch 1704 - loss: 0.005521\n",
            "I Training epoch 1705...\n",
            "I Finished training epoch 1705 - loss: 0.016170\n",
            "I Training epoch 1706...\n",
            "I Finished training epoch 1706 - loss: 0.012913\n",
            "I Training epoch 1707...\n",
            "I Finished training epoch 1707 - loss: 0.004888\n",
            "I Training epoch 1708...\n",
            "I Finished training epoch 1708 - loss: 0.013117\n",
            "I Training epoch 1709...\n",
            "I Finished training epoch 1709 - loss: 0.005290\n",
            "I Training epoch 1710...\n",
            "I Finished training epoch 1710 - loss: 0.531953\n",
            "I Training epoch 1711...\n",
            "I Finished training epoch 1711 - loss: 0.007587\n",
            "I Training epoch 1712...\n",
            "I Finished training epoch 1712 - loss: 0.006906\n",
            "I Training epoch 1713...\n",
            "I Finished training epoch 1713 - loss: 0.016497\n",
            "I Training epoch 1714...\n",
            "I Finished training epoch 1714 - loss: 0.005289\n",
            "I Training epoch 1715...\n",
            "I Finished training epoch 1715 - loss: 0.014131\n",
            "I Training epoch 1716...\n",
            "I Finished training epoch 1716 - loss: 0.008083\n",
            "I Training epoch 1717...\n",
            "I Finished training epoch 1717 - loss: 0.016512\n",
            "I Training epoch 1718...\n",
            "I Finished training epoch 1718 - loss: 0.006843\n",
            "I Training epoch 1719...\n",
            "I Finished training epoch 1719 - loss: 0.011613\n",
            "I Training epoch 1720...\n",
            "I Finished training epoch 1720 - loss: 0.008179\n",
            "I Training epoch 1721...\n",
            "I Finished training epoch 1721 - loss: 5.077734\n",
            "I Training epoch 1722...\n",
            "I Finished training epoch 1722 - loss: 0.007416\n",
            "I Training epoch 1723...\n",
            "I Finished training epoch 1723 - loss: 0.022153\n",
            "I Training epoch 1724...\n",
            "I Finished training epoch 1724 - loss: 0.010990\n",
            "I Training epoch 1725...\n",
            "I Finished training epoch 1725 - loss: 0.015542\n",
            "I Training epoch 1726...\n",
            "I Finished training epoch 1726 - loss: 0.016984\n",
            "I Training epoch 1727...\n",
            "I Finished training epoch 1727 - loss: 0.011335\n",
            "I Training epoch 1728...\n",
            "I Finished training epoch 1728 - loss: 0.015091\n",
            "I Training epoch 1729...\n",
            "I Finished training epoch 1729 - loss: 0.032703\n",
            "I Training epoch 1730...\n",
            "I Finished training epoch 1730 - loss: 0.018308\n",
            "I Training epoch 1731...\n",
            "I Finished training epoch 1731 - loss: 0.017147\n",
            "I Training epoch 1732...\n",
            "I Finished training epoch 1732 - loss: 0.007703\n",
            "I Training epoch 1733...\n",
            "I Finished training epoch 1733 - loss: 0.023368\n",
            "I Training epoch 1734...\n",
            "I Finished training epoch 1734 - loss: 0.035769\n",
            "I Training epoch 1735...\n",
            "I Finished training epoch 1735 - loss: 0.035638\n",
            "I Training epoch 1736...\n",
            "I Finished training epoch 1736 - loss: 0.078913\n",
            "I Training epoch 1737...\n",
            "I Finished training epoch 1737 - loss: 0.023714\n",
            "I Training epoch 1738...\n",
            "I Finished training epoch 1738 - loss: 0.017272\n",
            "I Training epoch 1739...\n",
            "I Finished training epoch 1739 - loss: 0.084637\n",
            "I Training epoch 1740...\n",
            "I Finished training epoch 1740 - loss: 0.038079\n",
            "I Training epoch 1741...\n",
            "I Finished training epoch 1741 - loss: 0.013087\n",
            "I Training epoch 1742...\n",
            "I Finished training epoch 1742 - loss: 0.035311\n",
            "I Training epoch 1743...\n",
            "I Finished training epoch 1743 - loss: 0.061319\n",
            "I Training epoch 1744...\n",
            "I Finished training epoch 1744 - loss: 0.036646\n",
            "I Training epoch 1745...\n",
            "I Finished training epoch 1745 - loss: 0.018764\n",
            "I Training epoch 1746...\n",
            "I Finished training epoch 1746 - loss: 0.026603\n",
            "I Training epoch 1747...\n",
            "I Finished training epoch 1747 - loss: 0.022821\n",
            "I Training epoch 1748...\n",
            "I Finished training epoch 1748 - loss: 0.017772\n",
            "I Training epoch 1749...\n",
            "I Finished training epoch 1749 - loss: 0.032061\n",
            "I Training epoch 1750...\n",
            "I Finished training epoch 1750 - loss: 0.037744\n",
            "I Training epoch 1751...\n",
            "I Finished training epoch 1751 - loss: 0.296085\n",
            "I Training epoch 1752...\n",
            "I Finished training epoch 1752 - loss: 0.009203\n",
            "I Training epoch 1753...\n",
            "I Finished training epoch 1753 - loss: 0.037373\n",
            "I Training epoch 1754...\n",
            "I Finished training epoch 1754 - loss: 0.007142\n",
            "I Training epoch 1755...\n",
            "I Finished training epoch 1755 - loss: 0.005451\n",
            "I Training epoch 1756...\n",
            "I Finished training epoch 1756 - loss: 0.017941\n",
            "I Training epoch 1757...\n",
            "I Finished training epoch 1757 - loss: 0.018725\n",
            "I Training epoch 1758...\n",
            "I Finished training epoch 1758 - loss: 0.007639\n",
            "I Training epoch 1759...\n",
            "I Finished training epoch 1759 - loss: 0.014933\n",
            "I Training epoch 1760...\n",
            "I Finished training epoch 1760 - loss: 0.004312\n",
            "I Training epoch 1761...\n",
            "I Finished training epoch 1761 - loss: 0.018465\n",
            "I Training epoch 1762...\n",
            "I Finished training epoch 1762 - loss: 0.007673\n",
            "I Training epoch 1763...\n",
            "I Finished training epoch 1763 - loss: 0.006969\n",
            "I Training epoch 1764...\n",
            "I Finished training epoch 1764 - loss: 0.009172\n",
            "I Training epoch 1765...\n",
            "I Finished training epoch 1765 - loss: 0.359312\n",
            "I Training epoch 1766...\n",
            "I Finished training epoch 1766 - loss: 0.012593\n",
            "I Training epoch 1767...\n",
            "I Finished training epoch 1767 - loss: 0.008519\n",
            "I Training epoch 1768...\n",
            "I Finished training epoch 1768 - loss: 0.005525\n",
            "I Training epoch 1769...\n",
            "I Finished training epoch 1769 - loss: 0.013250\n",
            "I Training epoch 1770...\n",
            "I Finished training epoch 1770 - loss: 0.012761\n",
            "I Training epoch 1771...\n",
            "I Finished training epoch 1771 - loss: 0.006147\n",
            "I Training epoch 1772...\n",
            "I Finished training epoch 1772 - loss: 0.008661\n",
            "I Training epoch 1773...\n",
            "I Finished training epoch 1773 - loss: 0.006324\n",
            "I Training epoch 1774...\n",
            "I Finished training epoch 1774 - loss: 0.030790\n",
            "I Training epoch 1775...\n",
            "I Finished training epoch 1775 - loss: 0.006273\n",
            "I Training epoch 1776...\n",
            "I Finished training epoch 1776 - loss: 0.471825\n",
            "I Training epoch 1777...\n",
            "I Finished training epoch 1777 - loss: 0.039786\n",
            "I Training epoch 1778...\n",
            "I Finished training epoch 1778 - loss: 0.014737\n",
            "I Training epoch 1779...\n",
            "I Finished training epoch 1779 - loss: 0.034669\n",
            "I Training epoch 1780...\n",
            "I Finished training epoch 1780 - loss: 0.007495\n",
            "I Training epoch 1781...\n",
            "I Finished training epoch 1781 - loss: 0.007569\n",
            "I Training epoch 1782...\n",
            "I Finished training epoch 1782 - loss: 0.014216\n",
            "I Training epoch 1783...\n",
            "I Finished training epoch 1783 - loss: 0.065959\n",
            "I Training epoch 1784...\n",
            "I Finished training epoch 1784 - loss: 0.008786\n",
            "I Training epoch 1785...\n",
            "I Finished training epoch 1785 - loss: 0.011757\n",
            "I Training epoch 1786...\n",
            "I Finished training epoch 1786 - loss: 0.011165\n",
            "I Training epoch 1787...\n",
            "I Finished training epoch 1787 - loss: 0.014059\n",
            "I Training epoch 1788...\n",
            "I Finished training epoch 1788 - loss: 0.005908\n",
            "I Training epoch 1789...\n",
            "I Finished training epoch 1789 - loss: 0.013286\n",
            "I Training epoch 1790...\n",
            "I Finished training epoch 1790 - loss: 0.005054\n",
            "I Training epoch 1791...\n",
            "I Finished training epoch 1791 - loss: 0.008038\n",
            "I Training epoch 1792...\n",
            "I Finished training epoch 1792 - loss: 0.010350\n",
            "I Training epoch 1793...\n",
            "I Finished training epoch 1793 - loss: 0.011049\n",
            "I Training epoch 1794...\n",
            "I Finished training epoch 1794 - loss: 0.010502\n",
            "I Training epoch 1795...\n",
            "I Finished training epoch 1795 - loss: 0.009374\n",
            "I Training epoch 1796...\n",
            "I Finished training epoch 1796 - loss: 0.023584\n",
            "I Training epoch 1797...\n",
            "I Finished training epoch 1797 - loss: 0.014620\n",
            "I Training epoch 1798...\n",
            "I Finished training epoch 1798 - loss: 0.010303\n",
            "I Training epoch 1799...\n",
            "I Finished training epoch 1799 - loss: 0.008690\n",
            "I Training epoch 1800...\n",
            "I Finished training epoch 1800 - loss: 0.005971\n",
            "I Training epoch 1801...\n",
            "I Finished training epoch 1801 - loss: 0.011194\n",
            "I Training epoch 1802...\n",
            "I Finished training epoch 1802 - loss: 0.017723\n",
            "I Training epoch 1803...\n",
            "I Finished training epoch 1803 - loss: 0.011727\n",
            "I Training epoch 1804...\n",
            "I Finished training epoch 1804 - loss: 0.005107\n",
            "I Training epoch 1805...\n",
            "I Finished training epoch 1805 - loss: 0.005353\n",
            "I Training epoch 1806...\n",
            "I Finished training epoch 1806 - loss: 0.018130\n",
            "I Training epoch 1807...\n",
            "I Finished training epoch 1807 - loss: 0.014586\n",
            "I Training epoch 1808...\n",
            "I Finished training epoch 1808 - loss: 0.020737\n",
            "I Training epoch 1809...\n",
            "I Finished training epoch 1809 - loss: 0.013761\n",
            "I Training epoch 1810...\n",
            "I Finished training epoch 1810 - loss: 0.006109\n",
            "I Training epoch 1811...\n",
            "I Finished training epoch 1811 - loss: 0.009191\n",
            "I Training epoch 1812...\n",
            "I Finished training epoch 1812 - loss: 0.022962\n",
            "I Training epoch 1813...\n",
            "I Finished training epoch 1813 - loss: 0.014738\n",
            "I Training epoch 1814...\n",
            "I Finished training epoch 1814 - loss: 0.004610\n",
            "I Training epoch 1815...\n",
            "I Finished training epoch 1815 - loss: 0.006132\n",
            "I Training epoch 1816...\n",
            "I Finished training epoch 1816 - loss: 0.004580\n",
            "I Training epoch 1817...\n",
            "I Finished training epoch 1817 - loss: 0.006259\n",
            "I Training epoch 1818...\n",
            "I Finished training epoch 1818 - loss: 0.011787\n",
            "I Training epoch 1819...\n",
            "I Finished training epoch 1819 - loss: 0.015499\n",
            "I Training epoch 1820...\n",
            "I Finished training epoch 1820 - loss: 0.006924\n",
            "I Training epoch 1821...\n",
            "I Finished training epoch 1821 - loss: 0.005649\n",
            "I Training epoch 1822...\n",
            "I Finished training epoch 1822 - loss: 0.006642\n",
            "I Training epoch 1823...\n",
            "I Finished training epoch 1823 - loss: 0.006661\n",
            "I Training epoch 1824...\n",
            "I Finished training epoch 1824 - loss: 0.004978\n",
            "I Training epoch 1825...\n",
            "I Finished training epoch 1825 - loss: 0.008008\n",
            "I Training epoch 1826...\n",
            "I Finished training epoch 1826 - loss: 0.008647\n",
            "I Training epoch 1827...\n",
            "I Finished training epoch 1827 - loss: 0.016593\n",
            "I Training epoch 1828...\n",
            "I Finished training epoch 1828 - loss: 0.008005\n",
            "I Training epoch 1829...\n",
            "I Finished training epoch 1829 - loss: 0.007435\n",
            "I Training epoch 1830...\n",
            "I Finished training epoch 1830 - loss: 0.075344\n",
            "I Training epoch 1831...\n",
            "I Finished training epoch 1831 - loss: 0.027132\n",
            "I Training epoch 1832...\n",
            "I Finished training epoch 1832 - loss: 0.010053\n",
            "I Training epoch 1833...\n",
            "I Finished training epoch 1833 - loss: 0.004816\n",
            "I Training epoch 1834...\n",
            "I Finished training epoch 1834 - loss: 0.017773\n",
            "I Training epoch 1835...\n",
            "I Finished training epoch 1835 - loss: 0.008600\n",
            "I Training epoch 1836...\n",
            "I Finished training epoch 1836 - loss: 0.005132\n",
            "I Training epoch 1837...\n",
            "I Finished training epoch 1837 - loss: 0.022390\n",
            "I Training epoch 1838...\n",
            "I Finished training epoch 1838 - loss: 0.006370\n",
            "I Training epoch 1839...\n",
            "I Finished training epoch 1839 - loss: 0.002611\n",
            "I Training epoch 1840...\n",
            "I Finished training epoch 1840 - loss: 0.021801\n",
            "I Training epoch 1841...\n",
            "I Finished training epoch 1841 - loss: 0.003539\n",
            "I Training epoch 1842...\n",
            "I Finished training epoch 1842 - loss: 0.007228\n",
            "I Training epoch 1843...\n",
            "I Finished training epoch 1843 - loss: 0.013135\n",
            "I Training epoch 1844...\n",
            "I Finished training epoch 1844 - loss: 0.006010\n",
            "I Training epoch 1845...\n",
            "I Finished training epoch 1845 - loss: 0.005223\n",
            "I Training epoch 1846...\n",
            "I Finished training epoch 1846 - loss: 0.005991\n",
            "I Training epoch 1847...\n",
            "I Finished training epoch 1847 - loss: 0.003774\n",
            "I Training epoch 1848...\n",
            "I Finished training epoch 1848 - loss: 0.003585\n",
            "I Training epoch 1849...\n",
            "I Finished training epoch 1849 - loss: 0.008638\n",
            "I Training epoch 1850...\n",
            "I Finished training epoch 1850 - loss: 0.006133\n",
            "I Training epoch 1851...\n",
            "I Finished training epoch 1851 - loss: 0.009061\n",
            "I Training epoch 1852...\n",
            "I Finished training epoch 1852 - loss: 0.014134\n",
            "I Training epoch 1853...\n",
            "I Finished training epoch 1853 - loss: 0.005124\n",
            "I Training epoch 1854...\n",
            "I Finished training epoch 1854 - loss: 0.005887\n",
            "I Training epoch 1855...\n",
            "I Finished training epoch 1855 - loss: 0.012938\n",
            "I Training epoch 1856...\n",
            "I Finished training epoch 1856 - loss: 0.006969\n",
            "I Training epoch 1857...\n",
            "I Finished training epoch 1857 - loss: 0.013212\n",
            "I Training epoch 1858...\n",
            "I Finished training epoch 1858 - loss: 0.005878\n",
            "I Training epoch 1859...\n",
            "I Finished training epoch 1859 - loss: 0.003436\n",
            "I Training epoch 1860...\n",
            "I Finished training epoch 1860 - loss: 0.008014\n",
            "I Training epoch 1861...\n",
            "I Finished training epoch 1861 - loss: 0.002872\n",
            "I Training epoch 1862...\n",
            "I Finished training epoch 1862 - loss: 0.004720\n",
            "I Training epoch 1863...\n",
            "I Finished training epoch 1863 - loss: 0.004463\n",
            "I Training epoch 1864...\n",
            "I Finished training epoch 1864 - loss: 0.007973\n",
            "I Training epoch 1865...\n",
            "I Finished training epoch 1865 - loss: 0.003502\n",
            "I Training epoch 1866...\n",
            "I Finished training epoch 1866 - loss: 0.007014\n",
            "I Training epoch 1867...\n",
            "I Finished training epoch 1867 - loss: 0.007543\n",
            "I Training epoch 1868...\n",
            "I Finished training epoch 1868 - loss: 0.005782\n",
            "I Training epoch 1869...\n",
            "I Finished training epoch 1869 - loss: 0.010949\n",
            "I Training epoch 1870...\n",
            "I Finished training epoch 1870 - loss: 0.005051\n",
            "I Training epoch 1871...\n",
            "I Finished training epoch 1871 - loss: 0.006570\n",
            "I Training epoch 1872...\n",
            "I Finished training epoch 1872 - loss: 0.006785\n",
            "I Training epoch 1873...\n",
            "I Finished training epoch 1873 - loss: 0.023499\n",
            "I Training epoch 1874...\n",
            "I Finished training epoch 1874 - loss: 0.005719\n",
            "I Training epoch 1875...\n",
            "I Finished training epoch 1875 - loss: 0.005215\n",
            "I Training epoch 1876...\n",
            "I Finished training epoch 1876 - loss: 0.003037\n",
            "I Training epoch 1877...\n",
            "I Finished training epoch 1877 - loss: 0.010984\n",
            "I Training epoch 1878...\n",
            "I Finished training epoch 1878 - loss: 0.004938\n",
            "I Training epoch 1879...\n",
            "I Finished training epoch 1879 - loss: 0.014111\n",
            "I Training epoch 1880...\n",
            "I Finished training epoch 1880 - loss: 0.004883\n",
            "I Training epoch 1881...\n",
            "I Finished training epoch 1881 - loss: 0.008685\n",
            "I Training epoch 1882...\n",
            "I Finished training epoch 1882 - loss: 0.004027\n",
            "I Training epoch 1883...\n",
            "I Finished training epoch 1883 - loss: 0.002977\n",
            "I Training epoch 1884...\n",
            "I Finished training epoch 1884 - loss: 0.003372\n",
            "I Training epoch 1885...\n",
            "I Finished training epoch 1885 - loss: 0.006169\n",
            "I Training epoch 1886...\n",
            "I Finished training epoch 1886 - loss: 0.238856\n",
            "I Training epoch 1887...\n",
            "I Finished training epoch 1887 - loss: 0.006681\n",
            "I Training epoch 1888...\n",
            "I Finished training epoch 1888 - loss: 0.003760\n",
            "I Training epoch 1889...\n",
            "I Finished training epoch 1889 - loss: 0.006015\n",
            "I Training epoch 1890...\n",
            "I Finished training epoch 1890 - loss: 0.007215\n",
            "I Training epoch 1891...\n",
            "I Finished training epoch 1891 - loss: 0.017751\n",
            "I Training epoch 1892...\n",
            "I Finished training epoch 1892 - loss: 0.004537\n",
            "I Training epoch 1893...\n",
            "I Finished training epoch 1893 - loss: 0.007561\n",
            "I Training epoch 1894...\n",
            "I Finished training epoch 1894 - loss: 0.003392\n",
            "I Training epoch 1895...\n",
            "I Finished training epoch 1895 - loss: 0.010946\n",
            "I Training epoch 1896...\n",
            "I Finished training epoch 1896 - loss: 0.007035\n",
            "I Training epoch 1897...\n",
            "I Finished training epoch 1897 - loss: 0.006259\n",
            "I Training epoch 1898...\n",
            "I Finished training epoch 1898 - loss: 0.004923\n",
            "I Training epoch 1899...\n",
            "I Finished training epoch 1899 - loss: 0.014614\n",
            "I Training epoch 1900...\n",
            "I Finished training epoch 1900 - loss: 0.008505\n",
            "I Training epoch 1901...\n",
            "I Finished training epoch 1901 - loss: 0.006135\n",
            "I Training epoch 1902...\n",
            "I Finished training epoch 1902 - loss: 0.016899\n",
            "I Training epoch 1903...\n",
            "I Finished training epoch 1903 - loss: 0.003610\n",
            "I Training epoch 1904...\n",
            "I Finished training epoch 1904 - loss: 0.008633\n",
            "I Training epoch 1905...\n",
            "I Finished training epoch 1905 - loss: 0.004221\n",
            "I Training epoch 1906...\n",
            "I Finished training epoch 1906 - loss: 0.005079\n",
            "I Training epoch 1907...\n",
            "I Finished training epoch 1907 - loss: 0.043058\n",
            "I Training epoch 1908...\n",
            "I Finished training epoch 1908 - loss: 0.007030\n",
            "I Training epoch 1909...\n",
            "I Finished training epoch 1909 - loss: 0.007085\n",
            "I Training epoch 1910...\n",
            "I Finished training epoch 1910 - loss: 0.003824\n",
            "I Training epoch 1911...\n",
            "I Finished training epoch 1911 - loss: 0.003762\n",
            "I Training epoch 1912...\n",
            "I Finished training epoch 1912 - loss: 0.007167\n",
            "I Training epoch 1913...\n",
            "I Finished training epoch 1913 - loss: 0.043040\n",
            "I Training epoch 1914...\n",
            "I Finished training epoch 1914 - loss: 0.010732\n",
            "I Training epoch 1915...\n",
            "I Finished training epoch 1915 - loss: 0.006156\n",
            "I Training epoch 1916...\n",
            "I Finished training epoch 1916 - loss: 0.009429\n",
            "I Training epoch 1917...\n",
            "I Finished training epoch 1917 - loss: 0.008758\n",
            "I Training epoch 1918...\n",
            "I Finished training epoch 1918 - loss: 0.010522\n",
            "I Training epoch 1919...\n",
            "I Finished training epoch 1919 - loss: 0.010416\n",
            "I Training epoch 1920...\n",
            "I Finished training epoch 1920 - loss: 0.007739\n",
            "I Training epoch 1921...\n",
            "I Finished training epoch 1921 - loss: 0.006059\n",
            "I Training epoch 1922...\n",
            "I Finished training epoch 1922 - loss: 0.004231\n",
            "I Training epoch 1923...\n",
            "I Finished training epoch 1923 - loss: 0.006696\n",
            "I Training epoch 1924...\n",
            "I Finished training epoch 1924 - loss: 0.005962\n",
            "I Training epoch 1925...\n",
            "I Finished training epoch 1925 - loss: 0.003542\n",
            "I Training epoch 1926...\n",
            "I Finished training epoch 1926 - loss: 0.134502\n",
            "I Training epoch 1927...\n",
            "I Finished training epoch 1927 - loss: 0.004770\n",
            "I Training epoch 1928...\n",
            "I Finished training epoch 1928 - loss: 0.005625\n",
            "I Training epoch 1929...\n",
            "I Finished training epoch 1929 - loss: 0.006627\n",
            "I Training epoch 1930...\n",
            "I Finished training epoch 1930 - loss: 0.008512\n",
            "I Training epoch 1931...\n",
            "I Finished training epoch 1931 - loss: 0.005537\n",
            "I Training epoch 1932...\n",
            "I Finished training epoch 1932 - loss: 0.002590\n",
            "I Training epoch 1933...\n",
            "I Finished training epoch 1933 - loss: 0.006170\n",
            "I Training epoch 1934...\n",
            "I Finished training epoch 1934 - loss: 0.003791\n",
            "I Training epoch 1935...\n",
            "I Finished training epoch 1935 - loss: 0.007069\n",
            "I Training epoch 1936...\n",
            "I Finished training epoch 1936 - loss: 0.008685\n",
            "I Training epoch 1937...\n",
            "I Finished training epoch 1937 - loss: 0.003213\n",
            "I Training epoch 1938...\n",
            "I Finished training epoch 1938 - loss: 0.006541\n",
            "I Training epoch 1939...\n",
            "I Finished training epoch 1939 - loss: 0.005842\n",
            "I Training epoch 1940...\n",
            "I Finished training epoch 1940 - loss: 0.004633\n",
            "I Training epoch 1941...\n",
            "I Finished training epoch 1941 - loss: 0.006774\n",
            "I Training epoch 1942...\n",
            "I Finished training epoch 1942 - loss: 0.007980\n",
            "I Training epoch 1943...\n",
            "I Finished training epoch 1943 - loss: 0.003305\n",
            "I Training epoch 1944...\n",
            "I Finished training epoch 1944 - loss: 0.005061\n",
            "I Training epoch 1945...\n",
            "I Finished training epoch 1945 - loss: 0.011529\n",
            "I Training epoch 1946...\n",
            "I Finished training epoch 1946 - loss: 0.007959\n",
            "I Training epoch 1947...\n",
            "I Finished training epoch 1947 - loss: 0.006617\n",
            "I Training epoch 1948...\n",
            "I Finished training epoch 1948 - loss: 0.002646\n",
            "I Training epoch 1949...\n",
            "I Finished training epoch 1949 - loss: 0.004618\n",
            "I Training epoch 1950...\n",
            "I Finished training epoch 1950 - loss: 0.017769\n",
            "I Training epoch 1951...\n",
            "I Finished training epoch 1951 - loss: 0.006356\n",
            "I Training epoch 1952...\n",
            "I Finished training epoch 1952 - loss: 0.002587\n",
            "I Training epoch 1953...\n",
            "I Finished training epoch 1953 - loss: 0.004665\n",
            "I Training epoch 1954...\n",
            "I Finished training epoch 1954 - loss: 0.009107\n",
            "I Training epoch 1955...\n",
            "I Finished training epoch 1955 - loss: 0.003577\n",
            "I Training epoch 1956...\n",
            "I Finished training epoch 1956 - loss: 0.007962\n",
            "I Training epoch 1957...\n",
            "I Finished training epoch 1957 - loss: 0.006994\n",
            "I Training epoch 1958...\n",
            "I Finished training epoch 1958 - loss: 0.010633\n",
            "I Training epoch 1959...\n",
            "I Finished training epoch 1959 - loss: 0.005217\n",
            "I Training epoch 1960...\n",
            "I Finished training epoch 1960 - loss: 0.008189\n",
            "I Training epoch 1961...\n",
            "I Finished training epoch 1961 - loss: 0.005340\n",
            "I Training epoch 1962...\n",
            "I Finished training epoch 1962 - loss: 0.005049\n",
            "I Training epoch 1963...\n",
            "I Finished training epoch 1963 - loss: 0.007730\n",
            "I Training epoch 1964...\n",
            "I Finished training epoch 1964 - loss: 0.006713\n",
            "I Training epoch 1965...\n",
            "I Finished training epoch 1965 - loss: 0.006356\n",
            "I Training epoch 1966...\n",
            "I Finished training epoch 1966 - loss: 0.006099\n",
            "I Training epoch 1967...\n",
            "I Finished training epoch 1967 - loss: 0.008990\n",
            "I Training epoch 1968...\n",
            "I Finished training epoch 1968 - loss: 0.003243\n",
            "I Training epoch 1969...\n",
            "I Finished training epoch 1969 - loss: 0.003727\n",
            "I Training epoch 1970...\n",
            "I Finished training epoch 1970 - loss: 0.014374\n",
            "I Training epoch 1971...\n",
            "I Finished training epoch 1971 - loss: 0.009751\n",
            "I Training epoch 1972...\n",
            "I Finished training epoch 1972 - loss: 0.003558\n",
            "I Training epoch 1973...\n",
            "I Finished training epoch 1973 - loss: 0.005258\n",
            "I Training epoch 1974...\n",
            "I Finished training epoch 1974 - loss: 0.006832\n",
            "I Training epoch 1975...\n",
            "I Finished training epoch 1975 - loss: 0.002569\n",
            "I Training epoch 1976...\n",
            "I Finished training epoch 1976 - loss: 0.006039\n",
            "I Training epoch 1977...\n",
            "I Finished training epoch 1977 - loss: 0.004362\n",
            "I Training epoch 1978...\n",
            "I Finished training epoch 1978 - loss: 0.003124\n",
            "I Training epoch 1979...\n",
            "I Finished training epoch 1979 - loss: 0.004743\n",
            "I Training epoch 1980...\n",
            "I Finished training epoch 1980 - loss: 0.002973\n",
            "I Training epoch 1981...\n",
            "I Finished training epoch 1981 - loss: 0.004253\n",
            "I Training epoch 1982...\n",
            "I Finished training epoch 1982 - loss: 0.007124\n",
            "I Training epoch 1983...\n",
            "I Finished training epoch 1983 - loss: 0.014642\n",
            "I Training epoch 1984...\n",
            "I Finished training epoch 1984 - loss: 0.006230\n",
            "I Training epoch 1985...\n",
            "I Finished training epoch 1985 - loss: 0.006647\n",
            "I Training epoch 1986...\n",
            "I Finished training epoch 1986 - loss: 0.005737\n",
            "I Training epoch 1987...\n",
            "I Finished training epoch 1987 - loss: 0.005542\n",
            "I Training epoch 1988...\n",
            "I Finished training epoch 1988 - loss: 0.004107\n",
            "I Training epoch 1989...\n",
            "I Finished training epoch 1989 - loss: 0.005090\n",
            "I Training epoch 1990...\n",
            "I Finished training epoch 1990 - loss: 0.002363\n",
            "I Training epoch 1991...\n",
            "I Finished training epoch 1991 - loss: 0.005408\n",
            "I Training epoch 1992...\n",
            "I Finished training epoch 1992 - loss: 0.004225\n",
            "I Training epoch 1993...\n",
            "I Finished training epoch 1993 - loss: 0.003479\n",
            "I Training epoch 1994...\n",
            "I Finished training epoch 1994 - loss: 0.006967\n",
            "I Training epoch 1995...\n",
            "I Finished training epoch 1995 - loss: 0.004115\n",
            "I Training epoch 1996...\n",
            "I Finished training epoch 1996 - loss: 0.005870\n",
            "I Training epoch 1997...\n",
            "I Finished training epoch 1997 - loss: 0.005643\n",
            "I Training epoch 1998...\n",
            "I Finished training epoch 1998 - loss: 0.003043\n",
            "I Training epoch 1999...\n",
            "I Finished training epoch 1999 - loss: 0.003928\n",
            "I FINISHED optimization in 0:08:08.324451\n",
            "WARNING:tensorflow:Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e42cd9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e42cd9e10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "W0531 02:06:11.379092 139837890242432 ag_logging.py:145] Entity <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e42cd9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMBlockWrapper.call of <tensorflow.contrib.rnn.python.ops.lstm_ops.LSTMBlockFusedCell object at 0x7f2e42cd9e10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "INFO:tensorflow:Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-5600\n",
            "I0531 02:06:11.487338 139837890242432 saver.py:1280] Restoring parameters from /root/.local/share/deepspeech/ldc93s1/train-5600\n",
            "I Restored variables from most recent checkpoint at /root/.local/share/deepspeech/ldc93s1/train-5600, step 5600\n",
            "Testing model on data/antibiotics/antibiotics.csv\n",
            "I Test epoch...\n",
            "Test on data/antibiotics/antibiotics.csv - WER: 1.000000, CER: 0.250000, loss: 0.001929\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 2.000000, CER: 0.250000, loss: 0.001929\n",
            " - wav: file:///content/drive/My Drive/deepspeech/DeepSpeech/data/antibiotics/amikacin.wav\n",
            " - src: \"amikacin\"\n",
            " - res: \"amik aci\"\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNpwWXaSXeBF",
        "colab_type": "text"
      },
      "source": [
        "We can observe that **the model is not able to recognise the antibiotic name**.\n",
        "Instead of returning `amikacin` the model returns `ami kain`\n",
        "  * Is this because the work \"amikacin\" is not part of the language model? \n",
        "  * How to improve this?\n",
        "\n",
        "\n",
        "I took `/data/ldc93s1` as an example. It contains a file called `LDC93S1.txt`.\n",
        "\n",
        "  * What is its purpose?\n",
        "  * What do the first 2 parameters correspond to? (I thought the 2nd parameter was the size of the wav file but this doesn't seem to be the case)\n",
        "\n",
        "```\n",
        "0 46797 She had your dark suit in greasy wash water all year.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJbmDAutXzKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}